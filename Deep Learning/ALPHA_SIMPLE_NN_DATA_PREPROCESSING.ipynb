{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w2Gxj-oIedP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sys import getsizeof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-4iKsF1IedU"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/IITMComp/Alpha-10-train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53tzXdnIqxR",
        "outputId": "d455323e-e618-4b66-eed9-91ec0afcb286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scb-uP7VIedU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "classes_count = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0, 'F': 0, 'G': 0, 'H': 0, 'Hello': 0, 'I': 0, 'J': 0, 'K': 0, 'L': 0, 'M': 0, 'Mynameis': 0, 'N': 0, 'Namaste': 0, 'O': 0, 'P': 0, 'Q': 0, 'R': 0, 'S': 0, 'Sorry': 0, 'T': 0, 'Team': 0, 'Thankyou': 0, 'U': 0, 'V': 0, 'W': 0, 'We': 0, 'Welcome': 0, 'World': 0, 'X': 0, 'Y': 0, 'Z': 0}"
      ],
      "metadata": {
        "id": "uuOzpNhLmxHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tVZKcBvIedV"
      },
      "outputs": [],
      "source": [
        "def data_load(path,train_split):\n",
        "    global input_per_sequence,length_of_sequence\n",
        "    # dir= glob.glob(path + '/tctodd12')\n",
        "    \n",
        "    # print(dir)\n",
        "    time_steps = []\n",
        "    X_data = [] # data\n",
        "    Y_data = [] # labels\n",
        "\n",
        "    np.random.seed(101)\n",
        "    for file in os.listdir(path):\n",
        "          print(file)\n",
        "          class_name = file.split('_')[0]\n",
        "  #             print(file.split('_'))\n",
        "          \n",
        "          \n",
        "          arr = np.genfromtxt(path + '/' + file, delimiter='/')\n",
        "          arr = np.delete(arr, -1, axis=1) # chopping last index in all rows\n",
        "          arr = np.delete(arr, 0, axis=1) # chopping 1st index in all rows\n",
        "          # arr = np.delete(arr, 0, axis=1) # chopping 1st index in all rows\n",
        "\n",
        "          for i in range(0,len(arr)-1,2):\n",
        "              arr_n =  np.append(arr[i],arr[i+1])\n",
        "              classes_count[class_name]+=1\n",
        "              X_data.append(arr_n)\n",
        "              Y_data.append(class_name)\n",
        "\n",
        "    for a in range(len(X_data)):\n",
        "        for k in range(0,2):\n",
        "          X_data[a][k] = (X_data[a][k])/360\n",
        "        for k in range(2,7):\n",
        "          X_data[a][k] /= 1023\n",
        "        for k in range(7,9):\n",
        "          X_data[a][k] = (X_data[a][k])/360\n",
        "        for k in range(9,14):\n",
        "          X_data[a][k] /= 1023\n",
        "    \n",
        "\n",
        "              \n",
        "    # return X_data,X_data,Y_data,Y_data\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data,train_size=train_split,stratify=Y_data,random_state=101)\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    \n",
        "    \n",
        "    # return np.array(X_data),Y_data\n",
        "    return X_train, X_test, Y_train, Y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfTW42m0IedW",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7943a3-be12-4bbe-872a-0094aa34cbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thankyou_train_1.txt\n",
            "Sorry_train_1.txt\n",
            "Hello_train_1.txt\n",
            "Namaste_train_1.txt\n",
            "We_train_1.txt\n",
            "Welcome_train_1.txt\n",
            "Welcome_train_2.txt\n",
            "We_train_2.txt\n",
            "Namaste_train_2.txt\n",
            "Hello_train_2.txt\n",
            "Sorry_train_2.txt\n",
            "Thankyou_train_2.txt\n",
            "Thankyou_train_3.txt\n",
            "Sorry_train_3.txt\n",
            "Welcome_train_3.txt\n",
            "We_train_3.txt\n",
            "Namaste_train_3.txt\n",
            "Hello_train_3.txt\n",
            "Hello_train_4.txt\n",
            "Thankyou_train_4.txt\n",
            "Sorry_train_4.txt\n",
            "We_train_4.txt\n",
            "Welcome_train_4.txt\n",
            "Namaste_train_4.txt\n",
            "Namaste_train_5.txt\n",
            "Hello_train_5.txt\n",
            "We_train_5.txt\n",
            "Welcome_train_5.txt\n",
            "Sorry_train_5.txt\n",
            "Thankyou_train_5.txt\n",
            "Sorry_train_6.txt\n",
            "Hello_train_6.txt\n",
            "Thankyou_train_6.txt\n",
            "We_train_6.txt\n",
            "Welcome_train_6.txt\n",
            "Namaste_train_6.txt\n",
            "Namaste_train_7.txt\n",
            "Sorry_train_7.txt\n",
            "Welcome_train_7.txt\n",
            "We_train_7.txt\n",
            "Thankyou_train_7.txt\n",
            "Hello_train_7.txt\n",
            "Hello_train_8.txt\n",
            "Namaste_train_8.txt\n",
            "Thankyou_train_8.txt\n",
            "Sorry_train_8.txt\n",
            "We_train_8.txt\n",
            "Welcome_train_8.txt\n",
            "Welcome_train_9.txt\n",
            "We_train_9.txt\n",
            "Namaste_train_9.txt\n",
            "Thankyou_train_9.txt\n",
            "Sorry_train_9.txt\n",
            "Hello_train_9.txt\n",
            "Hello_train_10.txt\n",
            "We_train_10.txt\n",
            "Welcome_train_10.txt\n",
            "Sorry_train_10.txt\n",
            "Namaste_train_10.txt\n",
            "Thankyou_train_10.txt\n",
            "Thankyou_train_11.txt\n",
            "Namaste_train_11.txt\n",
            "Sorry_train_11.txt\n",
            "Hello_train_11.txt\n",
            "We_train_11.txt\n",
            "Welcome_train_11.txt\n",
            "Sorry_train_12.txt\n",
            "Namaste_train_12.txt\n",
            "Welcome_train_12.txt\n",
            "We_train_12.txt\n",
            "Thankyou_train_12.txt\n",
            "Hello_train_12.txt\n",
            "Hello_train_13.txt\n",
            "We_train_13.txt\n",
            "Welcome_train_13.txt\n",
            "Sorry_train_13.txt\n",
            "Thankyou_train_13.txt\n",
            "Namaste_train_13.txt\n",
            "Namaste_train_14.txt\n",
            "Hello_train_14.txt\n",
            "Welcome_train_14.txt\n",
            "We_train_14.txt\n",
            "Thankyou_train_14.txt\n",
            "Sorry_train_14.txt\n",
            "Sorry_train_15.txt\n",
            "Welcome_train_15.txt\n",
            "We_train_15.txt\n",
            "Thankyou_train_15.txt\n",
            "Namaste_train_15.txt\n",
            "Hello_train_15.txt\n",
            "Hello_train_16.txt\n",
            "We_train_16.txt\n",
            "Welcome_train_16.txt\n",
            "Sorry_train_16.txt\n",
            "Thankyou_train_16.txt\n",
            "Namaste_train_16.txt\n",
            "Namaste_train_17.txt\n",
            "Sorry_train_17.txt\n",
            "Hello_train_17.txt\n",
            "Welcome_train_17.txt\n",
            "We_train_17.txt\n",
            "Thankyou_train_17.txt\n",
            "Thankyou_train_18.txt\n",
            "Sorry_train_18.txt\n",
            "Hello_train_18.txt\n",
            "Namaste_train_18.txt\n",
            "We_train_18.txt\n",
            "Welcome_train_18.txt\n",
            "Welcome_train_19.txt\n",
            "We_train_19.txt\n",
            "Thankyou_train_19.txt\n",
            "Sorry_train_19.txt\n",
            "Namaste_train_19.txt\n",
            "Hello_train_19.txt\n",
            "Hello_train_20.txt\n",
            "Welcome_train_20.txt\n",
            "Namaste_train_20.txt\n",
            "Sorry_train_20.txt\n",
            "Thankyou_train_20.txt\n",
            "We_train_20.txt\n",
            "World_train_1.txt\n",
            "A_train_1.txt\n",
            "L_train_1.txt\n",
            "H_train_1.txt\n",
            "H_train_2.txt\n",
            "World_train_2.txt\n",
            "A_train_2.txt\n",
            "L_train_2.txt\n",
            "L_train_3.txt\n",
            "H_train_3.txt\n",
            "World_train_3.txt\n",
            "A_train_3.txt\n",
            "A_train_4.txt\n",
            "World_train_4.txt\n",
            "H_train_4.txt\n",
            "L_train_4.txt\n",
            "A_train_5.txt\n",
            "L_train_5.txt\n",
            "H_train_5.txt\n",
            "World_train_5.txt\n",
            "World_train_6.txt\n",
            "L_train_6.txt\n",
            "A_train_6.txt\n",
            "H_train_6.txt\n",
            "World_train_7.txt\n",
            "A_train_7.txt\n",
            "L_train_7.txt\n",
            "H_train_7.txt\n",
            "A_train_8.txt\n",
            "H_train_8.txt\n",
            "L_train_8.txt\n",
            "World_train_8.txt\n",
            "A_train_9.txt\n",
            "L_train_9.txt\n",
            "H_train_9.txt\n",
            "World_train_9.txt\n",
            "World_train_10.txt\n",
            "L_train_10.txt\n",
            "H_train_10.txt\n",
            "A_train_10.txt\n",
            "A_train_11.txt\n",
            "H_train_11.txt\n",
            "L_train_11.txt\n",
            "World_train_11.txt\n",
            "H_train_12.txt\n",
            "L_train_12.txt\n",
            "World_train_12.txt\n",
            "A_train_12.txt\n",
            "A_train_13.txt\n",
            "H_train_13.txt\n",
            "L_train_13.txt\n",
            "World_train_13.txt\n",
            "A_train_14.txt\n",
            "World_train_14.txt\n",
            "H_train_14.txt\n",
            "L_train_14.txt\n",
            "L_train_15.txt\n",
            "H_train_15.txt\n",
            "World_train_15.txt\n",
            "A_train_15.txt\n",
            "L_train_16.txt\n",
            "H_train_16.txt\n",
            "World_train_16.txt\n",
            "A_train_16.txt\n",
            "A_train_17.txt\n",
            "L_train_17.txt\n",
            "H_train_17.txt\n",
            "World_train_17.txt\n",
            "L_train_18.txt\n",
            "H_train_18.txt\n",
            "World_train_18.txt\n",
            "A_train_18.txt\n",
            "A_train_19.txt\n",
            "L_train_19.txt\n",
            "H_train_19.txt\n",
            "World_train_19.txt\n",
            "World_train_20.txt\n",
            "H_train_20.txt\n",
            "L_train_20.txt\n",
            "A_train_20.txt\n"
          ]
        }
      ],
      "source": [
        "#loading dataset\n",
        "X_train, X_test, Y_train, Y_test = data_load(path,0.8)\n",
        "# # one hot encoding\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(Y_train)\n",
        "Y_train = to_categorical(le.transform(Y_train))\n",
        "Y_test = to_categorical(le.transform(Y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(X_train, columns = ['left_Roll','left_Pitch','left_flex_1','left_flex_2','left_flex_3','left_flex_4','left_flex_5','right_Roll','right_Pitch','right_flex_1','right_flex_2','right_flex_3','left_flex_4','left_flex_5'])\n",
        "df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "k7N4heDNJ4AU",
        "outputId": "b285f24d-c7dc-4d55-b042-700833067b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-839a1d44-e456-4f54-a621-7d281aaa6a7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>left_Roll</th>\n",
              "      <th>left_Pitch</th>\n",
              "      <th>left_flex_1</th>\n",
              "      <th>left_flex_2</th>\n",
              "      <th>left_flex_3</th>\n",
              "      <th>left_flex_4</th>\n",
              "      <th>left_flex_5</th>\n",
              "      <th>right_Roll</th>\n",
              "      <th>right_Pitch</th>\n",
              "      <th>right_flex_1</th>\n",
              "      <th>right_flex_2</th>\n",
              "      <th>right_flex_3</th>\n",
              "      <th>left_flex_4</th>\n",
              "      <th>left_flex_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.627778</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.095797</td>\n",
              "      <td>0.191593</td>\n",
              "      <td>0.267840</td>\n",
              "      <td>0.334311</td>\n",
              "      <td>0.210166</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.207234</td>\n",
              "      <td>0.227761</td>\n",
              "      <td>0.465298</td>\n",
              "      <td>0.318671</td>\n",
              "      <td>0.302053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.322222</td>\n",
              "      <td>0.219941</td>\n",
              "      <td>0.233627</td>\n",
              "      <td>0.373412</td>\n",
              "      <td>0.404692</td>\n",
              "      <td>0.256109</td>\n",
              "      <td>0.536111</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.203324</td>\n",
              "      <td>0.255132</td>\n",
              "      <td>0.534702</td>\n",
              "      <td>0.448680</td>\n",
              "      <td>0.321603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.630556</td>\n",
              "      <td>0.347222</td>\n",
              "      <td>0.208211</td>\n",
              "      <td>0.284457</td>\n",
              "      <td>0.458456</td>\n",
              "      <td>0.463343</td>\n",
              "      <td>0.336266</td>\n",
              "      <td>0.538889</td>\n",
              "      <td>0.563889</td>\n",
              "      <td>0.262952</td>\n",
              "      <td>0.268817</td>\n",
              "      <td>0.557185</td>\n",
              "      <td>0.490714</td>\n",
              "      <td>0.225806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.586111</td>\n",
              "      <td>0.402778</td>\n",
              "      <td>0.239492</td>\n",
              "      <td>0.222874</td>\n",
              "      <td>0.275660</td>\n",
              "      <td>0.378299</td>\n",
              "      <td>0.239492</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.257087</td>\n",
              "      <td>0.266862</td>\n",
              "      <td>0.497556</td>\n",
              "      <td>0.404692</td>\n",
              "      <td>0.345064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.237537</td>\n",
              "      <td>0.311828</td>\n",
              "      <td>0.497556</td>\n",
              "      <td>0.552297</td>\n",
              "      <td>0.371457</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.278592</td>\n",
              "      <td>0.265885</td>\n",
              "      <td>0.552297</td>\n",
              "      <td>0.442815</td>\n",
              "      <td>0.260020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.452778</td>\n",
              "      <td>0.491667</td>\n",
              "      <td>0.254154</td>\n",
              "      <td>0.252199</td>\n",
              "      <td>0.481916</td>\n",
              "      <td>0.485826</td>\n",
              "      <td>0.338221</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.538889</td>\n",
              "      <td>0.196481</td>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.414467</td>\n",
              "      <td>0.260997</td>\n",
              "      <td>0.150538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.594444</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.197458</td>\n",
              "      <td>0.257087</td>\n",
              "      <td>0.440860</td>\n",
              "      <td>0.448680</td>\n",
              "      <td>0.326491</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>0.220919</td>\n",
              "      <td>0.530792</td>\n",
              "      <td>0.458456</td>\n",
              "      <td>0.187683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.252199</td>\n",
              "      <td>0.276637</td>\n",
              "      <td>0.493646</td>\n",
              "      <td>0.520039</td>\n",
              "      <td>0.351906</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.561111</td>\n",
              "      <td>0.273705</td>\n",
              "      <td>0.254154</td>\n",
              "      <td>0.544477</td>\n",
              "      <td>0.452590</td>\n",
              "      <td>0.240469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.494444</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.188661</td>\n",
              "      <td>0.210166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384164</td>\n",
              "      <td>0.266862</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.502778</td>\n",
              "      <td>0.202346</td>\n",
              "      <td>0.212121</td>\n",
              "      <td>0.406647</td>\n",
              "      <td>0.323558</td>\n",
              "      <td>0.329423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.588889</td>\n",
              "      <td>0.469444</td>\n",
              "      <td>0.222874</td>\n",
              "      <td>0.259042</td>\n",
              "      <td>0.434995</td>\n",
              "      <td>0.436950</td>\n",
              "      <td>0.334311</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.237537</td>\n",
              "      <td>0.235582</td>\n",
              "      <td>0.547410</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.187683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.597222</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.201369</td>\n",
              "      <td>0.327468</td>\n",
              "      <td>0.510264</td>\n",
              "      <td>0.511241</td>\n",
              "      <td>0.352884</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.477778</td>\n",
              "      <td>0.257087</td>\n",
              "      <td>0.250244</td>\n",
              "      <td>0.557185</td>\n",
              "      <td>0.466276</td>\n",
              "      <td>0.240469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.636111</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.243402</td>\n",
              "      <td>0.282502</td>\n",
              "      <td>0.447703</td>\n",
              "      <td>0.483871</td>\n",
              "      <td>0.314761</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.233627</td>\n",
              "      <td>0.261975</td>\n",
              "      <td>0.535679</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.261975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.627778</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.189638</td>\n",
              "      <td>0.249267</td>\n",
              "      <td>0.232649</td>\n",
              "      <td>0.400782</td>\n",
              "      <td>0.281525</td>\n",
              "      <td>0.427778</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.291300</td>\n",
              "      <td>0.302053</td>\n",
              "      <td>0.573803</td>\n",
              "      <td>0.449658</td>\n",
              "      <td>0.375367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.588889</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.209189</td>\n",
              "      <td>0.264907</td>\n",
              "      <td>0.445748</td>\n",
              "      <td>0.442815</td>\n",
              "      <td>0.341153</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.486111</td>\n",
              "      <td>0.241447</td>\n",
              "      <td>0.239492</td>\n",
              "      <td>0.553275</td>\n",
              "      <td>0.491691</td>\n",
              "      <td>0.198436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.336111</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.264907</td>\n",
              "      <td>0.383187</td>\n",
              "      <td>0.410557</td>\n",
              "      <td>0.259042</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.275660</td>\n",
              "      <td>0.313783</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>0.381232</td>\n",
              "      <td>0.321603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.281525</td>\n",
              "      <td>0.278592</td>\n",
              "      <td>0.452590</td>\n",
              "      <td>0.512219</td>\n",
              "      <td>0.359726</td>\n",
              "      <td>0.536111</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.250244</td>\n",
              "      <td>0.266862</td>\n",
              "      <td>0.523949</td>\n",
              "      <td>0.477028</td>\n",
              "      <td>0.221896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.963889</td>\n",
              "      <td>0.838889</td>\n",
              "      <td>0.217986</td>\n",
              "      <td>0.341153</td>\n",
              "      <td>0.524927</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>0.380254</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.291300</td>\n",
              "      <td>0.289345</td>\n",
              "      <td>0.577713</td>\n",
              "      <td>0.468231</td>\n",
              "      <td>0.271750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.336111</td>\n",
              "      <td>0.294233</td>\n",
              "      <td>0.251222</td>\n",
              "      <td>0.401760</td>\n",
              "      <td>0.427175</td>\n",
              "      <td>0.284457</td>\n",
              "      <td>0.538889</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.242424</td>\n",
              "      <td>0.270772</td>\n",
              "      <td>0.556207</td>\n",
              "      <td>0.427175</td>\n",
              "      <td>0.343109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.508333</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>0.284457</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.410557</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>0.572222</td>\n",
              "      <td>0.338889</td>\n",
              "      <td>0.342131</td>\n",
              "      <td>0.312805</td>\n",
              "      <td>0.536657</td>\n",
              "      <td>0.327468</td>\n",
              "      <td>0.356794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.638889</td>\n",
              "      <td>0.338889</td>\n",
              "      <td>0.320626</td>\n",
              "      <td>0.257087</td>\n",
              "      <td>0.367546</td>\n",
              "      <td>0.421310</td>\n",
              "      <td>0.240469</td>\n",
              "      <td>0.527778</td>\n",
              "      <td>0.461111</td>\n",
              "      <td>0.250244</td>\n",
              "      <td>0.303030</td>\n",
              "      <td>0.565982</td>\n",
              "      <td>0.442815</td>\n",
              "      <td>0.348974</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-839a1d44-e456-4f54-a621-7d281aaa6a7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-839a1d44-e456-4f54-a621-7d281aaa6a7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-839a1d44-e456-4f54-a621-7d281aaa6a7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    left_Roll  left_Pitch  left_flex_1  ...  right_flex_3  left_flex_4  left_flex_5\n",
              "0    0.627778    0.525000     0.095797  ...      0.465298     0.318671     0.302053\n",
              "1    0.675000    0.322222     0.219941  ...      0.534702     0.448680     0.321603\n",
              "2    0.630556    0.347222     0.208211  ...      0.557185     0.490714     0.225806\n",
              "3    0.586111    0.402778     0.239492  ...      0.497556     0.404692     0.345064\n",
              "4    0.450000    0.475000     0.237537  ...      0.552297     0.442815     0.260020\n",
              "5    0.452778    0.491667     0.254154  ...      0.414467     0.260997     0.150538\n",
              "6    0.594444    0.472222     0.197458  ...      0.530792     0.458456     0.187683\n",
              "7    0.450000    0.516667     0.252199  ...      0.544477     0.452590     0.240469\n",
              "8    0.494444    0.458333     0.188661  ...      0.406647     0.323558     0.329423\n",
              "9    0.588889    0.469444     0.222874  ...      0.547410     0.484848     0.187683\n",
              "10   0.597222    0.466667     0.201369  ...      0.557185     0.466276     0.240469\n",
              "11   0.636111    0.355556     0.243402  ...      0.535679     0.484848     0.261975\n",
              "12   0.627778    0.516667     0.189638  ...      0.573803     0.449658     0.375367\n",
              "13   0.588889    0.472222     0.209189  ...      0.553275     0.491691     0.198436\n",
              "14   0.650000    0.336111     0.290323  ...      0.580645     0.381232     0.321603\n",
              "15   0.458333    0.500000     0.281525  ...      0.523949     0.477028     0.221896\n",
              "16   0.963889    0.838889     0.217986  ...      0.577713     0.468231     0.271750\n",
              "17   0.644444    0.336111     0.294233  ...      0.556207     0.427175     0.343109\n",
              "18   0.616667    0.508333     0.130010  ...      0.536657     0.327468     0.356794\n",
              "19   0.638889    0.338889     0.320626  ...      0.565982     0.442815     0.348974\n",
              "\n",
              "[20 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V_3OPF5IedX",
        "outputId": "e9e33eed-ccc4-48d4-f5d5-aa2a110b3f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 0, 'H': 1, 'Hello': 2, 'L': 3, 'Namaste': 4, 'Sorry': 5, 'Thankyou': 6, 'We': 7, 'Welcome': 8, 'World': 9}\n"
          ]
        }
      ],
      "source": [
        "le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(le_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O4vAVZAnSnt",
        "outputId": "44d7818d-c929-4235-94e8-07d497017315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': 1000,\n",
              " 'B': 0,\n",
              " 'C': 0,\n",
              " 'D': 0,\n",
              " 'E': 0,\n",
              " 'F': 0,\n",
              " 'G': 0,\n",
              " 'H': 1000,\n",
              " 'Hello': 1000,\n",
              " 'I': 0,\n",
              " 'J': 0,\n",
              " 'K': 0,\n",
              " 'L': 1000,\n",
              " 'M': 0,\n",
              " 'Mynameis': 0,\n",
              " 'N': 0,\n",
              " 'Namaste': 1000,\n",
              " 'O': 0,\n",
              " 'P': 0,\n",
              " 'Q': 0,\n",
              " 'R': 0,\n",
              " 'S': 0,\n",
              " 'Sorry': 1000,\n",
              " 'T': 0,\n",
              " 'Team': 0,\n",
              " 'Thankyou': 1000,\n",
              " 'U': 0,\n",
              " 'V': 0,\n",
              " 'W': 0,\n",
              " 'We': 1000,\n",
              " 'Welcome': 1000,\n",
              " 'World': 1000,\n",
              " 'X': 0,\n",
              " 'Y': 0,\n",
              " 'Z': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaiRc98UIedZ",
        "outputId": "bf60ad4c-96b3-4af1-d567-68e2dc218287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62777778 0.525      0.09579668 0.19159335 0.26783969 0.33431085\n",
            " 0.21016618 0.41666667 0.48611111 0.20723363 0.22776149 0.46529814\n",
            " 0.31867058 0.30205279] 14\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0],len(X_train[0]))\n",
        "print(Y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNfWY17uIedZ",
        "outputId": "cc6b2ec9-c5c7-423c-d3e8-5b2e06a8524d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 14)\n",
            "(2000, 14)\n",
            "(8000, 10)\n",
            "(2000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qaLliEXIeda",
        "outputId": "7da31025-217a-4f95-ad5e-78e3b38d1fad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Y_train.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj2RiavxIedb"
      },
      "outputs": [],
      "source": [
        "# def my_model():\n",
        "    \n",
        "#     inputs= keras.Input(shape=(X_train.shape[1]))   #(batch_size, timesteps, input_dim).\n",
        "#     x = layers.Dense(120,activation = \"tanx\")(inputs)\n",
        "#     outputs = layers.Dense(Y_train.shape[1],activation = \"softmax\")(x)\n",
        "#     model = keras.Model(inputs = inputs,outputs = outputs)\n",
        "#     return model\n",
        "# model = my_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI5Pne2_Iedb",
        "outputId": "f013e4bc-0ba2-4447-90f3-e656fd162b20",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 256)               3840      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 50)                6450      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,696\n",
            "Trainable params: 43,696\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.Input(shape=(X_train.shape[1])),\n",
        "  tf.keras.layers.Dense(256, activation='tanh'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(128, activation='tanh'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(50, activation='tanh'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(Y_train.shape[1],activation=\"softmax\")\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UftZKbQFIedc",
        "outputId": "9e37ccb3-0328-4538-894f-dac1954e9e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.CategoricalCrossentropy(),\n",
        "     optimizer = keras.optimizers.Adam(lr = 0.001),\n",
        "     metrics = [\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fPNHR51Iedc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFAfdNMkIedc",
        "outputId": "fdeebec9-c5a2-4f77-a8a7-0c7b8cab7584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "250/250 - 2s - loss: 1.7902 - accuracy: 0.3676 - val_loss: 1.0013 - val_accuracy: 0.8230 - 2s/epoch - 6ms/step\n",
            "Epoch 2/1000\n",
            "250/250 - 1s - loss: 0.8437 - accuracy: 0.7199 - val_loss: 0.4711 - val_accuracy: 0.9265 - 624ms/epoch - 2ms/step\n",
            "Epoch 3/1000\n",
            "250/250 - 1s - loss: 0.5229 - accuracy: 0.8487 - val_loss: 0.3070 - val_accuracy: 0.9435 - 627ms/epoch - 3ms/step\n",
            "Epoch 4/1000\n",
            "250/250 - 1s - loss: 0.4003 - accuracy: 0.8879 - val_loss: 0.2514 - val_accuracy: 0.9570 - 615ms/epoch - 2ms/step\n",
            "Epoch 5/1000\n",
            "250/250 - 1s - loss: 0.3590 - accuracy: 0.9041 - val_loss: 0.2061 - val_accuracy: 0.9655 - 602ms/epoch - 2ms/step\n",
            "Epoch 6/1000\n",
            "250/250 - 1s - loss: 0.3091 - accuracy: 0.9231 - val_loss: 0.2027 - val_accuracy: 0.9550 - 625ms/epoch - 3ms/step\n",
            "Epoch 7/1000\n",
            "250/250 - 1s - loss: 0.2789 - accuracy: 0.9290 - val_loss: 0.1755 - val_accuracy: 0.9670 - 527ms/epoch - 2ms/step\n",
            "Epoch 8/1000\n",
            "250/250 - 1s - loss: 0.2559 - accuracy: 0.9408 - val_loss: 0.1503 - val_accuracy: 0.9695 - 531ms/epoch - 2ms/step\n",
            "Epoch 9/1000\n",
            "250/250 - 1s - loss: 0.2485 - accuracy: 0.9376 - val_loss: 0.1450 - val_accuracy: 0.9725 - 522ms/epoch - 2ms/step\n",
            "Epoch 10/1000\n",
            "250/250 - 1s - loss: 0.2400 - accuracy: 0.9423 - val_loss: 0.1415 - val_accuracy: 0.9760 - 614ms/epoch - 2ms/step\n",
            "Epoch 11/1000\n",
            "250/250 - 1s - loss: 0.2474 - accuracy: 0.9420 - val_loss: 0.1356 - val_accuracy: 0.9750 - 524ms/epoch - 2ms/step\n",
            "Epoch 12/1000\n",
            "250/250 - 1s - loss: 0.2293 - accuracy: 0.9438 - val_loss: 0.1488 - val_accuracy: 0.9640 - 541ms/epoch - 2ms/step\n",
            "Epoch 13/1000\n",
            "250/250 - 1s - loss: 0.2248 - accuracy: 0.9442 - val_loss: 0.1295 - val_accuracy: 0.9745 - 636ms/epoch - 3ms/step\n",
            "Epoch 14/1000\n",
            "250/250 - 1s - loss: 0.2184 - accuracy: 0.9498 - val_loss: 0.1278 - val_accuracy: 0.9760 - 685ms/epoch - 3ms/step\n",
            "Epoch 15/1000\n",
            "250/250 - 1s - loss: 0.2293 - accuracy: 0.9449 - val_loss: 0.1200 - val_accuracy: 0.9775 - 637ms/epoch - 3ms/step\n",
            "Epoch 16/1000\n",
            "250/250 - 1s - loss: 0.2080 - accuracy: 0.9505 - val_loss: 0.1269 - val_accuracy: 0.9745 - 633ms/epoch - 3ms/step\n",
            "Epoch 17/1000\n",
            "250/250 - 1s - loss: 0.1926 - accuracy: 0.9551 - val_loss: 0.1172 - val_accuracy: 0.9770 - 530ms/epoch - 2ms/step\n",
            "Epoch 18/1000\n",
            "250/250 - 1s - loss: 0.1971 - accuracy: 0.9516 - val_loss: 0.1174 - val_accuracy: 0.9755 - 538ms/epoch - 2ms/step\n",
            "Epoch 19/1000\n",
            "250/250 - 1s - loss: 0.1977 - accuracy: 0.9539 - val_loss: 0.1284 - val_accuracy: 0.9700 - 600ms/epoch - 2ms/step\n",
            "Epoch 20/1000\n",
            "250/250 - 1s - loss: 0.2032 - accuracy: 0.9526 - val_loss: 0.1200 - val_accuracy: 0.9740 - 605ms/epoch - 2ms/step\n",
            "Epoch 21/1000\n",
            "250/250 - 1s - loss: 0.1824 - accuracy: 0.9576 - val_loss: 0.1042 - val_accuracy: 0.9795 - 601ms/epoch - 2ms/step\n",
            "Epoch 22/1000\n",
            "250/250 - 1s - loss: 0.1932 - accuracy: 0.9572 - val_loss: 0.1055 - val_accuracy: 0.9765 - 615ms/epoch - 2ms/step\n",
            "Epoch 23/1000\n",
            "250/250 - 1s - loss: 0.2005 - accuracy: 0.9507 - val_loss: 0.1091 - val_accuracy: 0.9780 - 604ms/epoch - 2ms/step\n",
            "Epoch 24/1000\n",
            "250/250 - 1s - loss: 0.1739 - accuracy: 0.9600 - val_loss: 0.1076 - val_accuracy: 0.9770 - 621ms/epoch - 2ms/step\n",
            "Epoch 25/1000\n",
            "250/250 - 1s - loss: 0.1922 - accuracy: 0.9523 - val_loss: 0.1091 - val_accuracy: 0.9755 - 560ms/epoch - 2ms/step\n",
            "Epoch 26/1000\n",
            "250/250 - 1s - loss: 0.1758 - accuracy: 0.9574 - val_loss: 0.1148 - val_accuracy: 0.9730 - 525ms/epoch - 2ms/step\n",
            "Epoch 27/1000\n",
            "250/250 - 1s - loss: 0.1905 - accuracy: 0.9545 - val_loss: 0.1187 - val_accuracy: 0.9730 - 533ms/epoch - 2ms/step\n",
            "Epoch 28/1000\n",
            "250/250 - 1s - loss: 0.1708 - accuracy: 0.9588 - val_loss: 0.1040 - val_accuracy: 0.9775 - 598ms/epoch - 2ms/step\n",
            "Epoch 29/1000\n",
            "250/250 - 1s - loss: 0.1828 - accuracy: 0.9556 - val_loss: 0.1031 - val_accuracy: 0.9785 - 535ms/epoch - 2ms/step\n",
            "Epoch 30/1000\n",
            "250/250 - 1s - loss: 0.1806 - accuracy: 0.9561 - val_loss: 0.1027 - val_accuracy: 0.9795 - 521ms/epoch - 2ms/step\n",
            "Epoch 31/1000\n",
            "250/250 - 1s - loss: 0.1668 - accuracy: 0.9600 - val_loss: 0.1066 - val_accuracy: 0.9785 - 604ms/epoch - 2ms/step\n",
            "Epoch 32/1000\n",
            "250/250 - 1s - loss: 0.1708 - accuracy: 0.9606 - val_loss: 0.1135 - val_accuracy: 0.9745 - 556ms/epoch - 2ms/step\n",
            "Epoch 33/1000\n",
            "250/250 - 1s - loss: 0.1636 - accuracy: 0.9640 - val_loss: 0.1054 - val_accuracy: 0.9740 - 556ms/epoch - 2ms/step\n",
            "Epoch 34/1000\n",
            "250/250 - 1s - loss: 0.1831 - accuracy: 0.9550 - val_loss: 0.1287 - val_accuracy: 0.9625 - 539ms/epoch - 2ms/step\n",
            "Epoch 35/1000\n",
            "250/250 - 1s - loss: 0.1676 - accuracy: 0.9586 - val_loss: 0.1082 - val_accuracy: 0.9715 - 606ms/epoch - 2ms/step\n",
            "Epoch 36/1000\n",
            "250/250 - 1s - loss: 0.1666 - accuracy: 0.9592 - val_loss: 0.0952 - val_accuracy: 0.9790 - 617ms/epoch - 2ms/step\n",
            "Epoch 37/1000\n",
            "250/250 - 1s - loss: 0.1637 - accuracy: 0.9629 - val_loss: 0.0935 - val_accuracy: 0.9795 - 531ms/epoch - 2ms/step\n",
            "Epoch 38/1000\n",
            "250/250 - 1s - loss: 0.1611 - accuracy: 0.9619 - val_loss: 0.1041 - val_accuracy: 0.9765 - 628ms/epoch - 3ms/step\n",
            "Epoch 39/1000\n",
            "250/250 - 1s - loss: 0.1650 - accuracy: 0.9599 - val_loss: 0.1071 - val_accuracy: 0.9760 - 604ms/epoch - 2ms/step\n",
            "Epoch 40/1000\n",
            "250/250 - 1s - loss: 0.1637 - accuracy: 0.9611 - val_loss: 0.1226 - val_accuracy: 0.9675 - 534ms/epoch - 2ms/step\n",
            "Epoch 41/1000\n",
            "250/250 - 1s - loss: 0.1697 - accuracy: 0.9588 - val_loss: 0.0951 - val_accuracy: 0.9780 - 540ms/epoch - 2ms/step\n",
            "Epoch 42/1000\n",
            "250/250 - 1s - loss: 0.1644 - accuracy: 0.9594 - val_loss: 0.0888 - val_accuracy: 0.9775 - 623ms/epoch - 2ms/step\n",
            "Epoch 43/1000\n",
            "250/250 - 1s - loss: 0.1627 - accuracy: 0.9601 - val_loss: 0.0897 - val_accuracy: 0.9800 - 546ms/epoch - 2ms/step\n",
            "Epoch 44/1000\n",
            "250/250 - 1s - loss: 0.1562 - accuracy: 0.9630 - val_loss: 0.0986 - val_accuracy: 0.9790 - 528ms/epoch - 2ms/step\n",
            "Epoch 45/1000\n",
            "250/250 - 1s - loss: 0.1559 - accuracy: 0.9647 - val_loss: 0.1165 - val_accuracy: 0.9700 - 618ms/epoch - 2ms/step\n",
            "Epoch 46/1000\n",
            "250/250 - 1s - loss: 0.1545 - accuracy: 0.9638 - val_loss: 0.0965 - val_accuracy: 0.9755 - 535ms/epoch - 2ms/step\n",
            "Epoch 47/1000\n",
            "250/250 - 1s - loss: 0.1548 - accuracy: 0.9631 - val_loss: 0.0977 - val_accuracy: 0.9800 - 543ms/epoch - 2ms/step\n",
            "Epoch 48/1000\n",
            "250/250 - 1s - loss: 0.1591 - accuracy: 0.9632 - val_loss: 0.0845 - val_accuracy: 0.9800 - 601ms/epoch - 2ms/step\n",
            "Epoch 49/1000\n",
            "250/250 - 1s - loss: 0.1622 - accuracy: 0.9622 - val_loss: 0.0864 - val_accuracy: 0.9795 - 600ms/epoch - 2ms/step\n",
            "Epoch 50/1000\n",
            "250/250 - 1s - loss: 0.1511 - accuracy: 0.9647 - val_loss: 0.0883 - val_accuracy: 0.9785 - 640ms/epoch - 3ms/step\n",
            "Epoch 51/1000\n",
            "250/250 - 1s - loss: 0.1562 - accuracy: 0.9617 - val_loss: 0.0983 - val_accuracy: 0.9760 - 520ms/epoch - 2ms/step\n",
            "Epoch 52/1000\n",
            "250/250 - 1s - loss: 0.1539 - accuracy: 0.9640 - val_loss: 0.0926 - val_accuracy: 0.9795 - 603ms/epoch - 2ms/step\n",
            "Epoch 53/1000\n",
            "250/250 - 1s - loss: 0.1555 - accuracy: 0.9624 - val_loss: 0.0932 - val_accuracy: 0.9795 - 532ms/epoch - 2ms/step\n",
            "Epoch 54/1000\n",
            "250/250 - 1s - loss: 0.1541 - accuracy: 0.9638 - val_loss: 0.0882 - val_accuracy: 0.9805 - 543ms/epoch - 2ms/step\n",
            "Epoch 55/1000\n",
            "250/250 - 1s - loss: 0.1703 - accuracy: 0.9561 - val_loss: 0.0863 - val_accuracy: 0.9795 - 601ms/epoch - 2ms/step\n",
            "Epoch 56/1000\n",
            "250/250 - 1s - loss: 0.1556 - accuracy: 0.9622 - val_loss: 0.0848 - val_accuracy: 0.9800 - 604ms/epoch - 2ms/step\n",
            "Epoch 57/1000\n",
            "250/250 - 1s - loss: 0.1480 - accuracy: 0.9657 - val_loss: 0.0909 - val_accuracy: 0.9795 - 551ms/epoch - 2ms/step\n",
            "Epoch 58/1000\n",
            "250/250 - 1s - loss: 0.1587 - accuracy: 0.9610 - val_loss: 0.0887 - val_accuracy: 0.9800 - 607ms/epoch - 2ms/step\n",
            "Epoch 59/1000\n",
            "250/250 - 1s - loss: 0.1429 - accuracy: 0.9696 - val_loss: 0.0844 - val_accuracy: 0.9800 - 536ms/epoch - 2ms/step\n",
            "Epoch 60/1000\n",
            "250/250 - 1s - loss: 0.1400 - accuracy: 0.9659 - val_loss: 0.0920 - val_accuracy: 0.9775 - 605ms/epoch - 2ms/step\n",
            "Epoch 61/1000\n",
            "250/250 - 1s - loss: 0.1635 - accuracy: 0.9609 - val_loss: 0.0871 - val_accuracy: 0.9795 - 546ms/epoch - 2ms/step\n",
            "Epoch 62/1000\n",
            "250/250 - 1s - loss: 0.1437 - accuracy: 0.9680 - val_loss: 0.0837 - val_accuracy: 0.9775 - 523ms/epoch - 2ms/step\n",
            "Epoch 63/1000\n",
            "250/250 - 1s - loss: 0.1563 - accuracy: 0.9617 - val_loss: 0.0855 - val_accuracy: 0.9790 - 598ms/epoch - 2ms/step\n",
            "Epoch 64/1000\n",
            "250/250 - 1s - loss: 0.1424 - accuracy: 0.9654 - val_loss: 0.0859 - val_accuracy: 0.9820 - 619ms/epoch - 2ms/step\n",
            "Epoch 65/1000\n",
            "250/250 - 1s - loss: 0.1464 - accuracy: 0.9655 - val_loss: 0.0953 - val_accuracy: 0.9750 - 606ms/epoch - 2ms/step\n",
            "Epoch 66/1000\n",
            "250/250 - 1s - loss: 0.1424 - accuracy: 0.9665 - val_loss: 0.0982 - val_accuracy: 0.9760 - 618ms/epoch - 2ms/step\n",
            "Epoch 67/1000\n",
            "250/250 - 1s - loss: 0.1424 - accuracy: 0.9650 - val_loss: 0.0839 - val_accuracy: 0.9805 - 536ms/epoch - 2ms/step\n",
            "Epoch 68/1000\n",
            "250/250 - 1s - loss: 0.1453 - accuracy: 0.9653 - val_loss: 0.0948 - val_accuracy: 0.9760 - 634ms/epoch - 3ms/step\n",
            "Epoch 69/1000\n",
            "250/250 - 1s - loss: 0.1388 - accuracy: 0.9689 - val_loss: 0.0890 - val_accuracy: 0.9775 - 540ms/epoch - 2ms/step\n",
            "Epoch 70/1000\n",
            "250/250 - 1s - loss: 0.1441 - accuracy: 0.9634 - val_loss: 0.0836 - val_accuracy: 0.9785 - 600ms/epoch - 2ms/step\n",
            "Epoch 71/1000\n",
            "250/250 - 1s - loss: 0.1409 - accuracy: 0.9655 - val_loss: 0.0835 - val_accuracy: 0.9800 - 544ms/epoch - 2ms/step\n",
            "Epoch 72/1000\n",
            "250/250 - 1s - loss: 0.1407 - accuracy: 0.9684 - val_loss: 0.0812 - val_accuracy: 0.9795 - 606ms/epoch - 2ms/step\n",
            "Epoch 73/1000\n",
            "250/250 - 1s - loss: 0.1397 - accuracy: 0.9656 - val_loss: 0.0880 - val_accuracy: 0.9780 - 541ms/epoch - 2ms/step\n",
            "Epoch 74/1000\n",
            "250/250 - 1s - loss: 0.1394 - accuracy: 0.9660 - val_loss: 0.0823 - val_accuracy: 0.9795 - 522ms/epoch - 2ms/step\n",
            "Epoch 75/1000\n",
            "250/250 - 1s - loss: 0.1410 - accuracy: 0.9665 - val_loss: 0.0825 - val_accuracy: 0.9825 - 549ms/epoch - 2ms/step\n",
            "Epoch 76/1000\n",
            "250/250 - 1s - loss: 0.1398 - accuracy: 0.9654 - val_loss: 0.0809 - val_accuracy: 0.9800 - 598ms/epoch - 2ms/step\n",
            "Epoch 77/1000\n",
            "250/250 - 1s - loss: 0.1443 - accuracy: 0.9663 - val_loss: 0.0862 - val_accuracy: 0.9820 - 597ms/epoch - 2ms/step\n",
            "Epoch 78/1000\n",
            "250/250 - 1s - loss: 0.1489 - accuracy: 0.9650 - val_loss: 0.0940 - val_accuracy: 0.9725 - 625ms/epoch - 3ms/step\n",
            "Epoch 79/1000\n",
            "250/250 - 1s - loss: 0.1399 - accuracy: 0.9656 - val_loss: 0.0792 - val_accuracy: 0.9795 - 613ms/epoch - 2ms/step\n",
            "Epoch 80/1000\n",
            "250/250 - 1s - loss: 0.1488 - accuracy: 0.9628 - val_loss: 0.0842 - val_accuracy: 0.9795 - 614ms/epoch - 2ms/step\n",
            "Epoch 81/1000\n",
            "250/250 - 1s - loss: 0.1371 - accuracy: 0.9669 - val_loss: 0.1094 - val_accuracy: 0.9700 - 612ms/epoch - 2ms/step\n",
            "Epoch 82/1000\n",
            "250/250 - 1s - loss: 0.1407 - accuracy: 0.9664 - val_loss: 0.0999 - val_accuracy: 0.9760 - 532ms/epoch - 2ms/step\n",
            "Epoch 83/1000\n",
            "250/250 - 1s - loss: 0.1334 - accuracy: 0.9674 - val_loss: 0.0901 - val_accuracy: 0.9780 - 539ms/epoch - 2ms/step\n",
            "Epoch 84/1000\n",
            "250/250 - 1s - loss: 0.1291 - accuracy: 0.9693 - val_loss: 0.0898 - val_accuracy: 0.9750 - 605ms/epoch - 2ms/step\n",
            "Epoch 85/1000\n",
            "250/250 - 1s - loss: 0.1488 - accuracy: 0.9640 - val_loss: 0.0835 - val_accuracy: 0.9795 - 555ms/epoch - 2ms/step\n",
            "Epoch 86/1000\n",
            "250/250 - 1s - loss: 0.1525 - accuracy: 0.9609 - val_loss: 0.0809 - val_accuracy: 0.9815 - 623ms/epoch - 2ms/step\n",
            "Epoch 87/1000\n",
            "250/250 - 1s - loss: 0.1380 - accuracy: 0.9681 - val_loss: 0.0853 - val_accuracy: 0.9800 - 535ms/epoch - 2ms/step\n",
            "Epoch 88/1000\n",
            "250/250 - 1s - loss: 0.1359 - accuracy: 0.9701 - val_loss: 0.0808 - val_accuracy: 0.9755 - 530ms/epoch - 2ms/step\n",
            "Epoch 89/1000\n",
            "250/250 - 1s - loss: 0.1406 - accuracy: 0.9636 - val_loss: 0.0768 - val_accuracy: 0.9790 - 617ms/epoch - 2ms/step\n",
            "Epoch 90/1000\n",
            "250/250 - 1s - loss: 0.1405 - accuracy: 0.9664 - val_loss: 0.1276 - val_accuracy: 0.9655 - 548ms/epoch - 2ms/step\n",
            "Epoch 91/1000\n",
            "250/250 - 1s - loss: 0.1314 - accuracy: 0.9668 - val_loss: 0.0874 - val_accuracy: 0.9795 - 605ms/epoch - 2ms/step\n",
            "Epoch 92/1000\n",
            "250/250 - 1s - loss: 0.1315 - accuracy: 0.9690 - val_loss: 0.0827 - val_accuracy: 0.9810 - 533ms/epoch - 2ms/step\n",
            "Epoch 93/1000\n",
            "250/250 - 1s - loss: 0.1319 - accuracy: 0.9691 - val_loss: 0.0775 - val_accuracy: 0.9800 - 530ms/epoch - 2ms/step\n",
            "Epoch 94/1000\n",
            "250/250 - 1s - loss: 0.1266 - accuracy: 0.9715 - val_loss: 0.0789 - val_accuracy: 0.9815 - 622ms/epoch - 2ms/step\n",
            "Epoch 95/1000\n",
            "250/250 - 1s - loss: 0.1263 - accuracy: 0.9720 - val_loss: 0.0766 - val_accuracy: 0.9815 - 611ms/epoch - 2ms/step\n",
            "Epoch 96/1000\n",
            "250/250 - 1s - loss: 0.1247 - accuracy: 0.9705 - val_loss: 0.0778 - val_accuracy: 0.9795 - 537ms/epoch - 2ms/step\n",
            "Epoch 97/1000\n",
            "250/250 - 1s - loss: 0.1431 - accuracy: 0.9654 - val_loss: 0.0753 - val_accuracy: 0.9815 - 550ms/epoch - 2ms/step\n",
            "Epoch 98/1000\n",
            "250/250 - 1s - loss: 0.1338 - accuracy: 0.9690 - val_loss: 0.0759 - val_accuracy: 0.9805 - 605ms/epoch - 2ms/step\n",
            "Epoch 99/1000\n",
            "250/250 - 1s - loss: 0.1293 - accuracy: 0.9691 - val_loss: 0.0803 - val_accuracy: 0.9810 - 617ms/epoch - 2ms/step\n",
            "Epoch 100/1000\n",
            "250/250 - 1s - loss: 0.1407 - accuracy: 0.9659 - val_loss: 0.1100 - val_accuracy: 0.9720 - 531ms/epoch - 2ms/step\n",
            "Epoch 101/1000\n",
            "250/250 - 1s - loss: 0.1361 - accuracy: 0.9676 - val_loss: 0.0778 - val_accuracy: 0.9820 - 546ms/epoch - 2ms/step\n",
            "Epoch 102/1000\n",
            "250/250 - 1s - loss: 0.1256 - accuracy: 0.9716 - val_loss: 0.0790 - val_accuracy: 0.9810 - 544ms/epoch - 2ms/step\n",
            "Epoch 103/1000\n",
            "250/250 - 1s - loss: 0.1344 - accuracy: 0.9694 - val_loss: 0.0855 - val_accuracy: 0.9780 - 530ms/epoch - 2ms/step\n",
            "Epoch 104/1000\n",
            "250/250 - 1s - loss: 0.1318 - accuracy: 0.9699 - val_loss: 0.0755 - val_accuracy: 0.9805 - 608ms/epoch - 2ms/step\n",
            "Epoch 105/1000\n",
            "250/250 - 1s - loss: 0.1214 - accuracy: 0.9679 - val_loss: 0.0782 - val_accuracy: 0.9815 - 550ms/epoch - 2ms/step\n",
            "Epoch 106/1000\n",
            "250/250 - 1s - loss: 0.1471 - accuracy: 0.9641 - val_loss: 0.0772 - val_accuracy: 0.9815 - 540ms/epoch - 2ms/step\n",
            "Epoch 107/1000\n",
            "250/250 - 1s - loss: 0.1352 - accuracy: 0.9657 - val_loss: 0.0846 - val_accuracy: 0.9760 - 606ms/epoch - 2ms/step\n",
            "Epoch 108/1000\n",
            "250/250 - 1s - loss: 0.1362 - accuracy: 0.9672 - val_loss: 0.1050 - val_accuracy: 0.9705 - 550ms/epoch - 2ms/step\n",
            "Epoch 109/1000\n",
            "250/250 - 1s - loss: 0.1260 - accuracy: 0.9704 - val_loss: 0.0740 - val_accuracy: 0.9800 - 532ms/epoch - 2ms/step\n",
            "Epoch 110/1000\n",
            "250/250 - 1s - loss: 0.1249 - accuracy: 0.9704 - val_loss: 0.0749 - val_accuracy: 0.9800 - 537ms/epoch - 2ms/step\n",
            "Epoch 111/1000\n",
            "250/250 - 1s - loss: 0.1285 - accuracy: 0.9704 - val_loss: 0.0739 - val_accuracy: 0.9805 - 609ms/epoch - 2ms/step\n",
            "Epoch 112/1000\n",
            "250/250 - 1s - loss: 0.1193 - accuracy: 0.9719 - val_loss: 0.0787 - val_accuracy: 0.9790 - 539ms/epoch - 2ms/step\n",
            "Epoch 113/1000\n",
            "250/250 - 1s - loss: 0.1307 - accuracy: 0.9674 - val_loss: 0.0835 - val_accuracy: 0.9800 - 604ms/epoch - 2ms/step\n",
            "Epoch 114/1000\n",
            "250/250 - 1s - loss: 0.1250 - accuracy: 0.9700 - val_loss: 0.0849 - val_accuracy: 0.9800 - 530ms/epoch - 2ms/step\n",
            "Epoch 115/1000\n",
            "250/250 - 1s - loss: 0.1316 - accuracy: 0.9676 - val_loss: 0.0758 - val_accuracy: 0.9815 - 603ms/epoch - 2ms/step\n",
            "Epoch 116/1000\n",
            "250/250 - 1s - loss: 0.1249 - accuracy: 0.9696 - val_loss: 0.0764 - val_accuracy: 0.9790 - 602ms/epoch - 2ms/step\n",
            "Epoch 117/1000\n",
            "250/250 - 1s - loss: 0.1247 - accuracy: 0.9694 - val_loss: 0.0756 - val_accuracy: 0.9800 - 535ms/epoch - 2ms/step\n",
            "Epoch 118/1000\n",
            "250/250 - 1s - loss: 0.1367 - accuracy: 0.9655 - val_loss: 0.0787 - val_accuracy: 0.9810 - 605ms/epoch - 2ms/step\n",
            "Epoch 119/1000\n",
            "250/250 - 1s - loss: 0.1211 - accuracy: 0.9706 - val_loss: 0.0758 - val_accuracy: 0.9800 - 610ms/epoch - 2ms/step\n",
            "Epoch 120/1000\n",
            "250/250 - 1s - loss: 0.1218 - accuracy: 0.9689 - val_loss: 0.0736 - val_accuracy: 0.9835 - 623ms/epoch - 2ms/step\n",
            "Epoch 121/1000\n",
            "250/250 - 1s - loss: 0.1399 - accuracy: 0.9661 - val_loss: 0.0930 - val_accuracy: 0.9745 - 606ms/epoch - 2ms/step\n",
            "Epoch 122/1000\n",
            "250/250 - 1s - loss: 0.1395 - accuracy: 0.9668 - val_loss: 0.0783 - val_accuracy: 0.9825 - 531ms/epoch - 2ms/step\n",
            "Epoch 123/1000\n",
            "250/250 - 1s - loss: 0.1139 - accuracy: 0.9745 - val_loss: 0.0736 - val_accuracy: 0.9820 - 557ms/epoch - 2ms/step\n",
            "Epoch 124/1000\n",
            "250/250 - 1s - loss: 0.1279 - accuracy: 0.9699 - val_loss: 0.1006 - val_accuracy: 0.9755 - 537ms/epoch - 2ms/step\n",
            "Epoch 125/1000\n",
            "250/250 - 1s - loss: 0.1269 - accuracy: 0.9709 - val_loss: 0.0763 - val_accuracy: 0.9805 - 529ms/epoch - 2ms/step\n",
            "Epoch 126/1000\n",
            "250/250 - 1s - loss: 0.1331 - accuracy: 0.9659 - val_loss: 0.0774 - val_accuracy: 0.9820 - 611ms/epoch - 2ms/step\n",
            "Epoch 127/1000\n",
            "250/250 - 1s - loss: 0.1245 - accuracy: 0.9715 - val_loss: 0.0839 - val_accuracy: 0.9760 - 526ms/epoch - 2ms/step\n",
            "Epoch 128/1000\n",
            "250/250 - 1s - loss: 0.1223 - accuracy: 0.9724 - val_loss: 0.0731 - val_accuracy: 0.9830 - 621ms/epoch - 2ms/step\n",
            "Epoch 129/1000\n",
            "250/250 - 1s - loss: 0.1319 - accuracy: 0.9699 - val_loss: 0.0754 - val_accuracy: 0.9820 - 546ms/epoch - 2ms/step\n",
            "Epoch 130/1000\n",
            "250/250 - 1s - loss: 0.1227 - accuracy: 0.9699 - val_loss: 0.0782 - val_accuracy: 0.9800 - 597ms/epoch - 2ms/step\n",
            "Epoch 131/1000\n",
            "250/250 - 1s - loss: 0.1210 - accuracy: 0.9705 - val_loss: 0.0741 - val_accuracy: 0.9810 - 619ms/epoch - 2ms/step\n",
            "Epoch 132/1000\n",
            "250/250 - 1s - loss: 0.1153 - accuracy: 0.9732 - val_loss: 0.0731 - val_accuracy: 0.9800 - 599ms/epoch - 2ms/step\n",
            "Epoch 133/1000\n",
            "250/250 - 1s - loss: 0.1208 - accuracy: 0.9719 - val_loss: 0.0942 - val_accuracy: 0.9760 - 544ms/epoch - 2ms/step\n",
            "Epoch 134/1000\n",
            "250/250 - 1s - loss: 0.1206 - accuracy: 0.9700 - val_loss: 0.0720 - val_accuracy: 0.9830 - 602ms/epoch - 2ms/step\n",
            "Epoch 135/1000\n",
            "250/250 - 1s - loss: 0.1331 - accuracy: 0.9672 - val_loss: 0.0736 - val_accuracy: 0.9805 - 616ms/epoch - 2ms/step\n",
            "Epoch 136/1000\n",
            "250/250 - 1s - loss: 0.1255 - accuracy: 0.9701 - val_loss: 0.0739 - val_accuracy: 0.9795 - 537ms/epoch - 2ms/step\n",
            "Epoch 137/1000\n",
            "250/250 - 1s - loss: 0.1153 - accuracy: 0.9728 - val_loss: 0.0706 - val_accuracy: 0.9825 - 543ms/epoch - 2ms/step\n",
            "Epoch 138/1000\n",
            "250/250 - 1s - loss: 0.1348 - accuracy: 0.9693 - val_loss: 0.0804 - val_accuracy: 0.9805 - 550ms/epoch - 2ms/step\n",
            "Epoch 139/1000\n",
            "250/250 - 1s - loss: 0.1217 - accuracy: 0.9712 - val_loss: 0.0709 - val_accuracy: 0.9805 - 532ms/epoch - 2ms/step\n",
            "Epoch 140/1000\n",
            "250/250 - 1s - loss: 0.1168 - accuracy: 0.9734 - val_loss: 0.0744 - val_accuracy: 0.9820 - 542ms/epoch - 2ms/step\n",
            "Epoch 141/1000\n",
            "250/250 - 1s - loss: 0.1190 - accuracy: 0.9734 - val_loss: 0.0722 - val_accuracy: 0.9805 - 522ms/epoch - 2ms/step\n",
            "Epoch 142/1000\n",
            "250/250 - 1s - loss: 0.1297 - accuracy: 0.9691 - val_loss: 0.0811 - val_accuracy: 0.9780 - 654ms/epoch - 3ms/step\n",
            "Epoch 143/1000\n",
            "250/250 - 1s - loss: 0.1240 - accuracy: 0.9720 - val_loss: 0.0701 - val_accuracy: 0.9820 - 610ms/epoch - 2ms/step\n",
            "Epoch 144/1000\n",
            "250/250 - 1s - loss: 0.1213 - accuracy: 0.9718 - val_loss: 0.0721 - val_accuracy: 0.9805 - 554ms/epoch - 2ms/step\n",
            "Epoch 145/1000\n",
            "250/250 - 1s - loss: 0.1207 - accuracy: 0.9712 - val_loss: 0.0790 - val_accuracy: 0.9785 - 549ms/epoch - 2ms/step\n",
            "Epoch 146/1000\n",
            "250/250 - 1s - loss: 0.1225 - accuracy: 0.9701 - val_loss: 0.0814 - val_accuracy: 0.9780 - 542ms/epoch - 2ms/step\n",
            "Epoch 147/1000\n",
            "250/250 - 1s - loss: 0.1395 - accuracy: 0.9657 - val_loss: 0.0903 - val_accuracy: 0.9780 - 612ms/epoch - 2ms/step\n",
            "Epoch 148/1000\n",
            "250/250 - 1s - loss: 0.1179 - accuracy: 0.9716 - val_loss: 0.0701 - val_accuracy: 0.9830 - 609ms/epoch - 2ms/step\n",
            "Epoch 149/1000\n",
            "250/250 - 1s - loss: 0.1238 - accuracy: 0.9705 - val_loss: 0.0704 - val_accuracy: 0.9830 - 549ms/epoch - 2ms/step\n",
            "Epoch 150/1000\n",
            "250/250 - 1s - loss: 0.1220 - accuracy: 0.9709 - val_loss: 0.0720 - val_accuracy: 0.9815 - 608ms/epoch - 2ms/step\n",
            "Epoch 151/1000\n",
            "250/250 - 1s - loss: 0.1237 - accuracy: 0.9690 - val_loss: 0.0717 - val_accuracy: 0.9830 - 626ms/epoch - 3ms/step\n",
            "Epoch 152/1000\n",
            "250/250 - 1s - loss: 0.1173 - accuracy: 0.9700 - val_loss: 0.0694 - val_accuracy: 0.9815 - 551ms/epoch - 2ms/step\n",
            "Epoch 153/1000\n",
            "250/250 - 1s - loss: 0.1248 - accuracy: 0.9701 - val_loss: 0.0771 - val_accuracy: 0.9805 - 561ms/epoch - 2ms/step\n",
            "Epoch 154/1000\n",
            "250/250 - 1s - loss: 0.1144 - accuracy: 0.9707 - val_loss: 0.0762 - val_accuracy: 0.9795 - 554ms/epoch - 2ms/step\n",
            "Epoch 155/1000\n",
            "250/250 - 1s - loss: 0.1174 - accuracy: 0.9719 - val_loss: 0.0801 - val_accuracy: 0.9815 - 531ms/epoch - 2ms/step\n",
            "Epoch 156/1000\n",
            "250/250 - 1s - loss: 0.1332 - accuracy: 0.9657 - val_loss: 0.0857 - val_accuracy: 0.9810 - 616ms/epoch - 2ms/step\n",
            "Epoch 157/1000\n",
            "250/250 - 1s - loss: 0.1185 - accuracy: 0.9719 - val_loss: 0.0692 - val_accuracy: 0.9825 - 614ms/epoch - 2ms/step\n",
            "Epoch 158/1000\n",
            "250/250 - 1s - loss: 0.1211 - accuracy: 0.9715 - val_loss: 0.0841 - val_accuracy: 0.9810 - 605ms/epoch - 2ms/step\n",
            "Epoch 159/1000\n",
            "250/250 - 1s - loss: 0.1143 - accuracy: 0.9730 - val_loss: 0.0689 - val_accuracy: 0.9830 - 612ms/epoch - 2ms/step\n",
            "Epoch 160/1000\n",
            "250/250 - 1s - loss: 0.1223 - accuracy: 0.9706 - val_loss: 0.0748 - val_accuracy: 0.9795 - 569ms/epoch - 2ms/step\n",
            "Epoch 161/1000\n",
            "250/250 - 1s - loss: 0.1159 - accuracy: 0.9730 - val_loss: 0.0696 - val_accuracy: 0.9810 - 554ms/epoch - 2ms/step\n",
            "Epoch 162/1000\n",
            "250/250 - 1s - loss: 0.1244 - accuracy: 0.9701 - val_loss: 0.0746 - val_accuracy: 0.9835 - 526ms/epoch - 2ms/step\n",
            "Epoch 163/1000\n",
            "250/250 - 1s - loss: 0.1190 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9830 - 611ms/epoch - 2ms/step\n",
            "Epoch 164/1000\n",
            "250/250 - 1s - loss: 0.1330 - accuracy: 0.9670 - val_loss: 0.0706 - val_accuracy: 0.9840 - 1s/epoch - 4ms/step\n",
            "Epoch 165/1000\n",
            "250/250 - 1s - loss: 0.1156 - accuracy: 0.9739 - val_loss: 0.0698 - val_accuracy: 0.9825 - 630ms/epoch - 3ms/step\n",
            "Epoch 166/1000\n",
            "250/250 - 1s - loss: 0.1099 - accuracy: 0.9726 - val_loss: 0.0689 - val_accuracy: 0.9825 - 530ms/epoch - 2ms/step\n",
            "Epoch 167/1000\n",
            "250/250 - 1s - loss: 0.1193 - accuracy: 0.9699 - val_loss: 0.0742 - val_accuracy: 0.9815 - 631ms/epoch - 3ms/step\n",
            "Epoch 168/1000\n",
            "250/250 - 1s - loss: 0.1188 - accuracy: 0.9725 - val_loss: 0.0696 - val_accuracy: 0.9815 - 540ms/epoch - 2ms/step\n",
            "Epoch 169/1000\n",
            "250/250 - 1s - loss: 0.1125 - accuracy: 0.9732 - val_loss: 0.0705 - val_accuracy: 0.9815 - 541ms/epoch - 2ms/step\n",
            "Epoch 170/1000\n",
            "250/250 - 1s - loss: 0.1258 - accuracy: 0.9700 - val_loss: 0.0745 - val_accuracy: 0.9850 - 531ms/epoch - 2ms/step\n",
            "Epoch 171/1000\n",
            "250/250 - 1s - loss: 0.1199 - accuracy: 0.9714 - val_loss: 0.0744 - val_accuracy: 0.9805 - 618ms/epoch - 2ms/step\n",
            "Epoch 172/1000\n",
            "250/250 - 1s - loss: 0.1143 - accuracy: 0.9704 - val_loss: 0.0739 - val_accuracy: 0.9835 - 538ms/epoch - 2ms/step\n",
            "Epoch 173/1000\n",
            "250/250 - 1s - loss: 0.1244 - accuracy: 0.9697 - val_loss: 0.0720 - val_accuracy: 0.9820 - 540ms/epoch - 2ms/step\n",
            "Epoch 174/1000\n",
            "250/250 - 1s - loss: 0.1117 - accuracy: 0.9741 - val_loss: 0.0710 - val_accuracy: 0.9840 - 602ms/epoch - 2ms/step\n",
            "Epoch 175/1000\n",
            "250/250 - 1s - loss: 0.1179 - accuracy: 0.9725 - val_loss: 0.0706 - val_accuracy: 0.9840 - 599ms/epoch - 2ms/step\n",
            "Epoch 176/1000\n",
            "250/250 - 1s - loss: 0.1196 - accuracy: 0.9718 - val_loss: 0.0729 - val_accuracy: 0.9835 - 557ms/epoch - 2ms/step\n",
            "Epoch 177/1000\n",
            "250/250 - 1s - loss: 0.1142 - accuracy: 0.9732 - val_loss: 0.0689 - val_accuracy: 0.9825 - 602ms/epoch - 2ms/step\n",
            "Epoch 178/1000\n",
            "250/250 - 1s - loss: 0.1057 - accuracy: 0.9744 - val_loss: 0.0684 - val_accuracy: 0.9830 - 623ms/epoch - 2ms/step\n",
            "Epoch 179/1000\n",
            "250/250 - 1s - loss: 0.1159 - accuracy: 0.9728 - val_loss: 0.0706 - val_accuracy: 0.9845 - 605ms/epoch - 2ms/step\n",
            "Epoch 180/1000\n",
            "250/250 - 1s - loss: 0.1196 - accuracy: 0.9728 - val_loss: 0.0687 - val_accuracy: 0.9835 - 549ms/epoch - 2ms/step\n",
            "Epoch 181/1000\n",
            "250/250 - 1s - loss: 0.1116 - accuracy: 0.9721 - val_loss: 0.0716 - val_accuracy: 0.9785 - 598ms/epoch - 2ms/step\n",
            "Epoch 182/1000\n",
            "250/250 - 1s - loss: 0.1167 - accuracy: 0.9731 - val_loss: 0.0834 - val_accuracy: 0.9775 - 562ms/epoch - 2ms/step\n",
            "Epoch 183/1000\n",
            "250/250 - 1s - loss: 0.1211 - accuracy: 0.9705 - val_loss: 0.0745 - val_accuracy: 0.9825 - 680ms/epoch - 3ms/step\n",
            "Epoch 184/1000\n",
            "250/250 - 1s - loss: 0.1217 - accuracy: 0.9696 - val_loss: 0.0666 - val_accuracy: 0.9820 - 570ms/epoch - 2ms/step\n",
            "Epoch 185/1000\n",
            "250/250 - 1s - loss: 0.1138 - accuracy: 0.9716 - val_loss: 0.0668 - val_accuracy: 0.9810 - 638ms/epoch - 3ms/step\n",
            "Epoch 186/1000\n",
            "250/250 - 1s - loss: 0.1139 - accuracy: 0.9739 - val_loss: 0.0914 - val_accuracy: 0.9720 - 715ms/epoch - 3ms/step\n",
            "Epoch 187/1000\n",
            "250/250 - 1s - loss: 0.1211 - accuracy: 0.9691 - val_loss: 0.0704 - val_accuracy: 0.9820 - 557ms/epoch - 2ms/step\n",
            "Epoch 188/1000\n",
            "250/250 - 1s - loss: 0.1122 - accuracy: 0.9741 - val_loss: 0.0700 - val_accuracy: 0.9820 - 579ms/epoch - 2ms/step\n",
            "Epoch 189/1000\n",
            "250/250 - 1s - loss: 0.1099 - accuracy: 0.9744 - val_loss: 0.0730 - val_accuracy: 0.9830 - 520ms/epoch - 2ms/step\n",
            "Epoch 190/1000\n",
            "250/250 - 1s - loss: 0.1165 - accuracy: 0.9721 - val_loss: 0.0643 - val_accuracy: 0.9845 - 543ms/epoch - 2ms/step\n",
            "Epoch 191/1000\n",
            "250/250 - 1s - loss: 0.1143 - accuracy: 0.9725 - val_loss: 0.0662 - val_accuracy: 0.9835 - 527ms/epoch - 2ms/step\n",
            "Epoch 192/1000\n",
            "250/250 - 1s - loss: 0.1090 - accuracy: 0.9743 - val_loss: 0.0851 - val_accuracy: 0.9785 - 545ms/epoch - 2ms/step\n",
            "Epoch 193/1000\n",
            "250/250 - 1s - loss: 0.1173 - accuracy: 0.9719 - val_loss: 0.0702 - val_accuracy: 0.9835 - 609ms/epoch - 2ms/step\n",
            "Epoch 194/1000\n",
            "250/250 - 1s - loss: 0.1140 - accuracy: 0.9744 - val_loss: 0.0755 - val_accuracy: 0.9805 - 626ms/epoch - 3ms/step\n",
            "Epoch 195/1000\n",
            "250/250 - 1s - loss: 0.1216 - accuracy: 0.9714 - val_loss: 0.0747 - val_accuracy: 0.9835 - 574ms/epoch - 2ms/step\n",
            "Epoch 196/1000\n",
            "250/250 - 1s - loss: 0.1106 - accuracy: 0.9721 - val_loss: 0.0687 - val_accuracy: 0.9825 - 598ms/epoch - 2ms/step\n",
            "Epoch 197/1000\n",
            "250/250 - 1s - loss: 0.1123 - accuracy: 0.9734 - val_loss: 0.0701 - val_accuracy: 0.9830 - 638ms/epoch - 3ms/step\n",
            "Epoch 198/1000\n",
            "250/250 - 1s - loss: 0.1099 - accuracy: 0.9725 - val_loss: 0.0635 - val_accuracy: 0.9835 - 608ms/epoch - 2ms/step\n",
            "Epoch 199/1000\n",
            "250/250 - 1s - loss: 0.1148 - accuracy: 0.9739 - val_loss: 0.0702 - val_accuracy: 0.9815 - 544ms/epoch - 2ms/step\n",
            "Epoch 200/1000\n",
            "250/250 - 1s - loss: 0.1079 - accuracy: 0.9751 - val_loss: 0.0694 - val_accuracy: 0.9845 - 633ms/epoch - 3ms/step\n",
            "Epoch 201/1000\n",
            "250/250 - 1s - loss: 0.1089 - accuracy: 0.9753 - val_loss: 0.0664 - val_accuracy: 0.9835 - 604ms/epoch - 2ms/step\n",
            "Epoch 202/1000\n",
            "250/250 - 1s - loss: 0.1284 - accuracy: 0.9681 - val_loss: 0.1108 - val_accuracy: 0.9695 - 578ms/epoch - 2ms/step\n",
            "Epoch 203/1000\n",
            "250/250 - 1s - loss: 0.1183 - accuracy: 0.9718 - val_loss: 0.0668 - val_accuracy: 0.9835 - 536ms/epoch - 2ms/step\n",
            "Epoch 204/1000\n",
            "250/250 - 1s - loss: 0.1177 - accuracy: 0.9722 - val_loss: 0.0692 - val_accuracy: 0.9835 - 673ms/epoch - 3ms/step\n",
            "Epoch 205/1000\n",
            "250/250 - 1s - loss: 0.1121 - accuracy: 0.9728 - val_loss: 0.0671 - val_accuracy: 0.9845 - 552ms/epoch - 2ms/step\n",
            "Epoch 206/1000\n",
            "250/250 - 1s - loss: 0.1181 - accuracy: 0.9716 - val_loss: 0.0680 - val_accuracy: 0.9840 - 629ms/epoch - 3ms/step\n",
            "Epoch 207/1000\n",
            "250/250 - 1s - loss: 0.1107 - accuracy: 0.9758 - val_loss: 0.0711 - val_accuracy: 0.9830 - 545ms/epoch - 2ms/step\n",
            "Epoch 208/1000\n",
            "250/250 - 1s - loss: 0.1149 - accuracy: 0.9718 - val_loss: 0.0702 - val_accuracy: 0.9835 - 619ms/epoch - 2ms/step\n",
            "Epoch 209/1000\n",
            "250/250 - 1s - loss: 0.1196 - accuracy: 0.9706 - val_loss: 0.0729 - val_accuracy: 0.9830 - 633ms/epoch - 3ms/step\n",
            "Epoch 210/1000\n",
            "250/250 - 1s - loss: 0.1084 - accuracy: 0.9772 - val_loss: 0.0731 - val_accuracy: 0.9835 - 544ms/epoch - 2ms/step\n",
            "Epoch 211/1000\n",
            "250/250 - 1s - loss: 0.1032 - accuracy: 0.9753 - val_loss: 0.0754 - val_accuracy: 0.9795 - 579ms/epoch - 2ms/step\n",
            "Epoch 212/1000\n",
            "250/250 - 1s - loss: 0.1116 - accuracy: 0.9739 - val_loss: 0.0681 - val_accuracy: 0.9835 - 625ms/epoch - 2ms/step\n",
            "Epoch 213/1000\n",
            "250/250 - 1s - loss: 0.1107 - accuracy: 0.9724 - val_loss: 0.0761 - val_accuracy: 0.9825 - 573ms/epoch - 2ms/step\n",
            "Epoch 214/1000\n",
            "250/250 - 1s - loss: 0.1248 - accuracy: 0.9681 - val_loss: 0.0682 - val_accuracy: 0.9825 - 652ms/epoch - 3ms/step\n",
            "Epoch 215/1000\n",
            "250/250 - 1s - loss: 0.1067 - accuracy: 0.9746 - val_loss: 0.1265 - val_accuracy: 0.9665 - 765ms/epoch - 3ms/step\n",
            "Epoch 216/1000\n",
            "250/250 - 1s - loss: 0.1139 - accuracy: 0.9728 - val_loss: 0.0694 - val_accuracy: 0.9840 - 638ms/epoch - 3ms/step\n",
            "Epoch 217/1000\n",
            "250/250 - 1s - loss: 0.1084 - accuracy: 0.9737 - val_loss: 0.0701 - val_accuracy: 0.9825 - 699ms/epoch - 3ms/step\n",
            "Epoch 218/1000\n",
            "250/250 - 1s - loss: 0.1103 - accuracy: 0.9739 - val_loss: 0.0654 - val_accuracy: 0.9845 - 711ms/epoch - 3ms/step\n",
            "Epoch 219/1000\n",
            "250/250 - 1s - loss: 0.1163 - accuracy: 0.9719 - val_loss: 0.0697 - val_accuracy: 0.9830 - 768ms/epoch - 3ms/step\n",
            "Epoch 220/1000\n",
            "250/250 - 1s - loss: 0.1191 - accuracy: 0.9719 - val_loss: 0.0850 - val_accuracy: 0.9785 - 696ms/epoch - 3ms/step\n",
            "Epoch 221/1000\n",
            "250/250 - 1s - loss: 0.1076 - accuracy: 0.9731 - val_loss: 0.0723 - val_accuracy: 0.9850 - 649ms/epoch - 3ms/step\n",
            "Epoch 222/1000\n",
            "250/250 - 1s - loss: 0.1091 - accuracy: 0.9736 - val_loss: 0.0648 - val_accuracy: 0.9840 - 538ms/epoch - 2ms/step\n",
            "Epoch 223/1000\n",
            "250/250 - 1s - loss: 0.1044 - accuracy: 0.9741 - val_loss: 0.0694 - val_accuracy: 0.9840 - 632ms/epoch - 3ms/step\n",
            "Epoch 224/1000\n",
            "250/250 - 1s - loss: 0.1245 - accuracy: 0.9711 - val_loss: 0.0711 - val_accuracy: 0.9795 - 632ms/epoch - 3ms/step\n",
            "Epoch 225/1000\n",
            "250/250 - 1s - loss: 0.1214 - accuracy: 0.9718 - val_loss: 0.0687 - val_accuracy: 0.9825 - 594ms/epoch - 2ms/step\n",
            "Epoch 226/1000\n",
            "250/250 - 1s - loss: 0.1098 - accuracy: 0.9754 - val_loss: 0.0907 - val_accuracy: 0.9785 - 655ms/epoch - 3ms/step\n",
            "Epoch 227/1000\n",
            "250/250 - 1s - loss: 0.1099 - accuracy: 0.9744 - val_loss: 0.0720 - val_accuracy: 0.9810 - 643ms/epoch - 3ms/step\n",
            "Epoch 228/1000\n",
            "250/250 - 1s - loss: 0.1158 - accuracy: 0.9707 - val_loss: 0.0841 - val_accuracy: 0.9780 - 643ms/epoch - 3ms/step\n",
            "Epoch 229/1000\n",
            "250/250 - 1s - loss: 0.1195 - accuracy: 0.9718 - val_loss: 0.0715 - val_accuracy: 0.9790 - 576ms/epoch - 2ms/step\n",
            "Epoch 230/1000\n",
            "250/250 - 1s - loss: 0.1182 - accuracy: 0.9712 - val_loss: 0.0645 - val_accuracy: 0.9855 - 681ms/epoch - 3ms/step\n",
            "Epoch 231/1000\n",
            "250/250 - 1s - loss: 0.1152 - accuracy: 0.9741 - val_loss: 0.0649 - val_accuracy: 0.9850 - 678ms/epoch - 3ms/step\n",
            "Epoch 232/1000\n",
            "250/250 - 1s - loss: 0.1112 - accuracy: 0.9732 - val_loss: 0.0650 - val_accuracy: 0.9830 - 740ms/epoch - 3ms/step\n",
            "Epoch 233/1000\n",
            "250/250 - 1s - loss: 0.1094 - accuracy: 0.9737 - val_loss: 0.0685 - val_accuracy: 0.9835 - 580ms/epoch - 2ms/step\n",
            "Epoch 234/1000\n",
            "250/250 - 1s - loss: 0.1096 - accuracy: 0.9712 - val_loss: 0.0688 - val_accuracy: 0.9825 - 682ms/epoch - 3ms/step\n",
            "Epoch 235/1000\n",
            "250/250 - 1s - loss: 0.1093 - accuracy: 0.9753 - val_loss: 0.0653 - val_accuracy: 0.9835 - 673ms/epoch - 3ms/step\n",
            "Epoch 236/1000\n",
            "250/250 - 1s - loss: 0.1194 - accuracy: 0.9703 - val_loss: 0.0647 - val_accuracy: 0.9855 - 672ms/epoch - 3ms/step\n",
            "Epoch 237/1000\n",
            "250/250 - 1s - loss: 0.1094 - accuracy: 0.9734 - val_loss: 0.0689 - val_accuracy: 0.9845 - 622ms/epoch - 2ms/step\n",
            "Epoch 238/1000\n",
            "250/250 - 1s - loss: 0.1097 - accuracy: 0.9719 - val_loss: 0.0928 - val_accuracy: 0.9795 - 663ms/epoch - 3ms/step\n",
            "Epoch 239/1000\n",
            "250/250 - 1s - loss: 0.1177 - accuracy: 0.9735 - val_loss: 0.0622 - val_accuracy: 0.9845 - 627ms/epoch - 3ms/step\n",
            "Epoch 240/1000\n",
            "250/250 - 1s - loss: 0.1036 - accuracy: 0.9747 - val_loss: 0.0675 - val_accuracy: 0.9825 - 601ms/epoch - 2ms/step\n",
            "Epoch 241/1000\n",
            "250/250 - 1s - loss: 0.1128 - accuracy: 0.9731 - val_loss: 0.0648 - val_accuracy: 0.9815 - 638ms/epoch - 3ms/step\n",
            "Epoch 242/1000\n",
            "250/250 - 1s - loss: 0.1040 - accuracy: 0.9771 - val_loss: 0.0702 - val_accuracy: 0.9840 - 551ms/epoch - 2ms/step\n",
            "Epoch 243/1000\n",
            "250/250 - 1s - loss: 0.1080 - accuracy: 0.9745 - val_loss: 0.0615 - val_accuracy: 0.9845 - 626ms/epoch - 3ms/step\n",
            "Epoch 244/1000\n",
            "250/250 - 1s - loss: 0.1020 - accuracy: 0.9747 - val_loss: 0.0648 - val_accuracy: 0.9835 - 623ms/epoch - 2ms/step\n",
            "Epoch 245/1000\n",
            "250/250 - 1s - loss: 0.1040 - accuracy: 0.9747 - val_loss: 0.0724 - val_accuracy: 0.9805 - 1s/epoch - 5ms/step\n",
            "Epoch 246/1000\n",
            "250/250 - 1s - loss: 0.1137 - accuracy: 0.9725 - val_loss: 0.0850 - val_accuracy: 0.9795 - 1s/epoch - 4ms/step\n",
            "Epoch 247/1000\n",
            "250/250 - 1s - loss: 0.1129 - accuracy: 0.9712 - val_loss: 0.0649 - val_accuracy: 0.9860 - 679ms/epoch - 3ms/step\n",
            "Epoch 248/1000\n",
            "250/250 - 1s - loss: 0.1097 - accuracy: 0.9735 - val_loss: 0.0675 - val_accuracy: 0.9855 - 647ms/epoch - 3ms/step\n",
            "Epoch 249/1000\n",
            "250/250 - 1s - loss: 0.1199 - accuracy: 0.9696 - val_loss: 0.0638 - val_accuracy: 0.9850 - 568ms/epoch - 2ms/step\n",
            "Epoch 250/1000\n",
            "250/250 - 1s - loss: 0.1057 - accuracy: 0.9749 - val_loss: 0.0674 - val_accuracy: 0.9855 - 588ms/epoch - 2ms/step\n",
            "Epoch 251/1000\n",
            "250/250 - 1s - loss: 0.1223 - accuracy: 0.9719 - val_loss: 0.0621 - val_accuracy: 0.9850 - 524ms/epoch - 2ms/step\n",
            "Epoch 252/1000\n",
            "250/250 - 1s - loss: 0.0992 - accuracy: 0.9758 - val_loss: 0.0729 - val_accuracy: 0.9850 - 604ms/epoch - 2ms/step\n",
            "Epoch 253/1000\n",
            "250/250 - 1s - loss: 0.1136 - accuracy: 0.9714 - val_loss: 0.0631 - val_accuracy: 0.9855 - 584ms/epoch - 2ms/step\n",
            "Epoch 254/1000\n",
            "250/250 - 1s - loss: 0.1043 - accuracy: 0.9760 - val_loss: 0.0634 - val_accuracy: 0.9840 - 635ms/epoch - 3ms/step\n",
            "Epoch 255/1000\n",
            "250/250 - 1s - loss: 0.1022 - accuracy: 0.9737 - val_loss: 0.0626 - val_accuracy: 0.9855 - 582ms/epoch - 2ms/step\n",
            "Epoch 256/1000\n",
            "250/250 - 1s - loss: 0.1058 - accuracy: 0.9760 - val_loss: 0.0597 - val_accuracy: 0.9835 - 557ms/epoch - 2ms/step\n",
            "Epoch 257/1000\n",
            "250/250 - 1s - loss: 0.1097 - accuracy: 0.9732 - val_loss: 0.0637 - val_accuracy: 0.9840 - 559ms/epoch - 2ms/step\n",
            "Epoch 258/1000\n",
            "250/250 - 1s - loss: 0.1022 - accuracy: 0.9755 - val_loss: 0.0621 - val_accuracy: 0.9835 - 587ms/epoch - 2ms/step\n",
            "Epoch 259/1000\n",
            "250/250 - 1s - loss: 0.1131 - accuracy: 0.9739 - val_loss: 0.0659 - val_accuracy: 0.9855 - 553ms/epoch - 2ms/step\n",
            "Epoch 260/1000\n",
            "250/250 - 1s - loss: 0.1090 - accuracy: 0.9737 - val_loss: 0.0690 - val_accuracy: 0.9840 - 638ms/epoch - 3ms/step\n",
            "Epoch 261/1000\n",
            "250/250 - 1s - loss: 0.1125 - accuracy: 0.9729 - val_loss: 0.0727 - val_accuracy: 0.9810 - 640ms/epoch - 3ms/step\n",
            "Epoch 262/1000\n",
            "250/250 - 1s - loss: 0.1174 - accuracy: 0.9711 - val_loss: 0.0664 - val_accuracy: 0.9845 - 690ms/epoch - 3ms/step\n",
            "Epoch 263/1000\n",
            "250/250 - 1s - loss: 0.1101 - accuracy: 0.9741 - val_loss: 0.0629 - val_accuracy: 0.9850 - 641ms/epoch - 3ms/step\n",
            "Epoch 264/1000\n",
            "250/250 - 1s - loss: 0.1074 - accuracy: 0.9740 - val_loss: 0.0669 - val_accuracy: 0.9850 - 707ms/epoch - 3ms/step\n",
            "Epoch 265/1000\n",
            "250/250 - 1s - loss: 0.1061 - accuracy: 0.9758 - val_loss: 0.0614 - val_accuracy: 0.9855 - 597ms/epoch - 2ms/step\n",
            "Epoch 266/1000\n",
            "250/250 - 1s - loss: 0.0996 - accuracy: 0.9787 - val_loss: 0.0653 - val_accuracy: 0.9840 - 613ms/epoch - 2ms/step\n",
            "Epoch 267/1000\n",
            "250/250 - 1s - loss: 0.1026 - accuracy: 0.9749 - val_loss: 0.0686 - val_accuracy: 0.9850 - 551ms/epoch - 2ms/step\n",
            "Epoch 268/1000\n",
            "250/250 - 1s - loss: 0.1139 - accuracy: 0.9706 - val_loss: 0.0649 - val_accuracy: 0.9830 - 538ms/epoch - 2ms/step\n",
            "Epoch 269/1000\n",
            "250/250 - 1s - loss: 0.1026 - accuracy: 0.9749 - val_loss: 0.0661 - val_accuracy: 0.9840 - 638ms/epoch - 3ms/step\n",
            "Epoch 270/1000\n",
            "250/250 - 1s - loss: 0.1102 - accuracy: 0.9736 - val_loss: 0.0621 - val_accuracy: 0.9855 - 625ms/epoch - 2ms/step\n",
            "Epoch 271/1000\n",
            "250/250 - 1s - loss: 0.0974 - accuracy: 0.9778 - val_loss: 0.0670 - val_accuracy: 0.9850 - 605ms/epoch - 2ms/step\n",
            "Epoch 272/1000\n",
            "250/250 - 1s - loss: 0.1102 - accuracy: 0.9737 - val_loss: 0.0658 - val_accuracy: 0.9845 - 564ms/epoch - 2ms/step\n",
            "Epoch 273/1000\n",
            "250/250 - 1s - loss: 0.1122 - accuracy: 0.9729 - val_loss: 0.0654 - val_accuracy: 0.9845 - 700ms/epoch - 3ms/step\n",
            "Epoch 274/1000\n",
            "250/250 - 1s - loss: 0.1049 - accuracy: 0.9744 - val_loss: 0.0879 - val_accuracy: 0.9785 - 745ms/epoch - 3ms/step\n",
            "Epoch 275/1000\n",
            "250/250 - 1s - loss: 0.0993 - accuracy: 0.9754 - val_loss: 0.0690 - val_accuracy: 0.9825 - 708ms/epoch - 3ms/step\n",
            "Epoch 276/1000\n",
            "250/250 - 1s - loss: 0.0980 - accuracy: 0.9758 - val_loss: 0.0622 - val_accuracy: 0.9840 - 571ms/epoch - 2ms/step\n",
            "Epoch 277/1000\n",
            "250/250 - 1s - loss: 0.1055 - accuracy: 0.9749 - val_loss: 0.0641 - val_accuracy: 0.9835 - 607ms/epoch - 2ms/step\n",
            "Epoch 278/1000\n",
            "250/250 - 1s - loss: 0.1117 - accuracy: 0.9722 - val_loss: 0.0655 - val_accuracy: 0.9830 - 615ms/epoch - 2ms/step\n",
            "Epoch 279/1000\n",
            "250/250 - 1s - loss: 0.1057 - accuracy: 0.9746 - val_loss: 0.0726 - val_accuracy: 0.9815 - 531ms/epoch - 2ms/step\n",
            "Epoch 280/1000\n",
            "250/250 - 1s - loss: 0.1035 - accuracy: 0.9758 - val_loss: 0.0767 - val_accuracy: 0.9810 - 554ms/epoch - 2ms/step\n",
            "Epoch 281/1000\n",
            "250/250 - 1s - loss: 0.1018 - accuracy: 0.9754 - val_loss: 0.0616 - val_accuracy: 0.9845 - 614ms/epoch - 2ms/step\n",
            "Epoch 282/1000\n",
            "250/250 - 1s - loss: 0.1179 - accuracy: 0.9712 - val_loss: 0.0614 - val_accuracy: 0.9840 - 630ms/epoch - 3ms/step\n",
            "Epoch 283/1000\n",
            "250/250 - 1s - loss: 0.1038 - accuracy: 0.9743 - val_loss: 0.0623 - val_accuracy: 0.9840 - 623ms/epoch - 2ms/step\n",
            "Epoch 284/1000\n",
            "250/250 - 1s - loss: 0.1015 - accuracy: 0.9758 - val_loss: 0.0610 - val_accuracy: 0.9840 - 626ms/epoch - 3ms/step\n",
            "Epoch 285/1000\n",
            "250/250 - 1s - loss: 0.1006 - accuracy: 0.9761 - val_loss: 0.0638 - val_accuracy: 0.9840 - 665ms/epoch - 3ms/step\n",
            "Epoch 286/1000\n",
            "250/250 - 1s - loss: 0.1036 - accuracy: 0.9746 - val_loss: 0.0625 - val_accuracy: 0.9865 - 546ms/epoch - 2ms/step\n",
            "Epoch 287/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9762 - val_loss: 0.0611 - val_accuracy: 0.9840 - 617ms/epoch - 2ms/step\n",
            "Epoch 288/1000\n",
            "250/250 - 1s - loss: 0.1238 - accuracy: 0.9712 - val_loss: 0.0659 - val_accuracy: 0.9835 - 539ms/epoch - 2ms/step\n",
            "Epoch 289/1000\n",
            "250/250 - 1s - loss: 0.1044 - accuracy: 0.9764 - val_loss: 0.0616 - val_accuracy: 0.9840 - 612ms/epoch - 2ms/step\n",
            "Epoch 290/1000\n",
            "250/250 - 1s - loss: 0.0946 - accuracy: 0.9780 - val_loss: 0.0615 - val_accuracy: 0.9840 - 615ms/epoch - 2ms/step\n",
            "Epoch 291/1000\n",
            "250/250 - 1s - loss: 0.1055 - accuracy: 0.9732 - val_loss: 0.0596 - val_accuracy: 0.9850 - 669ms/epoch - 3ms/step\n",
            "Epoch 292/1000\n",
            "250/250 - 1s - loss: 0.0957 - accuracy: 0.9781 - val_loss: 0.0615 - val_accuracy: 0.9845 - 652ms/epoch - 3ms/step\n",
            "Epoch 293/1000\n",
            "250/250 - 1s - loss: 0.1033 - accuracy: 0.9754 - val_loss: 0.0780 - val_accuracy: 0.9800 - 633ms/epoch - 3ms/step\n",
            "Epoch 294/1000\n",
            "250/250 - 1s - loss: 0.1041 - accuracy: 0.9761 - val_loss: 0.0633 - val_accuracy: 0.9825 - 606ms/epoch - 2ms/step\n",
            "Epoch 295/1000\n",
            "250/250 - 1s - loss: 0.1142 - accuracy: 0.9724 - val_loss: 0.0606 - val_accuracy: 0.9845 - 538ms/epoch - 2ms/step\n",
            "Epoch 296/1000\n",
            "250/250 - 1s - loss: 0.1074 - accuracy: 0.9730 - val_loss: 0.0921 - val_accuracy: 0.9740 - 533ms/epoch - 2ms/step\n",
            "Epoch 297/1000\n",
            "250/250 - 1s - loss: 0.1119 - accuracy: 0.9720 - val_loss: 0.0732 - val_accuracy: 0.9825 - 615ms/epoch - 2ms/step\n",
            "Epoch 298/1000\n",
            "250/250 - 1s - loss: 0.1047 - accuracy: 0.9754 - val_loss: 0.0576 - val_accuracy: 0.9845 - 623ms/epoch - 2ms/step\n",
            "Epoch 299/1000\n",
            "250/250 - 1s - loss: 0.0924 - accuracy: 0.9775 - val_loss: 0.0611 - val_accuracy: 0.9850 - 664ms/epoch - 3ms/step\n",
            "Epoch 300/1000\n",
            "250/250 - 1s - loss: 0.1004 - accuracy: 0.9747 - val_loss: 0.0637 - val_accuracy: 0.9850 - 642ms/epoch - 3ms/step\n",
            "Epoch 301/1000\n",
            "250/250 - 1s - loss: 0.1065 - accuracy: 0.9747 - val_loss: 0.0680 - val_accuracy: 0.9840 - 671ms/epoch - 3ms/step\n",
            "Epoch 302/1000\n",
            "250/250 - 1s - loss: 0.1041 - accuracy: 0.9754 - val_loss: 0.0643 - val_accuracy: 0.9820 - 621ms/epoch - 2ms/step\n",
            "Epoch 303/1000\n",
            "250/250 - 1s - loss: 0.1095 - accuracy: 0.9729 - val_loss: 0.0642 - val_accuracy: 0.9835 - 621ms/epoch - 2ms/step\n",
            "Epoch 304/1000\n",
            "250/250 - 1s - loss: 0.0988 - accuracy: 0.9766 - val_loss: 0.0621 - val_accuracy: 0.9845 - 604ms/epoch - 2ms/step\n",
            "Epoch 305/1000\n",
            "250/250 - 1s - loss: 0.1038 - accuracy: 0.9750 - val_loss: 0.0623 - val_accuracy: 0.9840 - 611ms/epoch - 2ms/step\n",
            "Epoch 306/1000\n",
            "250/250 - 1s - loss: 0.0945 - accuracy: 0.9775 - val_loss: 0.0635 - val_accuracy: 0.9835 - 609ms/epoch - 2ms/step\n",
            "Epoch 307/1000\n",
            "250/250 - 1s - loss: 0.0915 - accuracy: 0.9776 - val_loss: 0.0628 - val_accuracy: 0.9850 - 611ms/epoch - 2ms/step\n",
            "Epoch 308/1000\n",
            "250/250 - 1s - loss: 0.0928 - accuracy: 0.9779 - val_loss: 0.0736 - val_accuracy: 0.9805 - 569ms/epoch - 2ms/step\n",
            "Epoch 309/1000\n",
            "250/250 - 1s - loss: 0.1021 - accuracy: 0.9764 - val_loss: 0.0641 - val_accuracy: 0.9860 - 520ms/epoch - 2ms/step\n",
            "Epoch 310/1000\n",
            "250/250 - 1s - loss: 0.1029 - accuracy: 0.9759 - val_loss: 0.0731 - val_accuracy: 0.9810 - 543ms/epoch - 2ms/step\n",
            "Epoch 311/1000\n",
            "250/250 - 1s - loss: 0.1201 - accuracy: 0.9695 - val_loss: 0.1000 - val_accuracy: 0.9760 - 604ms/epoch - 2ms/step\n",
            "Epoch 312/1000\n",
            "250/250 - 1s - loss: 0.1039 - accuracy: 0.9751 - val_loss: 0.0653 - val_accuracy: 0.9845 - 620ms/epoch - 2ms/step\n",
            "Epoch 313/1000\n",
            "250/250 - 1s - loss: 0.1073 - accuracy: 0.9715 - val_loss: 0.0668 - val_accuracy: 0.9815 - 550ms/epoch - 2ms/step\n",
            "Epoch 314/1000\n",
            "250/250 - 1s - loss: 0.1079 - accuracy: 0.9722 - val_loss: 0.0635 - val_accuracy: 0.9855 - 530ms/epoch - 2ms/step\n",
            "Epoch 315/1000\n",
            "250/250 - 1s - loss: 0.0951 - accuracy: 0.9760 - val_loss: 0.0639 - val_accuracy: 0.9845 - 653ms/epoch - 3ms/step\n",
            "Epoch 316/1000\n",
            "250/250 - 1s - loss: 0.0971 - accuracy: 0.9780 - val_loss: 0.0605 - val_accuracy: 0.9840 - 521ms/epoch - 2ms/step\n",
            "Epoch 317/1000\n",
            "250/250 - 1s - loss: 0.1110 - accuracy: 0.9726 - val_loss: 0.0610 - val_accuracy: 0.9850 - 605ms/epoch - 2ms/step\n",
            "Epoch 318/1000\n",
            "250/250 - 1s - loss: 0.0985 - accuracy: 0.9779 - val_loss: 0.0600 - val_accuracy: 0.9850 - 536ms/epoch - 2ms/step\n",
            "Epoch 319/1000\n",
            "250/250 - 1s - loss: 0.0952 - accuracy: 0.9769 - val_loss: 0.0633 - val_accuracy: 0.9840 - 535ms/epoch - 2ms/step\n",
            "Epoch 320/1000\n",
            "250/250 - 1s - loss: 0.1014 - accuracy: 0.9751 - val_loss: 0.0769 - val_accuracy: 0.9800 - 536ms/epoch - 2ms/step\n",
            "Epoch 321/1000\n",
            "250/250 - 1s - loss: 0.1062 - accuracy: 0.9744 - val_loss: 0.0647 - val_accuracy: 0.9840 - 564ms/epoch - 2ms/step\n",
            "Epoch 322/1000\n",
            "250/250 - 1s - loss: 0.1014 - accuracy: 0.9764 - val_loss: 0.0604 - val_accuracy: 0.9845 - 560ms/epoch - 2ms/step\n",
            "Epoch 323/1000\n",
            "250/250 - 1s - loss: 0.1056 - accuracy: 0.9762 - val_loss: 0.0610 - val_accuracy: 0.9855 - 619ms/epoch - 2ms/step\n",
            "Epoch 324/1000\n",
            "250/250 - 1s - loss: 0.0934 - accuracy: 0.9775 - val_loss: 0.0597 - val_accuracy: 0.9845 - 540ms/epoch - 2ms/step\n",
            "Epoch 325/1000\n",
            "250/250 - 1s - loss: 0.1031 - accuracy: 0.9747 - val_loss: 0.0673 - val_accuracy: 0.9825 - 611ms/epoch - 2ms/step\n",
            "Epoch 326/1000\n",
            "250/250 - 1s - loss: 0.1056 - accuracy: 0.9745 - val_loss: 0.0639 - val_accuracy: 0.9855 - 537ms/epoch - 2ms/step\n",
            "Epoch 327/1000\n",
            "250/250 - 1s - loss: 0.1012 - accuracy: 0.9744 - val_loss: 0.0617 - val_accuracy: 0.9840 - 606ms/epoch - 2ms/step\n",
            "Epoch 328/1000\n",
            "250/250 - 1s - loss: 0.1029 - accuracy: 0.9761 - val_loss: 0.0645 - val_accuracy: 0.9860 - 617ms/epoch - 2ms/step\n",
            "Epoch 329/1000\n",
            "250/250 - 1s - loss: 0.0971 - accuracy: 0.9783 - val_loss: 0.0616 - val_accuracy: 0.9850 - 621ms/epoch - 2ms/step\n",
            "Epoch 330/1000\n",
            "250/250 - 1s - loss: 0.1056 - accuracy: 0.9758 - val_loss: 0.0619 - val_accuracy: 0.9850 - 631ms/epoch - 3ms/step\n",
            "Epoch 331/1000\n",
            "250/250 - 1s - loss: 0.1013 - accuracy: 0.9764 - val_loss: 0.0591 - val_accuracy: 0.9845 - 545ms/epoch - 2ms/step\n",
            "Epoch 332/1000\n",
            "250/250 - 1s - loss: 0.1032 - accuracy: 0.9739 - val_loss: 0.1012 - val_accuracy: 0.9710 - 536ms/epoch - 2ms/step\n",
            "Epoch 333/1000\n",
            "250/250 - 1s - loss: 0.1035 - accuracy: 0.9753 - val_loss: 0.0582 - val_accuracy: 0.9850 - 632ms/epoch - 3ms/step\n",
            "Epoch 334/1000\n",
            "250/250 - 1s - loss: 0.1074 - accuracy: 0.9725 - val_loss: 0.0661 - val_accuracy: 0.9840 - 537ms/epoch - 2ms/step\n",
            "Epoch 335/1000\n",
            "250/250 - 1s - loss: 0.1038 - accuracy: 0.9746 - val_loss: 0.0592 - val_accuracy: 0.9850 - 605ms/epoch - 2ms/step\n",
            "Epoch 336/1000\n",
            "250/250 - 1s - loss: 0.0990 - accuracy: 0.9753 - val_loss: 0.0626 - val_accuracy: 0.9845 - 615ms/epoch - 2ms/step\n",
            "Epoch 337/1000\n",
            "250/250 - 1s - loss: 0.0946 - accuracy: 0.9783 - val_loss: 0.0605 - val_accuracy: 0.9840 - 602ms/epoch - 2ms/step\n",
            "Epoch 338/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9781 - val_loss: 0.0602 - val_accuracy: 0.9845 - 619ms/epoch - 2ms/step\n",
            "Epoch 339/1000\n",
            "250/250 - 1s - loss: 0.0975 - accuracy: 0.9772 - val_loss: 0.0598 - val_accuracy: 0.9835 - 663ms/epoch - 3ms/step\n",
            "Epoch 340/1000\n",
            "250/250 - 1s - loss: 0.0984 - accuracy: 0.9776 - val_loss: 0.0738 - val_accuracy: 0.9815 - 689ms/epoch - 3ms/step\n",
            "Epoch 341/1000\n",
            "250/250 - 1s - loss: 0.0988 - accuracy: 0.9758 - val_loss: 0.0713 - val_accuracy: 0.9815 - 750ms/epoch - 3ms/step\n",
            "Epoch 342/1000\n",
            "250/250 - 1s - loss: 0.1006 - accuracy: 0.9764 - val_loss: 0.0611 - val_accuracy: 0.9845 - 562ms/epoch - 2ms/step\n",
            "Epoch 343/1000\n",
            "250/250 - 1s - loss: 0.0948 - accuracy: 0.9769 - val_loss: 0.0615 - val_accuracy: 0.9845 - 530ms/epoch - 2ms/step\n",
            "Epoch 344/1000\n",
            "250/250 - 1s - loss: 0.0920 - accuracy: 0.9790 - val_loss: 0.0574 - val_accuracy: 0.9845 - 546ms/epoch - 2ms/step\n",
            "Epoch 345/1000\n",
            "250/250 - 1s - loss: 0.1010 - accuracy: 0.9749 - val_loss: 0.0592 - val_accuracy: 0.9845 - 528ms/epoch - 2ms/step\n",
            "Epoch 346/1000\n",
            "250/250 - 1s - loss: 0.1018 - accuracy: 0.9765 - val_loss: 0.0636 - val_accuracy: 0.9840 - 620ms/epoch - 2ms/step\n",
            "Epoch 347/1000\n",
            "250/250 - 1s - loss: 0.0893 - accuracy: 0.9794 - val_loss: 0.0627 - val_accuracy: 0.9845 - 600ms/epoch - 2ms/step\n",
            "Epoch 348/1000\n",
            "250/250 - 1s - loss: 0.1018 - accuracy: 0.9754 - val_loss: 0.0884 - val_accuracy: 0.9725 - 534ms/epoch - 2ms/step\n",
            "Epoch 349/1000\n",
            "250/250 - 1s - loss: 0.1035 - accuracy: 0.9753 - val_loss: 0.0623 - val_accuracy: 0.9855 - 609ms/epoch - 2ms/step\n",
            "Epoch 350/1000\n",
            "250/250 - 1s - loss: 0.1012 - accuracy: 0.9755 - val_loss: 0.0640 - val_accuracy: 0.9835 - 552ms/epoch - 2ms/step\n",
            "Epoch 351/1000\n",
            "250/250 - 1s - loss: 0.0895 - accuracy: 0.9796 - val_loss: 0.0604 - val_accuracy: 0.9840 - 605ms/epoch - 2ms/step\n",
            "Epoch 352/1000\n",
            "250/250 - 1s - loss: 0.1027 - accuracy: 0.9756 - val_loss: 0.0598 - val_accuracy: 0.9825 - 628ms/epoch - 3ms/step\n",
            "Epoch 353/1000\n",
            "250/250 - 1s - loss: 0.0919 - accuracy: 0.9771 - val_loss: 0.0621 - val_accuracy: 0.9840 - 613ms/epoch - 2ms/step\n",
            "Epoch 354/1000\n",
            "250/250 - 1s - loss: 0.1014 - accuracy: 0.9766 - val_loss: 0.0624 - val_accuracy: 0.9840 - 536ms/epoch - 2ms/step\n",
            "Epoch 355/1000\n",
            "250/250 - 1s - loss: 0.0996 - accuracy: 0.9771 - val_loss: 0.0562 - val_accuracy: 0.9845 - 548ms/epoch - 2ms/step\n",
            "Epoch 356/1000\n",
            "250/250 - 1s - loss: 0.0892 - accuracy: 0.9791 - val_loss: 0.0611 - val_accuracy: 0.9845 - 596ms/epoch - 2ms/step\n",
            "Epoch 357/1000\n",
            "250/250 - 1s - loss: 0.0973 - accuracy: 0.9781 - val_loss: 0.0633 - val_accuracy: 0.9835 - 560ms/epoch - 2ms/step\n",
            "Epoch 358/1000\n",
            "250/250 - 1s - loss: 0.1027 - accuracy: 0.9772 - val_loss: 0.0576 - val_accuracy: 0.9835 - 610ms/epoch - 2ms/step\n",
            "Epoch 359/1000\n",
            "250/250 - 1s - loss: 0.0981 - accuracy: 0.9762 - val_loss: 0.0695 - val_accuracy: 0.9815 - 619ms/epoch - 2ms/step\n",
            "Epoch 360/1000\n",
            "250/250 - 1s - loss: 0.0977 - accuracy: 0.9770 - val_loss: 0.0589 - val_accuracy: 0.9855 - 613ms/epoch - 2ms/step\n",
            "Epoch 361/1000\n",
            "250/250 - 1s - loss: 0.0910 - accuracy: 0.9793 - val_loss: 0.0580 - val_accuracy: 0.9840 - 532ms/epoch - 2ms/step\n",
            "Epoch 362/1000\n",
            "250/250 - 1s - loss: 0.1046 - accuracy: 0.9745 - val_loss: 0.0697 - val_accuracy: 0.9825 - 623ms/epoch - 2ms/step\n",
            "Epoch 363/1000\n",
            "250/250 - 1s - loss: 0.0934 - accuracy: 0.9756 - val_loss: 0.0626 - val_accuracy: 0.9840 - 599ms/epoch - 2ms/step\n",
            "Epoch 364/1000\n",
            "250/250 - 1s - loss: 0.0988 - accuracy: 0.9766 - val_loss: 0.0638 - val_accuracy: 0.9830 - 524ms/epoch - 2ms/step\n",
            "Epoch 365/1000\n",
            "250/250 - 1s - loss: 0.0922 - accuracy: 0.9778 - val_loss: 0.0618 - val_accuracy: 0.9845 - 557ms/epoch - 2ms/step\n",
            "Epoch 366/1000\n",
            "250/250 - 1s - loss: 0.0930 - accuracy: 0.9783 - val_loss: 0.0622 - val_accuracy: 0.9845 - 607ms/epoch - 2ms/step\n",
            "Epoch 367/1000\n",
            "250/250 - 1s - loss: 0.0960 - accuracy: 0.9780 - val_loss: 0.0673 - val_accuracy: 0.9820 - 621ms/epoch - 2ms/step\n",
            "Epoch 368/1000\n",
            "250/250 - 1s - loss: 0.0959 - accuracy: 0.9775 - val_loss: 0.0579 - val_accuracy: 0.9835 - 525ms/epoch - 2ms/step\n",
            "Epoch 369/1000\n",
            "250/250 - 1s - loss: 0.1041 - accuracy: 0.9759 - val_loss: 0.0597 - val_accuracy: 0.9840 - 544ms/epoch - 2ms/step\n",
            "Epoch 370/1000\n",
            "250/250 - 1s - loss: 0.0936 - accuracy: 0.9781 - val_loss: 0.0575 - val_accuracy: 0.9835 - 604ms/epoch - 2ms/step\n",
            "Epoch 371/1000\n",
            "250/250 - 1s - loss: 0.1010 - accuracy: 0.9778 - val_loss: 0.0568 - val_accuracy: 0.9835 - 613ms/epoch - 2ms/step\n",
            "Epoch 372/1000\n",
            "250/250 - 1s - loss: 0.0939 - accuracy: 0.9781 - val_loss: 0.0640 - val_accuracy: 0.9850 - 560ms/epoch - 2ms/step\n",
            "Epoch 373/1000\n",
            "250/250 - 1s - loss: 0.0990 - accuracy: 0.9755 - val_loss: 0.0605 - val_accuracy: 0.9835 - 536ms/epoch - 2ms/step\n",
            "Epoch 374/1000\n",
            "250/250 - 1s - loss: 0.0962 - accuracy: 0.9775 - val_loss: 0.0589 - val_accuracy: 0.9845 - 543ms/epoch - 2ms/step\n",
            "Epoch 375/1000\n",
            "250/250 - 1s - loss: 0.1016 - accuracy: 0.9761 - val_loss: 0.0598 - val_accuracy: 0.9845 - 624ms/epoch - 2ms/step\n",
            "Epoch 376/1000\n",
            "250/250 - 1s - loss: 0.0947 - accuracy: 0.9772 - val_loss: 0.0606 - val_accuracy: 0.9850 - 620ms/epoch - 2ms/step\n",
            "Epoch 377/1000\n",
            "250/250 - 1s - loss: 0.0945 - accuracy: 0.9781 - val_loss: 0.0726 - val_accuracy: 0.9810 - 613ms/epoch - 2ms/step\n",
            "Epoch 378/1000\n",
            "250/250 - 1s - loss: 0.1002 - accuracy: 0.9764 - val_loss: 0.0613 - val_accuracy: 0.9850 - 612ms/epoch - 2ms/step\n",
            "Epoch 379/1000\n",
            "250/250 - 1s - loss: 0.1029 - accuracy: 0.9759 - val_loss: 0.0661 - val_accuracy: 0.9850 - 617ms/epoch - 2ms/step\n",
            "Epoch 380/1000\n",
            "250/250 - 1s - loss: 0.1027 - accuracy: 0.9755 - val_loss: 0.0626 - val_accuracy: 0.9850 - 535ms/epoch - 2ms/step\n",
            "Epoch 381/1000\n",
            "250/250 - 1s - loss: 0.0853 - accuracy: 0.9794 - val_loss: 0.0597 - val_accuracy: 0.9850 - 537ms/epoch - 2ms/step\n",
            "Epoch 382/1000\n",
            "250/250 - 1s - loss: 0.0910 - accuracy: 0.9803 - val_loss: 0.0639 - val_accuracy: 0.9840 - 599ms/epoch - 2ms/step\n",
            "Epoch 383/1000\n",
            "250/250 - 1s - loss: 0.0902 - accuracy: 0.9791 - val_loss: 0.0596 - val_accuracy: 0.9855 - 616ms/epoch - 2ms/step\n",
            "Epoch 384/1000\n",
            "250/250 - 1s - loss: 0.0978 - accuracy: 0.9775 - val_loss: 0.0596 - val_accuracy: 0.9850 - 642ms/epoch - 3ms/step\n",
            "Epoch 385/1000\n",
            "250/250 - 1s - loss: 0.0883 - accuracy: 0.9790 - val_loss: 0.0554 - val_accuracy: 0.9845 - 529ms/epoch - 2ms/step\n",
            "Epoch 386/1000\n",
            "250/250 - 1s - loss: 0.0952 - accuracy: 0.9785 - val_loss: 0.0604 - val_accuracy: 0.9845 - 563ms/epoch - 2ms/step\n",
            "Epoch 387/1000\n",
            "250/250 - 1s - loss: 0.0913 - accuracy: 0.9776 - val_loss: 0.0628 - val_accuracy: 0.9850 - 623ms/epoch - 2ms/step\n",
            "Epoch 388/1000\n",
            "250/250 - 1s - loss: 0.0937 - accuracy: 0.9794 - val_loss: 0.0563 - val_accuracy: 0.9855 - 549ms/epoch - 2ms/step\n",
            "Epoch 389/1000\n",
            "250/250 - 1s - loss: 0.0987 - accuracy: 0.9778 - val_loss: 0.0631 - val_accuracy: 0.9840 - 599ms/epoch - 2ms/step\n",
            "Epoch 390/1000\n",
            "250/250 - 1s - loss: 0.0959 - accuracy: 0.9768 - val_loss: 0.0819 - val_accuracy: 0.9785 - 539ms/epoch - 2ms/step\n",
            "Epoch 391/1000\n",
            "250/250 - 1s - loss: 0.1003 - accuracy: 0.9759 - val_loss: 0.0684 - val_accuracy: 0.9855 - 631ms/epoch - 3ms/step\n",
            "Epoch 392/1000\n",
            "250/250 - 1s - loss: 0.0927 - accuracy: 0.9789 - val_loss: 0.0608 - val_accuracy: 0.9850 - 604ms/epoch - 2ms/step\n",
            "Epoch 393/1000\n",
            "250/250 - 1s - loss: 0.0953 - accuracy: 0.9786 - val_loss: 0.0634 - val_accuracy: 0.9840 - 560ms/epoch - 2ms/step\n",
            "Epoch 394/1000\n",
            "250/250 - 1s - loss: 0.0977 - accuracy: 0.9776 - val_loss: 0.0575 - val_accuracy: 0.9860 - 554ms/epoch - 2ms/step\n",
            "Epoch 395/1000\n",
            "250/250 - 1s - loss: 0.1018 - accuracy: 0.9741 - val_loss: 0.0845 - val_accuracy: 0.9780 - 545ms/epoch - 2ms/step\n",
            "Epoch 396/1000\n",
            "250/250 - 1s - loss: 0.0889 - accuracy: 0.9799 - val_loss: 0.0613 - val_accuracy: 0.9845 - 605ms/epoch - 2ms/step\n",
            "Epoch 397/1000\n",
            "250/250 - 1s - loss: 0.0935 - accuracy: 0.9765 - val_loss: 0.0643 - val_accuracy: 0.9845 - 543ms/epoch - 2ms/step\n",
            "Epoch 398/1000\n",
            "250/250 - 1s - loss: 0.0952 - accuracy: 0.9786 - val_loss: 0.0569 - val_accuracy: 0.9845 - 540ms/epoch - 2ms/step\n",
            "Epoch 399/1000\n",
            "250/250 - 1s - loss: 0.0908 - accuracy: 0.9783 - val_loss: 0.0588 - val_accuracy: 0.9845 - 611ms/epoch - 2ms/step\n",
            "Epoch 400/1000\n",
            "250/250 - 1s - loss: 0.0872 - accuracy: 0.9772 - val_loss: 0.0583 - val_accuracy: 0.9845 - 549ms/epoch - 2ms/step\n",
            "Epoch 401/1000\n",
            "250/250 - 1s - loss: 0.0950 - accuracy: 0.9783 - val_loss: 0.0604 - val_accuracy: 0.9850 - 543ms/epoch - 2ms/step\n",
            "Epoch 402/1000\n",
            "250/250 - 1s - loss: 0.0954 - accuracy: 0.9771 - val_loss: 0.0604 - val_accuracy: 0.9830 - 543ms/epoch - 2ms/step\n",
            "Epoch 403/1000\n",
            "250/250 - 1s - loss: 0.0957 - accuracy: 0.9766 - val_loss: 0.0638 - val_accuracy: 0.9855 - 532ms/epoch - 2ms/step\n",
            "Epoch 404/1000\n",
            "250/250 - 1s - loss: 0.0944 - accuracy: 0.9776 - val_loss: 0.0570 - val_accuracy: 0.9850 - 612ms/epoch - 2ms/step\n",
            "Epoch 405/1000\n",
            "250/250 - 1s - loss: 0.0915 - accuracy: 0.9769 - val_loss: 0.0628 - val_accuracy: 0.9845 - 603ms/epoch - 2ms/step\n",
            "Epoch 406/1000\n",
            "250/250 - 1s - loss: 0.1021 - accuracy: 0.9769 - val_loss: 0.0618 - val_accuracy: 0.9845 - 609ms/epoch - 2ms/step\n",
            "Epoch 407/1000\n",
            "250/250 - 1s - loss: 0.0984 - accuracy: 0.9755 - val_loss: 0.0578 - val_accuracy: 0.9850 - 537ms/epoch - 2ms/step\n",
            "Epoch 408/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9795 - val_loss: 0.0561 - val_accuracy: 0.9855 - 603ms/epoch - 2ms/step\n",
            "Epoch 409/1000\n",
            "250/250 - 1s - loss: 0.0933 - accuracy: 0.9774 - val_loss: 0.0634 - val_accuracy: 0.9845 - 624ms/epoch - 2ms/step\n",
            "Epoch 410/1000\n",
            "250/250 - 1s - loss: 0.0970 - accuracy: 0.9781 - val_loss: 0.0571 - val_accuracy: 0.9855 - 608ms/epoch - 2ms/step\n",
            "Epoch 411/1000\n",
            "250/250 - 1s - loss: 0.0942 - accuracy: 0.9778 - val_loss: 0.0581 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 412/1000\n",
            "250/250 - 1s - loss: 0.0851 - accuracy: 0.9797 - val_loss: 0.0590 - val_accuracy: 0.9840 - 611ms/epoch - 2ms/step\n",
            "Epoch 413/1000\n",
            "250/250 - 1s - loss: 0.0917 - accuracy: 0.9768 - val_loss: 0.0727 - val_accuracy: 0.9820 - 552ms/epoch - 2ms/step\n",
            "Epoch 414/1000\n",
            "250/250 - 1s - loss: 0.1020 - accuracy: 0.9753 - val_loss: 0.0599 - val_accuracy: 0.9855 - 552ms/epoch - 2ms/step\n",
            "Epoch 415/1000\n",
            "250/250 - 1s - loss: 0.0955 - accuracy: 0.9770 - val_loss: 0.0559 - val_accuracy: 0.9845 - 538ms/epoch - 2ms/step\n",
            "Epoch 416/1000\n",
            "250/250 - 1s - loss: 0.0963 - accuracy: 0.9772 - val_loss: 0.0646 - val_accuracy: 0.9840 - 588ms/epoch - 2ms/step\n",
            "Epoch 417/1000\n",
            "250/250 - 1s - loss: 0.0990 - accuracy: 0.9759 - val_loss: 0.0644 - val_accuracy: 0.9845 - 580ms/epoch - 2ms/step\n",
            "Epoch 418/1000\n",
            "250/250 - 1s - loss: 0.0975 - accuracy: 0.9760 - val_loss: 0.0569 - val_accuracy: 0.9855 - 679ms/epoch - 3ms/step\n",
            "Epoch 419/1000\n",
            "250/250 - 1s - loss: 0.0988 - accuracy: 0.9778 - val_loss: 0.0617 - val_accuracy: 0.9855 - 622ms/epoch - 2ms/step\n",
            "Epoch 420/1000\n",
            "250/250 - 1s - loss: 0.0976 - accuracy: 0.9756 - val_loss: 0.0585 - val_accuracy: 0.9865 - 626ms/epoch - 3ms/step\n",
            "Epoch 421/1000\n",
            "250/250 - 1s - loss: 0.1009 - accuracy: 0.9739 - val_loss: 0.0575 - val_accuracy: 0.9870 - 648ms/epoch - 3ms/step\n",
            "Epoch 422/1000\n",
            "250/250 - 1s - loss: 0.0891 - accuracy: 0.9786 - val_loss: 0.0604 - val_accuracy: 0.9855 - 546ms/epoch - 2ms/step\n",
            "Epoch 423/1000\n",
            "250/250 - 1s - loss: 0.0998 - accuracy: 0.9750 - val_loss: 0.0645 - val_accuracy: 0.9855 - 691ms/epoch - 3ms/step\n",
            "Epoch 424/1000\n",
            "250/250 - 1s - loss: 0.0861 - accuracy: 0.9784 - val_loss: 0.0595 - val_accuracy: 0.9860 - 626ms/epoch - 3ms/step\n",
            "Epoch 425/1000\n",
            "250/250 - 1s - loss: 0.0838 - accuracy: 0.9810 - val_loss: 0.0642 - val_accuracy: 0.9860 - 609ms/epoch - 2ms/step\n",
            "Epoch 426/1000\n",
            "250/250 - 1s - loss: 0.0976 - accuracy: 0.9774 - val_loss: 0.0869 - val_accuracy: 0.9805 - 636ms/epoch - 3ms/step\n",
            "Epoch 427/1000\n",
            "250/250 - 1s - loss: 0.0897 - accuracy: 0.9789 - val_loss: 0.0611 - val_accuracy: 0.9845 - 597ms/epoch - 2ms/step\n",
            "Epoch 428/1000\n",
            "250/250 - 1s - loss: 0.0990 - accuracy: 0.9758 - val_loss: 0.0749 - val_accuracy: 0.9805 - 543ms/epoch - 2ms/step\n",
            "Epoch 429/1000\n",
            "250/250 - 1s - loss: 0.0922 - accuracy: 0.9775 - val_loss: 0.0577 - val_accuracy: 0.9870 - 542ms/epoch - 2ms/step\n",
            "Epoch 430/1000\n",
            "250/250 - 1s - loss: 0.1015 - accuracy: 0.9750 - val_loss: 0.0653 - val_accuracy: 0.9830 - 536ms/epoch - 2ms/step\n",
            "Epoch 431/1000\n",
            "250/250 - 1s - loss: 0.0939 - accuracy: 0.9771 - val_loss: 0.0615 - val_accuracy: 0.9865 - 652ms/epoch - 3ms/step\n",
            "Epoch 432/1000\n",
            "250/250 - 1s - loss: 0.0918 - accuracy: 0.9779 - val_loss: 0.0601 - val_accuracy: 0.9860 - 608ms/epoch - 2ms/step\n",
            "Epoch 433/1000\n",
            "250/250 - 1s - loss: 0.0802 - accuracy: 0.9805 - val_loss: 0.0593 - val_accuracy: 0.9860 - 539ms/epoch - 2ms/step\n",
            "Epoch 434/1000\n",
            "250/250 - 1s - loss: 0.0847 - accuracy: 0.9811 - val_loss: 0.0600 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 435/1000\n",
            "250/250 - 1s - loss: 0.0936 - accuracy: 0.9787 - val_loss: 0.0594 - val_accuracy: 0.9865 - 633ms/epoch - 3ms/step\n",
            "Epoch 436/1000\n",
            "250/250 - 1s - loss: 0.0894 - accuracy: 0.9804 - val_loss: 0.0584 - val_accuracy: 0.9865 - 540ms/epoch - 2ms/step\n",
            "Epoch 437/1000\n",
            "250/250 - 1s - loss: 0.0885 - accuracy: 0.9778 - val_loss: 0.0585 - val_accuracy: 0.9865 - 524ms/epoch - 2ms/step\n",
            "Epoch 438/1000\n",
            "250/250 - 1s - loss: 0.0853 - accuracy: 0.9785 - val_loss: 0.0612 - val_accuracy: 0.9860 - 545ms/epoch - 2ms/step\n",
            "Epoch 439/1000\n",
            "250/250 - 1s - loss: 0.0786 - accuracy: 0.9814 - val_loss: 0.0595 - val_accuracy: 0.9840 - 547ms/epoch - 2ms/step\n",
            "Epoch 440/1000\n",
            "250/250 - 1s - loss: 0.0947 - accuracy: 0.9762 - val_loss: 0.1369 - val_accuracy: 0.9640 - 628ms/epoch - 3ms/step\n",
            "Epoch 441/1000\n",
            "250/250 - 1s - loss: 0.1049 - accuracy: 0.9746 - val_loss: 0.0593 - val_accuracy: 0.9860 - 524ms/epoch - 2ms/step\n",
            "Epoch 442/1000\n",
            "250/250 - 1s - loss: 0.0860 - accuracy: 0.9801 - val_loss: 0.0570 - val_accuracy: 0.9865 - 531ms/epoch - 2ms/step\n",
            "Epoch 443/1000\n",
            "250/250 - 1s - loss: 0.0884 - accuracy: 0.9800 - val_loss: 0.0593 - val_accuracy: 0.9860 - 541ms/epoch - 2ms/step\n",
            "Epoch 444/1000\n",
            "250/250 - 1s - loss: 0.0941 - accuracy: 0.9774 - val_loss: 0.0585 - val_accuracy: 0.9870 - 619ms/epoch - 2ms/step\n",
            "Epoch 445/1000\n",
            "250/250 - 1s - loss: 0.0886 - accuracy: 0.9786 - val_loss: 0.0643 - val_accuracy: 0.9855 - 614ms/epoch - 2ms/step\n",
            "Epoch 446/1000\n",
            "250/250 - 1s - loss: 0.0856 - accuracy: 0.9789 - val_loss: 0.0583 - val_accuracy: 0.9850 - 680ms/epoch - 3ms/step\n",
            "Epoch 447/1000\n",
            "250/250 - 1s - loss: 0.0806 - accuracy: 0.9819 - val_loss: 0.0593 - val_accuracy: 0.9855 - 643ms/epoch - 3ms/step\n",
            "Epoch 448/1000\n",
            "250/250 - 1s - loss: 0.0967 - accuracy: 0.9774 - val_loss: 0.0682 - val_accuracy: 0.9850 - 600ms/epoch - 2ms/step\n",
            "Epoch 449/1000\n",
            "250/250 - 1s - loss: 0.0942 - accuracy: 0.9785 - val_loss: 0.0590 - val_accuracy: 0.9865 - 655ms/epoch - 3ms/step\n",
            "Epoch 450/1000\n",
            "250/250 - 1s - loss: 0.0863 - accuracy: 0.9795 - val_loss: 0.0665 - val_accuracy: 0.9855 - 652ms/epoch - 3ms/step\n",
            "Epoch 451/1000\n",
            "250/250 - 1s - loss: 0.0901 - accuracy: 0.9797 - val_loss: 0.0630 - val_accuracy: 0.9860 - 554ms/epoch - 2ms/step\n",
            "Epoch 452/1000\n",
            "250/250 - 1s - loss: 0.0984 - accuracy: 0.9776 - val_loss: 0.1029 - val_accuracy: 0.9720 - 553ms/epoch - 2ms/step\n",
            "Epoch 453/1000\n",
            "250/250 - 1s - loss: 0.0918 - accuracy: 0.9776 - val_loss: 0.0642 - val_accuracy: 0.9855 - 532ms/epoch - 2ms/step\n",
            "Epoch 454/1000\n",
            "250/250 - 1s - loss: 0.0953 - accuracy: 0.9779 - val_loss: 0.0606 - val_accuracy: 0.9855 - 544ms/epoch - 2ms/step\n",
            "Epoch 455/1000\n",
            "250/250 - 1s - loss: 0.0909 - accuracy: 0.9786 - val_loss: 0.0592 - val_accuracy: 0.9875 - 562ms/epoch - 2ms/step\n",
            "Epoch 456/1000\n",
            "250/250 - 1s - loss: 0.0851 - accuracy: 0.9796 - val_loss: 0.0558 - val_accuracy: 0.9860 - 554ms/epoch - 2ms/step\n",
            "Epoch 457/1000\n",
            "250/250 - 1s - loss: 0.1082 - accuracy: 0.9730 - val_loss: 0.0609 - val_accuracy: 0.9845 - 548ms/epoch - 2ms/step\n",
            "Epoch 458/1000\n",
            "250/250 - 1s - loss: 0.0997 - accuracy: 0.9758 - val_loss: 0.0576 - val_accuracy: 0.9870 - 618ms/epoch - 2ms/step\n",
            "Epoch 459/1000\n",
            "250/250 - 1s - loss: 0.0892 - accuracy: 0.9795 - val_loss: 0.0600 - val_accuracy: 0.9855 - 627ms/epoch - 3ms/step\n",
            "Epoch 460/1000\n",
            "250/250 - 1s - loss: 0.0911 - accuracy: 0.9791 - val_loss: 0.0598 - val_accuracy: 0.9870 - 609ms/epoch - 2ms/step\n",
            "Epoch 461/1000\n",
            "250/250 - 1s - loss: 0.0954 - accuracy: 0.9783 - val_loss: 0.0567 - val_accuracy: 0.9860 - 621ms/epoch - 2ms/step\n",
            "Epoch 462/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9801 - val_loss: 0.0609 - val_accuracy: 0.9850 - 544ms/epoch - 2ms/step\n",
            "Epoch 463/1000\n",
            "250/250 - 1s - loss: 0.0942 - accuracy: 0.9786 - val_loss: 0.0642 - val_accuracy: 0.9855 - 534ms/epoch - 2ms/step\n",
            "Epoch 464/1000\n",
            "250/250 - 1s - loss: 0.0892 - accuracy: 0.9799 - val_loss: 0.0610 - val_accuracy: 0.9855 - 557ms/epoch - 2ms/step\n",
            "Epoch 465/1000\n",
            "250/250 - 1s - loss: 0.0890 - accuracy: 0.9787 - val_loss: 0.0641 - val_accuracy: 0.9865 - 517ms/epoch - 2ms/step\n",
            "Epoch 466/1000\n",
            "250/250 - 1s - loss: 0.0938 - accuracy: 0.9775 - val_loss: 0.0586 - val_accuracy: 0.9860 - 543ms/epoch - 2ms/step\n",
            "Epoch 467/1000\n",
            "250/250 - 1s - loss: 0.0871 - accuracy: 0.9789 - val_loss: 0.0621 - val_accuracy: 0.9845 - 551ms/epoch - 2ms/step\n",
            "Epoch 468/1000\n",
            "250/250 - 1s - loss: 0.0921 - accuracy: 0.9785 - val_loss: 0.0592 - val_accuracy: 0.9860 - 627ms/epoch - 3ms/step\n",
            "Epoch 469/1000\n",
            "250/250 - 1s - loss: 0.0949 - accuracy: 0.9766 - val_loss: 0.0635 - val_accuracy: 0.9845 - 528ms/epoch - 2ms/step\n",
            "Epoch 470/1000\n",
            "250/250 - 1s - loss: 0.0900 - accuracy: 0.9780 - val_loss: 0.0604 - val_accuracy: 0.9845 - 544ms/epoch - 2ms/step\n",
            "Epoch 471/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9791 - val_loss: 0.0572 - val_accuracy: 0.9855 - 526ms/epoch - 2ms/step\n",
            "Epoch 472/1000\n",
            "250/250 - 1s - loss: 0.0835 - accuracy: 0.9787 - val_loss: 0.0626 - val_accuracy: 0.9840 - 619ms/epoch - 2ms/step\n",
            "Epoch 473/1000\n",
            "250/250 - 1s - loss: 0.1062 - accuracy: 0.9731 - val_loss: 0.0556 - val_accuracy: 0.9865 - 533ms/epoch - 2ms/step\n",
            "Epoch 474/1000\n",
            "250/250 - 1s - loss: 0.0903 - accuracy: 0.9787 - val_loss: 0.0594 - val_accuracy: 0.9855 - 534ms/epoch - 2ms/step\n",
            "Epoch 475/1000\n",
            "250/250 - 1s - loss: 0.0957 - accuracy: 0.9790 - val_loss: 0.0547 - val_accuracy: 0.9860 - 544ms/epoch - 2ms/step\n",
            "Epoch 476/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9799 - val_loss: 0.0569 - val_accuracy: 0.9860 - 611ms/epoch - 2ms/step\n",
            "Epoch 477/1000\n",
            "250/250 - 1s - loss: 0.0919 - accuracy: 0.9780 - val_loss: 0.0591 - val_accuracy: 0.9870 - 616ms/epoch - 2ms/step\n",
            "Epoch 478/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9819 - val_loss: 0.0551 - val_accuracy: 0.9880 - 597ms/epoch - 2ms/step\n",
            "Epoch 479/1000\n",
            "250/250 - 1s - loss: 0.0850 - accuracy: 0.9780 - val_loss: 0.0567 - val_accuracy: 0.9890 - 541ms/epoch - 2ms/step\n",
            "Epoch 480/1000\n",
            "250/250 - 1s - loss: 0.0842 - accuracy: 0.9806 - val_loss: 0.0569 - val_accuracy: 0.9885 - 538ms/epoch - 2ms/step\n",
            "Epoch 481/1000\n",
            "250/250 - 1s - loss: 0.0918 - accuracy: 0.9785 - val_loss: 0.0579 - val_accuracy: 0.9855 - 605ms/epoch - 2ms/step\n",
            "Epoch 482/1000\n",
            "250/250 - 1s - loss: 0.0801 - accuracy: 0.9806 - val_loss: 0.0590 - val_accuracy: 0.9875 - 623ms/epoch - 2ms/step\n",
            "Epoch 483/1000\n",
            "250/250 - 1s - loss: 0.0904 - accuracy: 0.9795 - val_loss: 0.0619 - val_accuracy: 0.9860 - 617ms/epoch - 2ms/step\n",
            "Epoch 484/1000\n",
            "250/250 - 1s - loss: 0.0930 - accuracy: 0.9784 - val_loss: 0.0569 - val_accuracy: 0.9870 - 549ms/epoch - 2ms/step\n",
            "Epoch 485/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9783 - val_loss: 0.0831 - val_accuracy: 0.9800 - 604ms/epoch - 2ms/step\n",
            "Epoch 486/1000\n",
            "250/250 - 1s - loss: 0.1021 - accuracy: 0.9741 - val_loss: 0.0609 - val_accuracy: 0.9870 - 556ms/epoch - 2ms/step\n",
            "Epoch 487/1000\n",
            "250/250 - 1s - loss: 0.1011 - accuracy: 0.9759 - val_loss: 0.0592 - val_accuracy: 0.9855 - 636ms/epoch - 3ms/step\n",
            "Epoch 488/1000\n",
            "250/250 - 1s - loss: 0.0768 - accuracy: 0.9829 - val_loss: 0.0628 - val_accuracy: 0.9860 - 598ms/epoch - 2ms/step\n",
            "Epoch 489/1000\n",
            "250/250 - 1s - loss: 0.0836 - accuracy: 0.9806 - val_loss: 0.0655 - val_accuracy: 0.9855 - 553ms/epoch - 2ms/step\n",
            "Epoch 490/1000\n",
            "250/250 - 1s - loss: 0.0944 - accuracy: 0.9779 - val_loss: 0.0593 - val_accuracy: 0.9845 - 533ms/epoch - 2ms/step\n",
            "Epoch 491/1000\n",
            "250/250 - 1s - loss: 0.0834 - accuracy: 0.9818 - val_loss: 0.0639 - val_accuracy: 0.9865 - 612ms/epoch - 2ms/step\n",
            "Epoch 492/1000\n",
            "250/250 - 1s - loss: 0.0886 - accuracy: 0.9803 - val_loss: 0.0580 - val_accuracy: 0.9850 - 529ms/epoch - 2ms/step\n",
            "Epoch 493/1000\n",
            "250/250 - 1s - loss: 0.0847 - accuracy: 0.9800 - val_loss: 0.0673 - val_accuracy: 0.9855 - 544ms/epoch - 2ms/step\n",
            "Epoch 494/1000\n",
            "250/250 - 1s - loss: 0.0928 - accuracy: 0.9783 - val_loss: 0.0608 - val_accuracy: 0.9845 - 543ms/epoch - 2ms/step\n",
            "Epoch 495/1000\n",
            "250/250 - 1s - loss: 0.0842 - accuracy: 0.9799 - val_loss: 0.0754 - val_accuracy: 0.9805 - 534ms/epoch - 2ms/step\n",
            "Epoch 496/1000\n",
            "250/250 - 1s - loss: 0.0917 - accuracy: 0.9781 - val_loss: 0.0582 - val_accuracy: 0.9865 - 609ms/epoch - 2ms/step\n",
            "Epoch 497/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9796 - val_loss: 0.0646 - val_accuracy: 0.9850 - 608ms/epoch - 2ms/step\n",
            "Epoch 498/1000\n",
            "250/250 - 1s - loss: 0.0937 - accuracy: 0.9787 - val_loss: 0.0591 - val_accuracy: 0.9860 - 540ms/epoch - 2ms/step\n",
            "Epoch 499/1000\n",
            "250/250 - 1s - loss: 0.0887 - accuracy: 0.9785 - val_loss: 0.0588 - val_accuracy: 0.9845 - 531ms/epoch - 2ms/step\n",
            "Epoch 500/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9797 - val_loss: 0.0650 - val_accuracy: 0.9850 - 606ms/epoch - 2ms/step\n",
            "Epoch 501/1000\n",
            "250/250 - 1s - loss: 0.0754 - accuracy: 0.9818 - val_loss: 0.0582 - val_accuracy: 0.9860 - 609ms/epoch - 2ms/step\n",
            "Epoch 502/1000\n",
            "250/250 - 1s - loss: 0.0964 - accuracy: 0.9771 - val_loss: 0.0600 - val_accuracy: 0.9845 - 533ms/epoch - 2ms/step\n",
            "Epoch 503/1000\n",
            "250/250 - 1s - loss: 0.0886 - accuracy: 0.9772 - val_loss: 0.0587 - val_accuracy: 0.9850 - 621ms/epoch - 2ms/step\n",
            "Epoch 504/1000\n",
            "250/250 - 1s - loss: 0.0833 - accuracy: 0.9808 - val_loss: 0.0657 - val_accuracy: 0.9850 - 547ms/epoch - 2ms/step\n",
            "Epoch 505/1000\n",
            "250/250 - 1s - loss: 0.0929 - accuracy: 0.9776 - val_loss: 0.0570 - val_accuracy: 0.9875 - 604ms/epoch - 2ms/step\n",
            "Epoch 506/1000\n",
            "250/250 - 1s - loss: 0.0973 - accuracy: 0.9770 - val_loss: 0.0692 - val_accuracy: 0.9820 - 536ms/epoch - 2ms/step\n",
            "Epoch 507/1000\n",
            "250/250 - 1s - loss: 0.0924 - accuracy: 0.9771 - val_loss: 0.0578 - val_accuracy: 0.9850 - 617ms/epoch - 2ms/step\n",
            "Epoch 508/1000\n",
            "250/250 - 1s - loss: 0.0924 - accuracy: 0.9771 - val_loss: 0.0667 - val_accuracy: 0.9850 - 602ms/epoch - 2ms/step\n",
            "Epoch 509/1000\n",
            "250/250 - 1s - loss: 0.0951 - accuracy: 0.9775 - val_loss: 0.0608 - val_accuracy: 0.9855 - 528ms/epoch - 2ms/step\n",
            "Epoch 510/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9793 - val_loss: 0.0609 - val_accuracy: 0.9840 - 536ms/epoch - 2ms/step\n",
            "Epoch 511/1000\n",
            "250/250 - 1s - loss: 0.0852 - accuracy: 0.9803 - val_loss: 0.0582 - val_accuracy: 0.9860 - 601ms/epoch - 2ms/step\n",
            "Epoch 512/1000\n",
            "250/250 - 1s - loss: 0.0923 - accuracy: 0.9774 - val_loss: 0.0570 - val_accuracy: 0.9870 - 531ms/epoch - 2ms/step\n",
            "Epoch 513/1000\n",
            "250/250 - 1s - loss: 0.0879 - accuracy: 0.9775 - val_loss: 0.0583 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 514/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9770 - val_loss: 0.0547 - val_accuracy: 0.9870 - 616ms/epoch - 2ms/step\n",
            "Epoch 515/1000\n",
            "250/250 - 1s - loss: 0.0891 - accuracy: 0.9775 - val_loss: 0.0584 - val_accuracy: 0.9860 - 617ms/epoch - 2ms/step\n",
            "Epoch 516/1000\n",
            "250/250 - 1s - loss: 0.0819 - accuracy: 0.9809 - val_loss: 0.0582 - val_accuracy: 0.9870 - 607ms/epoch - 2ms/step\n",
            "Epoch 517/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9786 - val_loss: 0.0641 - val_accuracy: 0.9850 - 548ms/epoch - 2ms/step\n",
            "Epoch 518/1000\n",
            "250/250 - 1s - loss: 0.0900 - accuracy: 0.9774 - val_loss: 0.0550 - val_accuracy: 0.9860 - 614ms/epoch - 2ms/step\n",
            "Epoch 519/1000\n",
            "250/250 - 1s - loss: 0.0911 - accuracy: 0.9789 - val_loss: 0.0563 - val_accuracy: 0.9855 - 550ms/epoch - 2ms/step\n",
            "Epoch 520/1000\n",
            "250/250 - 1s - loss: 0.0865 - accuracy: 0.9795 - val_loss: 0.0610 - val_accuracy: 0.9845 - 531ms/epoch - 2ms/step\n",
            "Epoch 521/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9789 - val_loss: 0.0690 - val_accuracy: 0.9860 - 539ms/epoch - 2ms/step\n",
            "Epoch 522/1000\n",
            "250/250 - 1s - loss: 0.0858 - accuracy: 0.9787 - val_loss: 0.0580 - val_accuracy: 0.9850 - 548ms/epoch - 2ms/step\n",
            "Epoch 523/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9806 - val_loss: 0.0602 - val_accuracy: 0.9860 - 542ms/epoch - 2ms/step\n",
            "Epoch 524/1000\n",
            "250/250 - 1s - loss: 0.0929 - accuracy: 0.9768 - val_loss: 0.0554 - val_accuracy: 0.9875 - 611ms/epoch - 2ms/step\n",
            "Epoch 525/1000\n",
            "250/250 - 1s - loss: 0.0872 - accuracy: 0.9791 - val_loss: 0.0601 - val_accuracy: 0.9860 - 607ms/epoch - 2ms/step\n",
            "Epoch 526/1000\n",
            "250/250 - 1s - loss: 0.0879 - accuracy: 0.9799 - val_loss: 0.0569 - val_accuracy: 0.9870 - 551ms/epoch - 2ms/step\n",
            "Epoch 527/1000\n",
            "250/250 - 1s - loss: 0.0899 - accuracy: 0.9799 - val_loss: 0.0556 - val_accuracy: 0.9865 - 534ms/epoch - 2ms/step\n",
            "Epoch 528/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9824 - val_loss: 0.0642 - val_accuracy: 0.9855 - 557ms/epoch - 2ms/step\n",
            "Epoch 529/1000\n",
            "250/250 - 1s - loss: 0.0986 - accuracy: 0.9766 - val_loss: 0.0636 - val_accuracy: 0.9865 - 534ms/epoch - 2ms/step\n",
            "Epoch 530/1000\n",
            "250/250 - 1s - loss: 0.0859 - accuracy: 0.9800 - val_loss: 0.0615 - val_accuracy: 0.9850 - 609ms/epoch - 2ms/step\n",
            "Epoch 531/1000\n",
            "250/250 - 1s - loss: 0.0896 - accuracy: 0.9794 - val_loss: 0.0646 - val_accuracy: 0.9865 - 611ms/epoch - 2ms/step\n",
            "Epoch 532/1000\n",
            "250/250 - 1s - loss: 0.0850 - accuracy: 0.9803 - val_loss: 0.0540 - val_accuracy: 0.9880 - 615ms/epoch - 2ms/step\n",
            "Epoch 533/1000\n",
            "250/250 - 1s - loss: 0.0896 - accuracy: 0.9795 - val_loss: 0.0572 - val_accuracy: 0.9860 - 613ms/epoch - 2ms/step\n",
            "Epoch 534/1000\n",
            "250/250 - 1s - loss: 0.0921 - accuracy: 0.9785 - val_loss: 0.0602 - val_accuracy: 0.9850 - 524ms/epoch - 2ms/step\n",
            "Epoch 535/1000\n",
            "250/250 - 1s - loss: 0.0962 - accuracy: 0.9784 - val_loss: 0.0550 - val_accuracy: 0.9855 - 622ms/epoch - 2ms/step\n",
            "Epoch 536/1000\n",
            "250/250 - 1s - loss: 0.0853 - accuracy: 0.9799 - val_loss: 0.0624 - val_accuracy: 0.9855 - 531ms/epoch - 2ms/step\n",
            "Epoch 537/1000\n",
            "250/250 - 1s - loss: 0.0916 - accuracy: 0.9774 - val_loss: 0.0545 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 538/1000\n",
            "250/250 - 1s - loss: 0.0912 - accuracy: 0.9786 - val_loss: 0.0628 - val_accuracy: 0.9825 - 547ms/epoch - 2ms/step\n",
            "Epoch 539/1000\n",
            "250/250 - 1s - loss: 0.0996 - accuracy: 0.9769 - val_loss: 0.0574 - val_accuracy: 0.9870 - 638ms/epoch - 3ms/step\n",
            "Epoch 540/1000\n",
            "250/250 - 1s - loss: 0.0896 - accuracy: 0.9784 - val_loss: 0.0598 - val_accuracy: 0.9860 - 562ms/epoch - 2ms/step\n",
            "Epoch 541/1000\n",
            "250/250 - 1s - loss: 0.0877 - accuracy: 0.9780 - val_loss: 0.0566 - val_accuracy: 0.9855 - 605ms/epoch - 2ms/step\n",
            "Epoch 542/1000\n",
            "250/250 - 1s - loss: 0.0912 - accuracy: 0.9778 - val_loss: 0.0542 - val_accuracy: 0.9865 - 548ms/epoch - 2ms/step\n",
            "Epoch 543/1000\n",
            "250/250 - 1s - loss: 0.0821 - accuracy: 0.9783 - val_loss: 0.0597 - val_accuracy: 0.9850 - 535ms/epoch - 2ms/step\n",
            "Epoch 544/1000\n",
            "250/250 - 1s - loss: 0.0921 - accuracy: 0.9790 - val_loss: 0.0563 - val_accuracy: 0.9850 - 611ms/epoch - 2ms/step\n",
            "Epoch 545/1000\n",
            "250/250 - 1s - loss: 0.0877 - accuracy: 0.9795 - val_loss: 0.0607 - val_accuracy: 0.9860 - 605ms/epoch - 2ms/step\n",
            "Epoch 546/1000\n",
            "250/250 - 1s - loss: 0.0812 - accuracy: 0.9806 - val_loss: 0.0588 - val_accuracy: 0.9860 - 545ms/epoch - 2ms/step\n",
            "Epoch 547/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9787 - val_loss: 0.0548 - val_accuracy: 0.9855 - 604ms/epoch - 2ms/step\n",
            "Epoch 548/1000\n",
            "250/250 - 1s - loss: 0.0953 - accuracy: 0.9758 - val_loss: 0.0586 - val_accuracy: 0.9840 - 530ms/epoch - 2ms/step\n",
            "Epoch 549/1000\n",
            "250/250 - 1s - loss: 0.0964 - accuracy: 0.9776 - val_loss: 0.0615 - val_accuracy: 0.9860 - 538ms/epoch - 2ms/step\n",
            "Epoch 550/1000\n",
            "250/250 - 1s - loss: 0.0899 - accuracy: 0.9791 - val_loss: 0.0568 - val_accuracy: 0.9860 - 574ms/epoch - 2ms/step\n",
            "Epoch 551/1000\n",
            "250/250 - 1s - loss: 0.0844 - accuracy: 0.9785 - val_loss: 0.0573 - val_accuracy: 0.9865 - 627ms/epoch - 3ms/step\n",
            "Epoch 552/1000\n",
            "250/250 - 1s - loss: 0.0882 - accuracy: 0.9805 - val_loss: 0.0521 - val_accuracy: 0.9875 - 531ms/epoch - 2ms/step\n",
            "Epoch 553/1000\n",
            "250/250 - 1s - loss: 0.0850 - accuracy: 0.9791 - val_loss: 0.0525 - val_accuracy: 0.9870 - 619ms/epoch - 2ms/step\n",
            "Epoch 554/1000\n",
            "250/250 - 1s - loss: 0.0940 - accuracy: 0.9774 - val_loss: 0.0576 - val_accuracy: 0.9850 - 530ms/epoch - 2ms/step\n",
            "Epoch 555/1000\n",
            "250/250 - 1s - loss: 0.0915 - accuracy: 0.9776 - val_loss: 0.0567 - val_accuracy: 0.9865 - 533ms/epoch - 2ms/step\n",
            "Epoch 556/1000\n",
            "250/250 - 1s - loss: 0.0820 - accuracy: 0.9797 - val_loss: 0.0595 - val_accuracy: 0.9860 - 532ms/epoch - 2ms/step\n",
            "Epoch 557/1000\n",
            "250/250 - 1s - loss: 0.0889 - accuracy: 0.9789 - val_loss: 0.0617 - val_accuracy: 0.9865 - 528ms/epoch - 2ms/step\n",
            "Epoch 558/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9799 - val_loss: 0.0572 - val_accuracy: 0.9850 - 615ms/epoch - 2ms/step\n",
            "Epoch 559/1000\n",
            "250/250 - 1s - loss: 0.0912 - accuracy: 0.9786 - val_loss: 0.0588 - val_accuracy: 0.9860 - 553ms/epoch - 2ms/step\n",
            "Epoch 560/1000\n",
            "250/250 - 1s - loss: 0.0909 - accuracy: 0.9795 - val_loss: 0.0619 - val_accuracy: 0.9860 - 636ms/epoch - 3ms/step\n",
            "Epoch 561/1000\n",
            "250/250 - 1s - loss: 0.0911 - accuracy: 0.9776 - val_loss: 0.0584 - val_accuracy: 0.9865 - 535ms/epoch - 2ms/step\n",
            "Epoch 562/1000\n",
            "250/250 - 1s - loss: 0.0890 - accuracy: 0.9793 - val_loss: 0.0647 - val_accuracy: 0.9865 - 546ms/epoch - 2ms/step\n",
            "Epoch 563/1000\n",
            "250/250 - 1s - loss: 0.0920 - accuracy: 0.9772 - val_loss: 0.0617 - val_accuracy: 0.9865 - 608ms/epoch - 2ms/step\n",
            "Epoch 564/1000\n",
            "250/250 - 1s - loss: 0.0807 - accuracy: 0.9806 - val_loss: 0.0552 - val_accuracy: 0.9835 - 605ms/epoch - 2ms/step\n",
            "Epoch 565/1000\n",
            "250/250 - 1s - loss: 0.0998 - accuracy: 0.9775 - val_loss: 0.0560 - val_accuracy: 0.9865 - 541ms/epoch - 2ms/step\n",
            "Epoch 566/1000\n",
            "250/250 - 1s - loss: 0.0867 - accuracy: 0.9795 - val_loss: 0.0562 - val_accuracy: 0.9850 - 547ms/epoch - 2ms/step\n",
            "Epoch 567/1000\n",
            "250/250 - 1s - loss: 0.0799 - accuracy: 0.9816 - val_loss: 0.0575 - val_accuracy: 0.9865 - 618ms/epoch - 2ms/step\n",
            "Epoch 568/1000\n",
            "250/250 - 1s - loss: 0.0867 - accuracy: 0.9795 - val_loss: 0.0650 - val_accuracy: 0.9815 - 603ms/epoch - 2ms/step\n",
            "Epoch 569/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9795 - val_loss: 0.0639 - val_accuracy: 0.9860 - 556ms/epoch - 2ms/step\n",
            "Epoch 570/1000\n",
            "250/250 - 1s - loss: 0.0798 - accuracy: 0.9816 - val_loss: 0.0551 - val_accuracy: 0.9860 - 530ms/epoch - 2ms/step\n",
            "Epoch 571/1000\n",
            "250/250 - 1s - loss: 0.0894 - accuracy: 0.9805 - val_loss: 0.0623 - val_accuracy: 0.9845 - 544ms/epoch - 2ms/step\n",
            "Epoch 572/1000\n",
            "250/250 - 1s - loss: 0.0848 - accuracy: 0.9815 - val_loss: 0.0621 - val_accuracy: 0.9850 - 547ms/epoch - 2ms/step\n",
            "Epoch 573/1000\n",
            "250/250 - 1s - loss: 0.0874 - accuracy: 0.9800 - val_loss: 0.0613 - val_accuracy: 0.9850 - 603ms/epoch - 2ms/step\n",
            "Epoch 574/1000\n",
            "250/250 - 1s - loss: 0.0823 - accuracy: 0.9809 - val_loss: 0.0625 - val_accuracy: 0.9850 - 543ms/epoch - 2ms/step\n",
            "Epoch 575/1000\n",
            "250/250 - 1s - loss: 0.0880 - accuracy: 0.9803 - val_loss: 0.0642 - val_accuracy: 0.9860 - 525ms/epoch - 2ms/step\n",
            "Epoch 576/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9787 - val_loss: 0.0547 - val_accuracy: 0.9855 - 552ms/epoch - 2ms/step\n",
            "Epoch 577/1000\n",
            "250/250 - 1s - loss: 0.0884 - accuracy: 0.9779 - val_loss: 0.0572 - val_accuracy: 0.9855 - 605ms/epoch - 2ms/step\n",
            "Epoch 578/1000\n",
            "250/250 - 1s - loss: 0.0953 - accuracy: 0.9783 - val_loss: 0.0558 - val_accuracy: 0.9845 - 640ms/epoch - 3ms/step\n",
            "Epoch 579/1000\n",
            "250/250 - 1s - loss: 0.0837 - accuracy: 0.9808 - val_loss: 0.0541 - val_accuracy: 0.9840 - 622ms/epoch - 2ms/step\n",
            "Epoch 580/1000\n",
            "250/250 - 1s - loss: 0.0953 - accuracy: 0.9770 - val_loss: 0.0576 - val_accuracy: 0.9855 - 525ms/epoch - 2ms/step\n",
            "Epoch 581/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9803 - val_loss: 0.0642 - val_accuracy: 0.9850 - 535ms/epoch - 2ms/step\n",
            "Epoch 582/1000\n",
            "250/250 - 1s - loss: 0.0863 - accuracy: 0.9805 - val_loss: 0.0657 - val_accuracy: 0.9845 - 615ms/epoch - 2ms/step\n",
            "Epoch 583/1000\n",
            "250/250 - 1s - loss: 0.0919 - accuracy: 0.9796 - val_loss: 0.0631 - val_accuracy: 0.9845 - 552ms/epoch - 2ms/step\n",
            "Epoch 584/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9809 - val_loss: 0.0600 - val_accuracy: 0.9855 - 528ms/epoch - 2ms/step\n",
            "Epoch 585/1000\n",
            "250/250 - 1s - loss: 0.0847 - accuracy: 0.9809 - val_loss: 0.0586 - val_accuracy: 0.9855 - 535ms/epoch - 2ms/step\n",
            "Epoch 586/1000\n",
            "250/250 - 1s - loss: 0.0917 - accuracy: 0.9780 - val_loss: 0.0625 - val_accuracy: 0.9845 - 608ms/epoch - 2ms/step\n",
            "Epoch 587/1000\n",
            "250/250 - 1s - loss: 0.0854 - accuracy: 0.9786 - val_loss: 0.0611 - val_accuracy: 0.9845 - 618ms/epoch - 2ms/step\n",
            "Epoch 588/1000\n",
            "250/250 - 1s - loss: 0.0927 - accuracy: 0.9787 - val_loss: 0.0627 - val_accuracy: 0.9860 - 537ms/epoch - 2ms/step\n",
            "Epoch 589/1000\n",
            "250/250 - 1s - loss: 0.0951 - accuracy: 0.9795 - val_loss: 0.0593 - val_accuracy: 0.9870 - 609ms/epoch - 2ms/step\n",
            "Epoch 590/1000\n",
            "250/250 - 1s - loss: 0.0836 - accuracy: 0.9790 - val_loss: 0.0646 - val_accuracy: 0.9850 - 661ms/epoch - 3ms/step\n",
            "Epoch 591/1000\n",
            "250/250 - 1s - loss: 0.0892 - accuracy: 0.9793 - val_loss: 0.0642 - val_accuracy: 0.9860 - 573ms/epoch - 2ms/step\n",
            "Epoch 592/1000\n",
            "250/250 - 1s - loss: 0.0826 - accuracy: 0.9806 - val_loss: 0.0589 - val_accuracy: 0.9860 - 648ms/epoch - 3ms/step\n",
            "Epoch 593/1000\n",
            "250/250 - 1s - loss: 0.0872 - accuracy: 0.9787 - val_loss: 0.0581 - val_accuracy: 0.9860 - 549ms/epoch - 2ms/step\n",
            "Epoch 594/1000\n",
            "250/250 - 1s - loss: 0.0914 - accuracy: 0.9791 - val_loss: 0.0577 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 595/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9776 - val_loss: 0.0576 - val_accuracy: 0.9870 - 566ms/epoch - 2ms/step\n",
            "Epoch 596/1000\n",
            "250/250 - 1s - loss: 0.0713 - accuracy: 0.9846 - val_loss: 0.0607 - val_accuracy: 0.9860 - 605ms/epoch - 2ms/step\n",
            "Epoch 597/1000\n",
            "250/250 - 1s - loss: 0.0824 - accuracy: 0.9809 - val_loss: 0.0568 - val_accuracy: 0.9865 - 552ms/epoch - 2ms/step\n",
            "Epoch 598/1000\n",
            "250/250 - 1s - loss: 0.0921 - accuracy: 0.9790 - val_loss: 0.0583 - val_accuracy: 0.9865 - 527ms/epoch - 2ms/step\n",
            "Epoch 599/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9796 - val_loss: 0.0587 - val_accuracy: 0.9855 - 551ms/epoch - 2ms/step\n",
            "Epoch 600/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9776 - val_loss: 0.0577 - val_accuracy: 0.9865 - 613ms/epoch - 2ms/step\n",
            "Epoch 601/1000\n",
            "250/250 - 1s - loss: 0.0876 - accuracy: 0.9804 - val_loss: 0.0579 - val_accuracy: 0.9855 - 545ms/epoch - 2ms/step\n",
            "Epoch 602/1000\n",
            "250/250 - 1s - loss: 0.0777 - accuracy: 0.9822 - val_loss: 0.0565 - val_accuracy: 0.9860 - 537ms/epoch - 2ms/step\n",
            "Epoch 603/1000\n",
            "250/250 - 1s - loss: 0.0813 - accuracy: 0.9791 - val_loss: 0.0630 - val_accuracy: 0.9865 - 529ms/epoch - 2ms/step\n",
            "Epoch 604/1000\n",
            "250/250 - 1s - loss: 0.0799 - accuracy: 0.9794 - val_loss: 0.0612 - val_accuracy: 0.9865 - 551ms/epoch - 2ms/step\n",
            "Epoch 605/1000\n",
            "250/250 - 1s - loss: 0.0783 - accuracy: 0.9824 - val_loss: 0.0571 - val_accuracy: 0.9850 - 533ms/epoch - 2ms/step\n",
            "Epoch 606/1000\n",
            "250/250 - 1s - loss: 0.0933 - accuracy: 0.9786 - val_loss: 0.0577 - val_accuracy: 0.9860 - 550ms/epoch - 2ms/step\n",
            "Epoch 607/1000\n",
            "250/250 - 1s - loss: 0.0935 - accuracy: 0.9766 - val_loss: 0.0556 - val_accuracy: 0.9865 - 533ms/epoch - 2ms/step\n",
            "Epoch 608/1000\n",
            "250/250 - 1s - loss: 0.0958 - accuracy: 0.9762 - val_loss: 0.0591 - val_accuracy: 0.9865 - 634ms/epoch - 3ms/step\n",
            "Epoch 609/1000\n",
            "250/250 - 1s - loss: 0.0862 - accuracy: 0.9794 - val_loss: 0.0859 - val_accuracy: 0.9815 - 642ms/epoch - 3ms/step\n",
            "Epoch 610/1000\n",
            "250/250 - 1s - loss: 0.0819 - accuracy: 0.9808 - val_loss: 0.0618 - val_accuracy: 0.9860 - 611ms/epoch - 2ms/step\n",
            "Epoch 611/1000\n",
            "250/250 - 1s - loss: 0.0895 - accuracy: 0.9796 - val_loss: 0.0574 - val_accuracy: 0.9860 - 534ms/epoch - 2ms/step\n",
            "Epoch 612/1000\n",
            "250/250 - 1s - loss: 0.0933 - accuracy: 0.9779 - val_loss: 0.0598 - val_accuracy: 0.9840 - 523ms/epoch - 2ms/step\n",
            "Epoch 613/1000\n",
            "250/250 - 1s - loss: 0.0883 - accuracy: 0.9787 - val_loss: 0.0579 - val_accuracy: 0.9865 - 534ms/epoch - 2ms/step\n",
            "Epoch 614/1000\n",
            "250/250 - 1s - loss: 0.0840 - accuracy: 0.9804 - val_loss: 0.0562 - val_accuracy: 0.9860 - 552ms/epoch - 2ms/step\n",
            "Epoch 615/1000\n",
            "250/250 - 1s - loss: 0.0871 - accuracy: 0.9791 - val_loss: 0.0586 - val_accuracy: 0.9860 - 531ms/epoch - 2ms/step\n",
            "Epoch 616/1000\n",
            "250/250 - 1s - loss: 0.0842 - accuracy: 0.9781 - val_loss: 0.0562 - val_accuracy: 0.9865 - 545ms/epoch - 2ms/step\n",
            "Epoch 617/1000\n",
            "250/250 - 1s - loss: 0.0806 - accuracy: 0.9810 - val_loss: 0.0571 - val_accuracy: 0.9860 - 577ms/epoch - 2ms/step\n",
            "Epoch 618/1000\n",
            "250/250 - 1s - loss: 0.0908 - accuracy: 0.9775 - val_loss: 0.0607 - val_accuracy: 0.9860 - 527ms/epoch - 2ms/step\n",
            "Epoch 619/1000\n",
            "250/250 - 1s - loss: 0.0835 - accuracy: 0.9790 - val_loss: 0.0551 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 620/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9793 - val_loss: 0.0544 - val_accuracy: 0.9860 - 525ms/epoch - 2ms/step\n",
            "Epoch 621/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9818 - val_loss: 0.0619 - val_accuracy: 0.9865 - 606ms/epoch - 2ms/step\n",
            "Epoch 622/1000\n",
            "250/250 - 1s - loss: 0.0863 - accuracy: 0.9799 - val_loss: 0.0588 - val_accuracy: 0.9855 - 613ms/epoch - 2ms/step\n",
            "Epoch 623/1000\n",
            "250/250 - 1s - loss: 0.0874 - accuracy: 0.9789 - val_loss: 0.0559 - val_accuracy: 0.9870 - 603ms/epoch - 2ms/step\n",
            "Epoch 624/1000\n",
            "250/250 - 1s - loss: 0.0898 - accuracy: 0.9783 - val_loss: 0.0586 - val_accuracy: 0.9850 - 529ms/epoch - 2ms/step\n",
            "Epoch 625/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9811 - val_loss: 0.0612 - val_accuracy: 0.9845 - 547ms/epoch - 2ms/step\n",
            "Epoch 626/1000\n",
            "250/250 - 1s - loss: 0.0816 - accuracy: 0.9809 - val_loss: 0.0558 - val_accuracy: 0.9870 - 564ms/epoch - 2ms/step\n",
            "Epoch 627/1000\n",
            "250/250 - 1s - loss: 0.0939 - accuracy: 0.9789 - val_loss: 0.0609 - val_accuracy: 0.9865 - 537ms/epoch - 2ms/step\n",
            "Epoch 628/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9803 - val_loss: 0.0557 - val_accuracy: 0.9850 - 616ms/epoch - 2ms/step\n",
            "Epoch 629/1000\n",
            "250/250 - 1s - loss: 0.1009 - accuracy: 0.9758 - val_loss: 0.0584 - val_accuracy: 0.9855 - 539ms/epoch - 2ms/step\n",
            "Epoch 630/1000\n",
            "250/250 - 1s - loss: 0.0910 - accuracy: 0.9786 - val_loss: 0.0568 - val_accuracy: 0.9855 - 620ms/epoch - 2ms/step\n",
            "Epoch 631/1000\n",
            "250/250 - 1s - loss: 0.0725 - accuracy: 0.9835 - val_loss: 0.0586 - val_accuracy: 0.9860 - 547ms/epoch - 2ms/step\n",
            "Epoch 632/1000\n",
            "250/250 - 1s - loss: 0.0749 - accuracy: 0.9829 - val_loss: 0.0621 - val_accuracy: 0.9860 - 611ms/epoch - 2ms/step\n",
            "Epoch 633/1000\n",
            "250/250 - 1s - loss: 0.0824 - accuracy: 0.9796 - val_loss: 0.0593 - val_accuracy: 0.9860 - 555ms/epoch - 2ms/step\n",
            "Epoch 634/1000\n",
            "250/250 - 1s - loss: 0.0822 - accuracy: 0.9805 - val_loss: 0.0616 - val_accuracy: 0.9855 - 612ms/epoch - 2ms/step\n",
            "Epoch 635/1000\n",
            "250/250 - 1s - loss: 0.0902 - accuracy: 0.9778 - val_loss: 0.0590 - val_accuracy: 0.9860 - 624ms/epoch - 2ms/step\n",
            "Epoch 636/1000\n",
            "250/250 - 1s - loss: 0.0987 - accuracy: 0.9779 - val_loss: 0.0549 - val_accuracy: 0.9865 - 547ms/epoch - 2ms/step\n",
            "Epoch 637/1000\n",
            "250/250 - 1s - loss: 0.0796 - accuracy: 0.9811 - val_loss: 0.0546 - val_accuracy: 0.9875 - 532ms/epoch - 2ms/step\n",
            "Epoch 638/1000\n",
            "250/250 - 1s - loss: 0.0868 - accuracy: 0.9800 - val_loss: 0.0538 - val_accuracy: 0.9865 - 620ms/epoch - 2ms/step\n",
            "Epoch 639/1000\n",
            "250/250 - 1s - loss: 0.0827 - accuracy: 0.9786 - val_loss: 0.0547 - val_accuracy: 0.9850 - 536ms/epoch - 2ms/step\n",
            "Epoch 640/1000\n",
            "250/250 - 1s - loss: 0.0840 - accuracy: 0.9789 - val_loss: 0.0539 - val_accuracy: 0.9875 - 542ms/epoch - 2ms/step\n",
            "Epoch 641/1000\n",
            "250/250 - 1s - loss: 0.0922 - accuracy: 0.9765 - val_loss: 0.0617 - val_accuracy: 0.9850 - 609ms/epoch - 2ms/step\n",
            "Epoch 642/1000\n",
            "250/250 - 1s - loss: 0.0826 - accuracy: 0.9801 - val_loss: 0.0585 - val_accuracy: 0.9865 - 618ms/epoch - 2ms/step\n",
            "Epoch 643/1000\n",
            "250/250 - 1s - loss: 0.0902 - accuracy: 0.9801 - val_loss: 0.0688 - val_accuracy: 0.9805 - 539ms/epoch - 2ms/step\n",
            "Epoch 644/1000\n",
            "250/250 - 1s - loss: 0.0838 - accuracy: 0.9793 - val_loss: 0.0577 - val_accuracy: 0.9855 - 548ms/epoch - 2ms/step\n",
            "Epoch 645/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9816 - val_loss: 0.0574 - val_accuracy: 0.9860 - 605ms/epoch - 2ms/step\n",
            "Epoch 646/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9811 - val_loss: 0.0580 - val_accuracy: 0.9870 - 534ms/epoch - 2ms/step\n",
            "Epoch 647/1000\n",
            "250/250 - 1s - loss: 0.0817 - accuracy: 0.9797 - val_loss: 0.0579 - val_accuracy: 0.9860 - 538ms/epoch - 2ms/step\n",
            "Epoch 648/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9801 - val_loss: 0.0561 - val_accuracy: 0.9870 - 600ms/epoch - 2ms/step\n",
            "Epoch 649/1000\n",
            "250/250 - 1s - loss: 0.0858 - accuracy: 0.9797 - val_loss: 0.0568 - val_accuracy: 0.9860 - 651ms/epoch - 3ms/step\n",
            "Epoch 650/1000\n",
            "250/250 - 1s - loss: 0.0811 - accuracy: 0.9799 - val_loss: 0.0772 - val_accuracy: 0.9795 - 549ms/epoch - 2ms/step\n",
            "Epoch 651/1000\n",
            "250/250 - 1s - loss: 0.0915 - accuracy: 0.9764 - val_loss: 0.0540 - val_accuracy: 0.9870 - 645ms/epoch - 3ms/step\n",
            "Epoch 652/1000\n",
            "250/250 - 1s - loss: 0.0929 - accuracy: 0.9790 - val_loss: 0.0622 - val_accuracy: 0.9855 - 549ms/epoch - 2ms/step\n",
            "Epoch 653/1000\n",
            "250/250 - 1s - loss: 0.0823 - accuracy: 0.9810 - val_loss: 0.0881 - val_accuracy: 0.9740 - 541ms/epoch - 2ms/step\n",
            "Epoch 654/1000\n",
            "250/250 - 1s - loss: 0.0877 - accuracy: 0.9784 - val_loss: 0.0546 - val_accuracy: 0.9865 - 541ms/epoch - 2ms/step\n",
            "Epoch 655/1000\n",
            "250/250 - 1s - loss: 0.0749 - accuracy: 0.9815 - val_loss: 0.0590 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 656/1000\n",
            "250/250 - 1s - loss: 0.0895 - accuracy: 0.9780 - val_loss: 0.0623 - val_accuracy: 0.9850 - 544ms/epoch - 2ms/step\n",
            "Epoch 657/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9797 - val_loss: 0.0572 - val_accuracy: 0.9860 - 595ms/epoch - 2ms/step\n",
            "Epoch 658/1000\n",
            "250/250 - 1s - loss: 0.0850 - accuracy: 0.9796 - val_loss: 0.0599 - val_accuracy: 0.9855 - 613ms/epoch - 2ms/step\n",
            "Epoch 659/1000\n",
            "250/250 - 1s - loss: 0.0780 - accuracy: 0.9822 - val_loss: 0.0575 - val_accuracy: 0.9850 - 530ms/epoch - 2ms/step\n",
            "Epoch 660/1000\n",
            "250/250 - 1s - loss: 0.0822 - accuracy: 0.9809 - val_loss: 0.0562 - val_accuracy: 0.9860 - 525ms/epoch - 2ms/step\n",
            "Epoch 661/1000\n",
            "250/250 - 1s - loss: 0.0912 - accuracy: 0.9783 - val_loss: 0.0588 - val_accuracy: 0.9845 - 595ms/epoch - 2ms/step\n",
            "Epoch 662/1000\n",
            "250/250 - 1s - loss: 0.0912 - accuracy: 0.9789 - val_loss: 0.0562 - val_accuracy: 0.9855 - 609ms/epoch - 2ms/step\n",
            "Epoch 663/1000\n",
            "250/250 - 1s - loss: 0.0824 - accuracy: 0.9811 - val_loss: 0.0609 - val_accuracy: 0.9860 - 547ms/epoch - 2ms/step\n",
            "Epoch 664/1000\n",
            "250/250 - 1s - loss: 0.0780 - accuracy: 0.9825 - val_loss: 0.0570 - val_accuracy: 0.9845 - 537ms/epoch - 2ms/step\n",
            "Epoch 665/1000\n",
            "250/250 - 1s - loss: 0.0783 - accuracy: 0.9826 - val_loss: 0.0559 - val_accuracy: 0.9850 - 623ms/epoch - 2ms/step\n",
            "Epoch 666/1000\n",
            "250/250 - 1s - loss: 0.0875 - accuracy: 0.9781 - val_loss: 0.0583 - val_accuracy: 0.9845 - 616ms/epoch - 2ms/step\n",
            "Epoch 667/1000\n",
            "250/250 - 1s - loss: 0.0829 - accuracy: 0.9796 - val_loss: 0.0550 - val_accuracy: 0.9870 - 545ms/epoch - 2ms/step\n",
            "Epoch 668/1000\n",
            "250/250 - 1s - loss: 0.0932 - accuracy: 0.9781 - val_loss: 0.0546 - val_accuracy: 0.9875 - 606ms/epoch - 2ms/step\n",
            "Epoch 669/1000\n",
            "250/250 - 1s - loss: 0.0889 - accuracy: 0.9776 - val_loss: 0.0568 - val_accuracy: 0.9870 - 619ms/epoch - 2ms/step\n",
            "Epoch 670/1000\n",
            "250/250 - 1s - loss: 0.0774 - accuracy: 0.9821 - val_loss: 0.0584 - val_accuracy: 0.9855 - 545ms/epoch - 2ms/step\n",
            "Epoch 671/1000\n",
            "250/250 - 1s - loss: 0.0868 - accuracy: 0.9779 - val_loss: 0.0561 - val_accuracy: 0.9870 - 538ms/epoch - 2ms/step\n",
            "Epoch 672/1000\n",
            "250/250 - 1s - loss: 0.0731 - accuracy: 0.9815 - val_loss: 0.0576 - val_accuracy: 0.9860 - 551ms/epoch - 2ms/step\n",
            "Epoch 673/1000\n",
            "250/250 - 1s - loss: 0.0884 - accuracy: 0.9799 - val_loss: 0.0767 - val_accuracy: 0.9810 - 604ms/epoch - 2ms/step\n",
            "Epoch 674/1000\n",
            "250/250 - 1s - loss: 0.0873 - accuracy: 0.9797 - val_loss: 0.0582 - val_accuracy: 0.9870 - 612ms/epoch - 2ms/step\n",
            "Epoch 675/1000\n",
            "250/250 - 1s - loss: 0.0909 - accuracy: 0.9779 - val_loss: 0.0556 - val_accuracy: 0.9870 - 544ms/epoch - 2ms/step\n",
            "Epoch 676/1000\n",
            "250/250 - 1s - loss: 0.0894 - accuracy: 0.9793 - val_loss: 0.0581 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 677/1000\n",
            "250/250 - 1s - loss: 0.0738 - accuracy: 0.9825 - val_loss: 0.0538 - val_accuracy: 0.9860 - 533ms/epoch - 2ms/step\n",
            "Epoch 678/1000\n",
            "250/250 - 1s - loss: 0.0759 - accuracy: 0.9816 - val_loss: 0.0558 - val_accuracy: 0.9865 - 615ms/epoch - 2ms/step\n",
            "Epoch 679/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9809 - val_loss: 0.0545 - val_accuracy: 0.9865 - 542ms/epoch - 2ms/step\n",
            "Epoch 680/1000\n",
            "250/250 - 1s - loss: 0.0886 - accuracy: 0.9795 - val_loss: 0.0583 - val_accuracy: 0.9875 - 533ms/epoch - 2ms/step\n",
            "Epoch 681/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9799 - val_loss: 0.0583 - val_accuracy: 0.9855 - 542ms/epoch - 2ms/step\n",
            "Epoch 682/1000\n",
            "250/250 - 1s - loss: 0.0867 - accuracy: 0.9790 - val_loss: 0.0582 - val_accuracy: 0.9835 - 527ms/epoch - 2ms/step\n",
            "Epoch 683/1000\n",
            "250/250 - 1s - loss: 0.0791 - accuracy: 0.9810 - val_loss: 0.0569 - val_accuracy: 0.9865 - 618ms/epoch - 2ms/step\n",
            "Epoch 684/1000\n",
            "250/250 - 1s - loss: 0.0785 - accuracy: 0.9814 - val_loss: 0.0599 - val_accuracy: 0.9850 - 545ms/epoch - 2ms/step\n",
            "Epoch 685/1000\n",
            "250/250 - 1s - loss: 0.0880 - accuracy: 0.9795 - val_loss: 0.0548 - val_accuracy: 0.9875 - 535ms/epoch - 2ms/step\n",
            "Epoch 686/1000\n",
            "250/250 - 1s - loss: 0.0795 - accuracy: 0.9814 - val_loss: 0.0616 - val_accuracy: 0.9855 - 628ms/epoch - 3ms/step\n",
            "Epoch 687/1000\n",
            "250/250 - 1s - loss: 0.0922 - accuracy: 0.9766 - val_loss: 0.0577 - val_accuracy: 0.9875 - 639ms/epoch - 3ms/step\n",
            "Epoch 688/1000\n",
            "250/250 - 1s - loss: 0.0827 - accuracy: 0.9803 - val_loss: 0.0581 - val_accuracy: 0.9860 - 608ms/epoch - 2ms/step\n",
            "Epoch 689/1000\n",
            "250/250 - 1s - loss: 0.0795 - accuracy: 0.9814 - val_loss: 0.0546 - val_accuracy: 0.9880 - 531ms/epoch - 2ms/step\n",
            "Epoch 690/1000\n",
            "250/250 - 1s - loss: 0.0746 - accuracy: 0.9811 - val_loss: 0.0558 - val_accuracy: 0.9875 - 548ms/epoch - 2ms/step\n",
            "Epoch 691/1000\n",
            "250/250 - 1s - loss: 0.0770 - accuracy: 0.9812 - val_loss: 0.0603 - val_accuracy: 0.9870 - 605ms/epoch - 2ms/step\n",
            "Epoch 692/1000\n",
            "250/250 - 1s - loss: 0.0923 - accuracy: 0.9779 - val_loss: 0.0563 - val_accuracy: 0.9855 - 602ms/epoch - 2ms/step\n",
            "Epoch 693/1000\n",
            "250/250 - 1s - loss: 0.0844 - accuracy: 0.9781 - val_loss: 0.0604 - val_accuracy: 0.9840 - 631ms/epoch - 3ms/step\n",
            "Epoch 694/1000\n",
            "250/250 - 1s - loss: 0.0856 - accuracy: 0.9779 - val_loss: 0.0581 - val_accuracy: 0.9845 - 538ms/epoch - 2ms/step\n",
            "Epoch 695/1000\n",
            "250/250 - 1s - loss: 0.0896 - accuracy: 0.9793 - val_loss: 0.0561 - val_accuracy: 0.9870 - 620ms/epoch - 2ms/step\n",
            "Epoch 696/1000\n",
            "250/250 - 1s - loss: 0.0740 - accuracy: 0.9818 - val_loss: 0.0540 - val_accuracy: 0.9880 - 606ms/epoch - 2ms/step\n",
            "Epoch 697/1000\n",
            "250/250 - 1s - loss: 0.0860 - accuracy: 0.9811 - val_loss: 0.0575 - val_accuracy: 0.9860 - 590ms/epoch - 2ms/step\n",
            "Epoch 698/1000\n",
            "250/250 - 1s - loss: 0.0797 - accuracy: 0.9810 - val_loss: 0.0569 - val_accuracy: 0.9850 - 627ms/epoch - 3ms/step\n",
            "Epoch 699/1000\n",
            "250/250 - 1s - loss: 0.0872 - accuracy: 0.9803 - val_loss: 0.0540 - val_accuracy: 0.9870 - 529ms/epoch - 2ms/step\n",
            "Epoch 700/1000\n",
            "250/250 - 1s - loss: 0.0821 - accuracy: 0.9805 - val_loss: 0.0551 - val_accuracy: 0.9860 - 540ms/epoch - 2ms/step\n",
            "Epoch 701/1000\n",
            "250/250 - 1s - loss: 0.0902 - accuracy: 0.9762 - val_loss: 0.0605 - val_accuracy: 0.9845 - 535ms/epoch - 2ms/step\n",
            "Epoch 702/1000\n",
            "250/250 - 1s - loss: 0.0786 - accuracy: 0.9808 - val_loss: 0.0589 - val_accuracy: 0.9865 - 546ms/epoch - 2ms/step\n",
            "Epoch 703/1000\n",
            "250/250 - 1s - loss: 0.0757 - accuracy: 0.9812 - val_loss: 0.0592 - val_accuracy: 0.9855 - 539ms/epoch - 2ms/step\n",
            "Epoch 704/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9821 - val_loss: 0.0523 - val_accuracy: 0.9870 - 623ms/epoch - 2ms/step\n",
            "Epoch 705/1000\n",
            "250/250 - 1s - loss: 0.0743 - accuracy: 0.9839 - val_loss: 0.0551 - val_accuracy: 0.9865 - 627ms/epoch - 3ms/step\n",
            "Epoch 706/1000\n",
            "250/250 - 1s - loss: 0.0841 - accuracy: 0.9799 - val_loss: 0.0563 - val_accuracy: 0.9865 - 608ms/epoch - 2ms/step\n",
            "Epoch 707/1000\n",
            "250/250 - 1s - loss: 0.0897 - accuracy: 0.9774 - val_loss: 0.0565 - val_accuracy: 0.9855 - 547ms/epoch - 2ms/step\n",
            "Epoch 708/1000\n",
            "250/250 - 1s - loss: 0.0770 - accuracy: 0.9816 - val_loss: 0.0576 - val_accuracy: 0.9860 - 607ms/epoch - 2ms/step\n",
            "Epoch 709/1000\n",
            "250/250 - 1s - loss: 0.0861 - accuracy: 0.9804 - val_loss: 0.0551 - val_accuracy: 0.9870 - 620ms/epoch - 2ms/step\n",
            "Epoch 710/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9831 - val_loss: 0.0611 - val_accuracy: 0.9865 - 628ms/epoch - 3ms/step\n",
            "Epoch 711/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9822 - val_loss: 0.0573 - val_accuracy: 0.9860 - 610ms/epoch - 2ms/step\n",
            "Epoch 712/1000\n",
            "250/250 - 1s - loss: 0.0853 - accuracy: 0.9797 - val_loss: 0.0593 - val_accuracy: 0.9860 - 544ms/epoch - 2ms/step\n",
            "Epoch 713/1000\n",
            "250/250 - 1s - loss: 0.0930 - accuracy: 0.9775 - val_loss: 0.0585 - val_accuracy: 0.9865 - 533ms/epoch - 2ms/step\n",
            "Epoch 714/1000\n",
            "250/250 - 1s - loss: 0.0802 - accuracy: 0.9822 - val_loss: 0.0551 - val_accuracy: 0.9850 - 537ms/epoch - 2ms/step\n",
            "Epoch 715/1000\n",
            "250/250 - 1s - loss: 0.0818 - accuracy: 0.9810 - val_loss: 0.0573 - val_accuracy: 0.9855 - 603ms/epoch - 2ms/step\n",
            "Epoch 716/1000\n",
            "250/250 - 1s - loss: 0.0807 - accuracy: 0.9821 - val_loss: 0.0632 - val_accuracy: 0.9865 - 548ms/epoch - 2ms/step\n",
            "Epoch 717/1000\n",
            "250/250 - 1s - loss: 0.0930 - accuracy: 0.9784 - val_loss: 0.0580 - val_accuracy: 0.9855 - 611ms/epoch - 2ms/step\n",
            "Epoch 718/1000\n",
            "250/250 - 1s - loss: 0.0861 - accuracy: 0.9795 - val_loss: 0.0547 - val_accuracy: 0.9860 - 539ms/epoch - 2ms/step\n",
            "Epoch 719/1000\n",
            "250/250 - 1s - loss: 0.0761 - accuracy: 0.9812 - val_loss: 0.0558 - val_accuracy: 0.9865 - 608ms/epoch - 2ms/step\n",
            "Epoch 720/1000\n",
            "250/250 - 1s - loss: 0.0891 - accuracy: 0.9789 - val_loss: 0.0516 - val_accuracy: 0.9870 - 531ms/epoch - 2ms/step\n",
            "Epoch 721/1000\n",
            "250/250 - 1s - loss: 0.0869 - accuracy: 0.9799 - val_loss: 0.0524 - val_accuracy: 0.9875 - 617ms/epoch - 2ms/step\n",
            "Epoch 722/1000\n",
            "250/250 - 1s - loss: 0.0775 - accuracy: 0.9806 - val_loss: 0.0574 - val_accuracy: 0.9850 - 600ms/epoch - 2ms/step\n",
            "Epoch 723/1000\n",
            "250/250 - 1s - loss: 0.0880 - accuracy: 0.9794 - val_loss: 0.0546 - val_accuracy: 0.9875 - 634ms/epoch - 3ms/step\n",
            "Epoch 724/1000\n",
            "250/250 - 1s - loss: 0.0799 - accuracy: 0.9815 - val_loss: 0.0525 - val_accuracy: 0.9875 - 607ms/epoch - 2ms/step\n",
            "Epoch 725/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9806 - val_loss: 0.0546 - val_accuracy: 0.9870 - 624ms/epoch - 2ms/step\n",
            "Epoch 726/1000\n",
            "250/250 - 1s - loss: 0.0896 - accuracy: 0.9791 - val_loss: 0.0519 - val_accuracy: 0.9865 - 539ms/epoch - 2ms/step\n",
            "Epoch 727/1000\n",
            "250/250 - 1s - loss: 0.0833 - accuracy: 0.9806 - val_loss: 0.0520 - val_accuracy: 0.9890 - 527ms/epoch - 2ms/step\n",
            "Epoch 728/1000\n",
            "250/250 - 1s - loss: 0.0763 - accuracy: 0.9826 - val_loss: 0.0567 - val_accuracy: 0.9865 - 557ms/epoch - 2ms/step\n",
            "Epoch 729/1000\n",
            "250/250 - 1s - loss: 0.0848 - accuracy: 0.9799 - val_loss: 0.0645 - val_accuracy: 0.9835 - 539ms/epoch - 2ms/step\n",
            "Epoch 730/1000\n",
            "250/250 - 1s - loss: 0.0850 - accuracy: 0.9794 - val_loss: 0.0693 - val_accuracy: 0.9825 - 530ms/epoch - 2ms/step\n",
            "Epoch 731/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9805 - val_loss: 0.0564 - val_accuracy: 0.9870 - 608ms/epoch - 2ms/step\n",
            "Epoch 732/1000\n",
            "250/250 - 1s - loss: 0.0805 - accuracy: 0.9796 - val_loss: 0.0533 - val_accuracy: 0.9870 - 560ms/epoch - 2ms/step\n",
            "Epoch 733/1000\n",
            "250/250 - 1s - loss: 0.0758 - accuracy: 0.9836 - val_loss: 0.0525 - val_accuracy: 0.9860 - 542ms/epoch - 2ms/step\n",
            "Epoch 734/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9796 - val_loss: 0.0534 - val_accuracy: 0.9880 - 610ms/epoch - 2ms/step\n",
            "Epoch 735/1000\n",
            "250/250 - 1s - loss: 0.0865 - accuracy: 0.9803 - val_loss: 0.0574 - val_accuracy: 0.9860 - 622ms/epoch - 2ms/step\n",
            "Epoch 736/1000\n",
            "250/250 - 1s - loss: 0.0852 - accuracy: 0.9819 - val_loss: 0.0593 - val_accuracy: 0.9855 - 620ms/epoch - 2ms/step\n",
            "Epoch 737/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9811 - val_loss: 0.0547 - val_accuracy: 0.9865 - 633ms/epoch - 3ms/step\n",
            "Epoch 738/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9804 - val_loss: 0.0559 - val_accuracy: 0.9870 - 626ms/epoch - 3ms/step\n",
            "Epoch 739/1000\n",
            "250/250 - 1s - loss: 0.0752 - accuracy: 0.9826 - val_loss: 0.0571 - val_accuracy: 0.9855 - 534ms/epoch - 2ms/step\n",
            "Epoch 740/1000\n",
            "250/250 - 1s - loss: 0.0926 - accuracy: 0.9775 - val_loss: 0.0614 - val_accuracy: 0.9865 - 614ms/epoch - 2ms/step\n",
            "Epoch 741/1000\n",
            "250/250 - 1s - loss: 0.0949 - accuracy: 0.9776 - val_loss: 0.0640 - val_accuracy: 0.9860 - 621ms/epoch - 2ms/step\n",
            "Epoch 742/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9818 - val_loss: 0.0541 - val_accuracy: 0.9860 - 560ms/epoch - 2ms/step\n",
            "Epoch 743/1000\n",
            "250/250 - 1s - loss: 0.0815 - accuracy: 0.9814 - val_loss: 0.0576 - val_accuracy: 0.9865 - 604ms/epoch - 2ms/step\n",
            "Epoch 744/1000\n",
            "250/250 - 1s - loss: 0.0765 - accuracy: 0.9811 - val_loss: 0.0549 - val_accuracy: 0.9870 - 608ms/epoch - 2ms/step\n",
            "Epoch 745/1000\n",
            "250/250 - 1s - loss: 0.0721 - accuracy: 0.9827 - val_loss: 0.0611 - val_accuracy: 0.9865 - 615ms/epoch - 2ms/step\n",
            "Epoch 746/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9808 - val_loss: 0.0554 - val_accuracy: 0.9860 - 537ms/epoch - 2ms/step\n",
            "Epoch 747/1000\n",
            "250/250 - 1s - loss: 0.0905 - accuracy: 0.9790 - val_loss: 0.0569 - val_accuracy: 0.9860 - 636ms/epoch - 3ms/step\n",
            "Epoch 748/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9811 - val_loss: 0.0625 - val_accuracy: 0.9865 - 530ms/epoch - 2ms/step\n",
            "Epoch 749/1000\n",
            "250/250 - 1s - loss: 0.0746 - accuracy: 0.9819 - val_loss: 0.0558 - val_accuracy: 0.9855 - 622ms/epoch - 2ms/step\n",
            "Epoch 750/1000\n",
            "250/250 - 1s - loss: 0.0843 - accuracy: 0.9796 - val_loss: 0.0567 - val_accuracy: 0.9855 - 628ms/epoch - 3ms/step\n",
            "Epoch 751/1000\n",
            "250/250 - 1s - loss: 0.0782 - accuracy: 0.9801 - val_loss: 0.0536 - val_accuracy: 0.9865 - 539ms/epoch - 2ms/step\n",
            "Epoch 752/1000\n",
            "250/250 - 1s - loss: 0.0918 - accuracy: 0.9769 - val_loss: 0.0585 - val_accuracy: 0.9855 - 539ms/epoch - 2ms/step\n",
            "Epoch 753/1000\n",
            "250/250 - 1s - loss: 0.0791 - accuracy: 0.9812 - val_loss: 0.0538 - val_accuracy: 0.9865 - 618ms/epoch - 2ms/step\n",
            "Epoch 754/1000\n",
            "250/250 - 1s - loss: 0.0807 - accuracy: 0.9810 - val_loss: 0.0557 - val_accuracy: 0.9860 - 560ms/epoch - 2ms/step\n",
            "Epoch 755/1000\n",
            "250/250 - 1s - loss: 0.0828 - accuracy: 0.9809 - val_loss: 0.0585 - val_accuracy: 0.9860 - 539ms/epoch - 2ms/step\n",
            "Epoch 756/1000\n",
            "250/250 - 1s - loss: 0.0782 - accuracy: 0.9827 - val_loss: 0.0619 - val_accuracy: 0.9850 - 552ms/epoch - 2ms/step\n",
            "Epoch 757/1000\n",
            "250/250 - 1s - loss: 0.0882 - accuracy: 0.9791 - val_loss: 0.0553 - val_accuracy: 0.9865 - 550ms/epoch - 2ms/step\n",
            "Epoch 758/1000\n",
            "250/250 - 1s - loss: 0.0827 - accuracy: 0.9810 - val_loss: 0.0584 - val_accuracy: 0.9860 - 538ms/epoch - 2ms/step\n",
            "Epoch 759/1000\n",
            "250/250 - 1s - loss: 0.0910 - accuracy: 0.9795 - val_loss: 0.0562 - val_accuracy: 0.9845 - 636ms/epoch - 3ms/step\n",
            "Epoch 760/1000\n",
            "250/250 - 1s - loss: 0.0820 - accuracy: 0.9805 - val_loss: 0.0566 - val_accuracy: 0.9855 - 533ms/epoch - 2ms/step\n",
            "Epoch 761/1000\n",
            "250/250 - 1s - loss: 0.0765 - accuracy: 0.9805 - val_loss: 0.0580 - val_accuracy: 0.9865 - 626ms/epoch - 3ms/step\n",
            "Epoch 762/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9810 - val_loss: 0.0596 - val_accuracy: 0.9860 - 608ms/epoch - 2ms/step\n",
            "Epoch 763/1000\n",
            "250/250 - 1s - loss: 0.0841 - accuracy: 0.9805 - val_loss: 0.0589 - val_accuracy: 0.9855 - 541ms/epoch - 2ms/step\n",
            "Epoch 764/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9804 - val_loss: 0.0553 - val_accuracy: 0.9860 - 610ms/epoch - 2ms/step\n",
            "Epoch 765/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9797 - val_loss: 0.0548 - val_accuracy: 0.9870 - 533ms/epoch - 2ms/step\n",
            "Epoch 766/1000\n",
            "250/250 - 1s - loss: 0.0757 - accuracy: 0.9814 - val_loss: 0.0600 - val_accuracy: 0.9850 - 612ms/epoch - 2ms/step\n",
            "Epoch 767/1000\n",
            "250/250 - 1s - loss: 0.0751 - accuracy: 0.9822 - val_loss: 0.0557 - val_accuracy: 0.9860 - 530ms/epoch - 2ms/step\n",
            "Epoch 768/1000\n",
            "250/250 - 1s - loss: 0.0791 - accuracy: 0.9811 - val_loss: 0.0565 - val_accuracy: 0.9860 - 620ms/epoch - 2ms/step\n",
            "Epoch 769/1000\n",
            "250/250 - 1s - loss: 0.0824 - accuracy: 0.9810 - val_loss: 0.0652 - val_accuracy: 0.9820 - 535ms/epoch - 2ms/step\n",
            "Epoch 770/1000\n",
            "250/250 - 1s - loss: 0.0966 - accuracy: 0.9761 - val_loss: 0.0564 - val_accuracy: 0.9855 - 620ms/epoch - 2ms/step\n",
            "Epoch 771/1000\n",
            "250/250 - 1s - loss: 0.0864 - accuracy: 0.9801 - val_loss: 0.0573 - val_accuracy: 0.9855 - 605ms/epoch - 2ms/step\n",
            "Epoch 772/1000\n",
            "250/250 - 1s - loss: 0.0761 - accuracy: 0.9831 - val_loss: 0.0561 - val_accuracy: 0.9865 - 604ms/epoch - 2ms/step\n",
            "Epoch 773/1000\n",
            "250/250 - 1s - loss: 0.0739 - accuracy: 0.9824 - val_loss: 0.0553 - val_accuracy: 0.9860 - 545ms/epoch - 2ms/step\n",
            "Epoch 774/1000\n",
            "250/250 - 1s - loss: 0.0813 - accuracy: 0.9805 - val_loss: 0.0605 - val_accuracy: 0.9860 - 538ms/epoch - 2ms/step\n",
            "Epoch 775/1000\n",
            "250/250 - 1s - loss: 0.0861 - accuracy: 0.9793 - val_loss: 0.0576 - val_accuracy: 0.9855 - 630ms/epoch - 3ms/step\n",
            "Epoch 776/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9806 - val_loss: 0.0547 - val_accuracy: 0.9850 - 533ms/epoch - 2ms/step\n",
            "Epoch 777/1000\n",
            "250/250 - 1s - loss: 0.0810 - accuracy: 0.9805 - val_loss: 0.0597 - val_accuracy: 0.9860 - 618ms/epoch - 2ms/step\n",
            "Epoch 778/1000\n",
            "250/250 - 1s - loss: 0.0856 - accuracy: 0.9766 - val_loss: 0.0578 - val_accuracy: 0.9880 - 545ms/epoch - 2ms/step\n",
            "Epoch 779/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9805 - val_loss: 0.0580 - val_accuracy: 0.9870 - 526ms/epoch - 2ms/step\n",
            "Epoch 780/1000\n",
            "250/250 - 1s - loss: 0.0791 - accuracy: 0.9816 - val_loss: 0.0570 - val_accuracy: 0.9860 - 539ms/epoch - 2ms/step\n",
            "Epoch 781/1000\n",
            "250/250 - 1s - loss: 0.0790 - accuracy: 0.9803 - val_loss: 0.0579 - val_accuracy: 0.9865 - 605ms/epoch - 2ms/step\n",
            "Epoch 782/1000\n",
            "250/250 - 1s - loss: 0.0716 - accuracy: 0.9834 - val_loss: 0.0570 - val_accuracy: 0.9860 - 622ms/epoch - 2ms/step\n",
            "Epoch 783/1000\n",
            "250/250 - 1s - loss: 0.0745 - accuracy: 0.9801 - val_loss: 0.0597 - val_accuracy: 0.9875 - 611ms/epoch - 2ms/step\n",
            "Epoch 784/1000\n",
            "250/250 - 1s - loss: 0.0778 - accuracy: 0.9820 - val_loss: 0.0548 - val_accuracy: 0.9870 - 546ms/epoch - 2ms/step\n",
            "Epoch 785/1000\n",
            "250/250 - 1s - loss: 0.0854 - accuracy: 0.9781 - val_loss: 0.0555 - val_accuracy: 0.9880 - 546ms/epoch - 2ms/step\n",
            "Epoch 786/1000\n",
            "250/250 - 1s - loss: 0.0791 - accuracy: 0.9822 - val_loss: 0.0538 - val_accuracy: 0.9860 - 553ms/epoch - 2ms/step\n",
            "Epoch 787/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9822 - val_loss: 0.0609 - val_accuracy: 0.9850 - 542ms/epoch - 2ms/step\n",
            "Epoch 788/1000\n",
            "250/250 - 1s - loss: 0.0955 - accuracy: 0.9780 - val_loss: 0.0563 - val_accuracy: 0.9855 - 532ms/epoch - 2ms/step\n",
            "Epoch 789/1000\n",
            "250/250 - 1s - loss: 0.0768 - accuracy: 0.9803 - val_loss: 0.0550 - val_accuracy: 0.9860 - 616ms/epoch - 2ms/step\n",
            "Epoch 790/1000\n",
            "250/250 - 1s - loss: 0.0753 - accuracy: 0.9827 - val_loss: 0.0572 - val_accuracy: 0.9880 - 624ms/epoch - 2ms/step\n",
            "Epoch 791/1000\n",
            "250/250 - 1s - loss: 0.0772 - accuracy: 0.9815 - val_loss: 0.0568 - val_accuracy: 0.9855 - 546ms/epoch - 2ms/step\n",
            "Epoch 792/1000\n",
            "250/250 - 1s - loss: 0.0816 - accuracy: 0.9803 - val_loss: 0.0598 - val_accuracy: 0.9855 - 599ms/epoch - 2ms/step\n",
            "Epoch 793/1000\n",
            "250/250 - 1s - loss: 0.0801 - accuracy: 0.9803 - val_loss: 0.0594 - val_accuracy: 0.9865 - 603ms/epoch - 2ms/step\n",
            "Epoch 794/1000\n",
            "250/250 - 1s - loss: 0.0787 - accuracy: 0.9821 - val_loss: 0.0608 - val_accuracy: 0.9850 - 550ms/epoch - 2ms/step\n",
            "Epoch 795/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9794 - val_loss: 0.0561 - val_accuracy: 0.9875 - 599ms/epoch - 2ms/step\n",
            "Epoch 796/1000\n",
            "250/250 - 1s - loss: 0.0819 - accuracy: 0.9809 - val_loss: 0.0604 - val_accuracy: 0.9865 - 635ms/epoch - 3ms/step\n",
            "Epoch 797/1000\n",
            "250/250 - 1s - loss: 0.0828 - accuracy: 0.9804 - val_loss: 0.0570 - val_accuracy: 0.9860 - 601ms/epoch - 2ms/step\n",
            "Epoch 798/1000\n",
            "250/250 - 1s - loss: 0.0952 - accuracy: 0.9778 - val_loss: 0.0566 - val_accuracy: 0.9870 - 541ms/epoch - 2ms/step\n",
            "Epoch 799/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9774 - val_loss: 0.0580 - val_accuracy: 0.9855 - 612ms/epoch - 2ms/step\n",
            "Epoch 800/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9799 - val_loss: 0.0564 - val_accuracy: 0.9875 - 591ms/epoch - 2ms/step\n",
            "Epoch 801/1000\n",
            "250/250 - 1s - loss: 0.0808 - accuracy: 0.9793 - val_loss: 0.0553 - val_accuracy: 0.9865 - 624ms/epoch - 2ms/step\n",
            "Epoch 802/1000\n",
            "250/250 - 1s - loss: 0.0726 - accuracy: 0.9835 - val_loss: 0.0563 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 803/1000\n",
            "250/250 - 1s - loss: 0.0737 - accuracy: 0.9833 - val_loss: 0.0546 - val_accuracy: 0.9865 - 540ms/epoch - 2ms/step\n",
            "Epoch 804/1000\n",
            "250/250 - 1s - loss: 0.0836 - accuracy: 0.9797 - val_loss: 0.0555 - val_accuracy: 0.9865 - 537ms/epoch - 2ms/step\n",
            "Epoch 805/1000\n",
            "250/250 - 1s - loss: 0.0781 - accuracy: 0.9821 - val_loss: 0.0569 - val_accuracy: 0.9860 - 618ms/epoch - 2ms/step\n",
            "Epoch 806/1000\n",
            "250/250 - 1s - loss: 0.0833 - accuracy: 0.9800 - val_loss: 0.0556 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 807/1000\n",
            "250/250 - 1s - loss: 0.0865 - accuracy: 0.9797 - val_loss: 0.0560 - val_accuracy: 0.9860 - 553ms/epoch - 2ms/step\n",
            "Epoch 808/1000\n",
            "250/250 - 1s - loss: 0.0765 - accuracy: 0.9812 - val_loss: 0.0548 - val_accuracy: 0.9860 - 604ms/epoch - 2ms/step\n",
            "Epoch 809/1000\n",
            "250/250 - 1s - loss: 0.0818 - accuracy: 0.9806 - val_loss: 0.0524 - val_accuracy: 0.9860 - 529ms/epoch - 2ms/step\n",
            "Epoch 810/1000\n",
            "250/250 - 1s - loss: 0.0760 - accuracy: 0.9822 - val_loss: 0.0729 - val_accuracy: 0.9810 - 542ms/epoch - 2ms/step\n",
            "Epoch 811/1000\n",
            "250/250 - 1s - loss: 0.0758 - accuracy: 0.9825 - val_loss: 0.0594 - val_accuracy: 0.9870 - 605ms/epoch - 2ms/step\n",
            "Epoch 812/1000\n",
            "250/250 - 1s - loss: 0.0740 - accuracy: 0.9829 - val_loss: 0.0561 - val_accuracy: 0.9875 - 534ms/epoch - 2ms/step\n",
            "Epoch 813/1000\n",
            "250/250 - 1s - loss: 0.0833 - accuracy: 0.9818 - val_loss: 0.0551 - val_accuracy: 0.9880 - 538ms/epoch - 2ms/step\n",
            "Epoch 814/1000\n",
            "250/250 - 1s - loss: 0.0810 - accuracy: 0.9810 - val_loss: 0.0587 - val_accuracy: 0.9855 - 547ms/epoch - 2ms/step\n",
            "Epoch 815/1000\n",
            "250/250 - 1s - loss: 0.0827 - accuracy: 0.9801 - val_loss: 0.0562 - val_accuracy: 0.9865 - 613ms/epoch - 2ms/step\n",
            "Epoch 816/1000\n",
            "250/250 - 1s - loss: 0.0807 - accuracy: 0.9809 - val_loss: 0.0542 - val_accuracy: 0.9860 - 537ms/epoch - 2ms/step\n",
            "Epoch 817/1000\n",
            "250/250 - 1s - loss: 0.0738 - accuracy: 0.9827 - val_loss: 0.0591 - val_accuracy: 0.9850 - 609ms/epoch - 2ms/step\n",
            "Epoch 818/1000\n",
            "250/250 - 1s - loss: 0.0787 - accuracy: 0.9818 - val_loss: 0.0606 - val_accuracy: 0.9850 - 606ms/epoch - 2ms/step\n",
            "Epoch 819/1000\n",
            "250/250 - 1s - loss: 0.0724 - accuracy: 0.9826 - val_loss: 0.0577 - val_accuracy: 0.9850 - 547ms/epoch - 2ms/step\n",
            "Epoch 820/1000\n",
            "250/250 - 1s - loss: 0.0762 - accuracy: 0.9825 - val_loss: 0.0561 - val_accuracy: 0.9860 - 527ms/epoch - 2ms/step\n",
            "Epoch 821/1000\n",
            "250/250 - 1s - loss: 0.0761 - accuracy: 0.9819 - val_loss: 0.0556 - val_accuracy: 0.9860 - 555ms/epoch - 2ms/step\n",
            "Epoch 822/1000\n",
            "250/250 - 1s - loss: 0.0840 - accuracy: 0.9783 - val_loss: 0.0595 - val_accuracy: 0.9845 - 526ms/epoch - 2ms/step\n",
            "Epoch 823/1000\n",
            "250/250 - 1s - loss: 0.0784 - accuracy: 0.9801 - val_loss: 0.0561 - val_accuracy: 0.9850 - 530ms/epoch - 2ms/step\n",
            "Epoch 824/1000\n",
            "250/250 - 1s - loss: 0.0798 - accuracy: 0.9816 - val_loss: 0.0615 - val_accuracy: 0.9855 - 609ms/epoch - 2ms/step\n",
            "Epoch 825/1000\n",
            "250/250 - 1s - loss: 0.0917 - accuracy: 0.9769 - val_loss: 0.0645 - val_accuracy: 0.9865 - 619ms/epoch - 2ms/step\n",
            "Epoch 826/1000\n",
            "250/250 - 1s - loss: 0.0759 - accuracy: 0.9810 - val_loss: 0.0545 - val_accuracy: 0.9855 - 538ms/epoch - 2ms/step\n",
            "Epoch 827/1000\n",
            "250/250 - 1s - loss: 0.0732 - accuracy: 0.9831 - val_loss: 0.0599 - val_accuracy: 0.9855 - 531ms/epoch - 2ms/step\n",
            "Epoch 828/1000\n",
            "250/250 - 1s - loss: 0.0790 - accuracy: 0.9814 - val_loss: 0.0729 - val_accuracy: 0.9795 - 624ms/epoch - 2ms/step\n",
            "Epoch 829/1000\n",
            "250/250 - 1s - loss: 0.0795 - accuracy: 0.9805 - val_loss: 0.0585 - val_accuracy: 0.9855 - 525ms/epoch - 2ms/step\n",
            "Epoch 830/1000\n",
            "250/250 - 1s - loss: 0.0846 - accuracy: 0.9789 - val_loss: 0.0555 - val_accuracy: 0.9865 - 615ms/epoch - 2ms/step\n",
            "Epoch 831/1000\n",
            "250/250 - 1s - loss: 0.0933 - accuracy: 0.9772 - val_loss: 0.0535 - val_accuracy: 0.9850 - 601ms/epoch - 2ms/step\n",
            "Epoch 832/1000\n",
            "250/250 - 1s - loss: 0.0805 - accuracy: 0.9805 - val_loss: 0.0608 - val_accuracy: 0.9855 - 626ms/epoch - 3ms/step\n",
            "Epoch 833/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9819 - val_loss: 0.0570 - val_accuracy: 0.9855 - 613ms/epoch - 2ms/step\n",
            "Epoch 834/1000\n",
            "250/250 - 1s - loss: 0.0730 - accuracy: 0.9829 - val_loss: 0.0536 - val_accuracy: 0.9865 - 602ms/epoch - 2ms/step\n",
            "Epoch 835/1000\n",
            "250/250 - 1s - loss: 0.0786 - accuracy: 0.9818 - val_loss: 0.0561 - val_accuracy: 0.9850 - 616ms/epoch - 2ms/step\n",
            "Epoch 836/1000\n",
            "250/250 - 1s - loss: 0.0868 - accuracy: 0.9800 - val_loss: 0.0583 - val_accuracy: 0.9855 - 528ms/epoch - 2ms/step\n",
            "Epoch 837/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9809 - val_loss: 0.0599 - val_accuracy: 0.9850 - 616ms/epoch - 2ms/step\n",
            "Epoch 838/1000\n",
            "250/250 - 1s - loss: 0.0864 - accuracy: 0.9786 - val_loss: 0.0573 - val_accuracy: 0.9865 - 607ms/epoch - 2ms/step\n",
            "Epoch 839/1000\n",
            "250/250 - 1s - loss: 0.0768 - accuracy: 0.9827 - val_loss: 0.0539 - val_accuracy: 0.9860 - 610ms/epoch - 2ms/step\n",
            "Epoch 840/1000\n",
            "250/250 - 1s - loss: 0.0714 - accuracy: 0.9821 - val_loss: 0.0549 - val_accuracy: 0.9870 - 543ms/epoch - 2ms/step\n",
            "Epoch 841/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9830 - val_loss: 0.0571 - val_accuracy: 0.9860 - 527ms/epoch - 2ms/step\n",
            "Epoch 842/1000\n",
            "250/250 - 1s - loss: 0.0753 - accuracy: 0.9819 - val_loss: 0.0612 - val_accuracy: 0.9855 - 566ms/epoch - 2ms/step\n",
            "Epoch 843/1000\n",
            "250/250 - 1s - loss: 0.0844 - accuracy: 0.9801 - val_loss: 0.0564 - val_accuracy: 0.9870 - 537ms/epoch - 2ms/step\n",
            "Epoch 844/1000\n",
            "250/250 - 1s - loss: 0.0834 - accuracy: 0.9808 - val_loss: 0.0566 - val_accuracy: 0.9870 - 545ms/epoch - 2ms/step\n",
            "Epoch 845/1000\n",
            "250/250 - 1s - loss: 0.0811 - accuracy: 0.9808 - val_loss: 0.0569 - val_accuracy: 0.9865 - 559ms/epoch - 2ms/step\n",
            "Epoch 846/1000\n",
            "250/250 - 1s - loss: 0.0722 - accuracy: 0.9831 - val_loss: 0.0595 - val_accuracy: 0.9870 - 563ms/epoch - 2ms/step\n",
            "Epoch 847/1000\n",
            "250/250 - 1s - loss: 0.0812 - accuracy: 0.9804 - val_loss: 0.0595 - val_accuracy: 0.9850 - 537ms/epoch - 2ms/step\n",
            "Epoch 848/1000\n",
            "250/250 - 1s - loss: 0.0813 - accuracy: 0.9806 - val_loss: 0.0596 - val_accuracy: 0.9860 - 521ms/epoch - 2ms/step\n",
            "Epoch 849/1000\n",
            "250/250 - 1s - loss: 0.0820 - accuracy: 0.9804 - val_loss: 0.0559 - val_accuracy: 0.9865 - 621ms/epoch - 2ms/step\n",
            "Epoch 850/1000\n",
            "250/250 - 1s - loss: 0.0796 - accuracy: 0.9819 - val_loss: 0.0548 - val_accuracy: 0.9855 - 551ms/epoch - 2ms/step\n",
            "Epoch 851/1000\n",
            "250/250 - 1s - loss: 0.0748 - accuracy: 0.9829 - val_loss: 0.0591 - val_accuracy: 0.9855 - 556ms/epoch - 2ms/step\n",
            "Epoch 852/1000\n",
            "250/250 - 1s - loss: 0.0855 - accuracy: 0.9809 - val_loss: 0.0547 - val_accuracy: 0.9860 - 602ms/epoch - 2ms/step\n",
            "Epoch 853/1000\n",
            "250/250 - 1s - loss: 0.0805 - accuracy: 0.9805 - val_loss: 0.0531 - val_accuracy: 0.9875 - 563ms/epoch - 2ms/step\n",
            "Epoch 854/1000\n",
            "250/250 - 1s - loss: 0.0810 - accuracy: 0.9795 - val_loss: 0.0562 - val_accuracy: 0.9875 - 628ms/epoch - 3ms/step\n",
            "Epoch 855/1000\n",
            "250/250 - 1s - loss: 0.0707 - accuracy: 0.9830 - val_loss: 0.0586 - val_accuracy: 0.9860 - 534ms/epoch - 2ms/step\n",
            "Epoch 856/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9801 - val_loss: 0.0596 - val_accuracy: 0.9860 - 613ms/epoch - 2ms/step\n",
            "Epoch 857/1000\n",
            "250/250 - 1s - loss: 0.0725 - accuracy: 0.9830 - val_loss: 0.0524 - val_accuracy: 0.9870 - 608ms/epoch - 2ms/step\n",
            "Epoch 858/1000\n",
            "250/250 - 1s - loss: 0.0918 - accuracy: 0.9768 - val_loss: 0.0594 - val_accuracy: 0.9845 - 542ms/epoch - 2ms/step\n",
            "Epoch 859/1000\n",
            "250/250 - 1s - loss: 0.0856 - accuracy: 0.9789 - val_loss: 0.0550 - val_accuracy: 0.9855 - 602ms/epoch - 2ms/step\n",
            "Epoch 860/1000\n",
            "250/250 - 1s - loss: 0.0741 - accuracy: 0.9814 - val_loss: 0.0579 - val_accuracy: 0.9880 - 622ms/epoch - 2ms/step\n",
            "Epoch 861/1000\n",
            "250/250 - 1s - loss: 0.0754 - accuracy: 0.9824 - val_loss: 0.0540 - val_accuracy: 0.9855 - 626ms/epoch - 3ms/step\n",
            "Epoch 862/1000\n",
            "250/250 - 1s - loss: 0.0884 - accuracy: 0.9779 - val_loss: 0.0586 - val_accuracy: 0.9855 - 528ms/epoch - 2ms/step\n",
            "Epoch 863/1000\n",
            "250/250 - 1s - loss: 0.0785 - accuracy: 0.9816 - val_loss: 0.0542 - val_accuracy: 0.9855 - 551ms/epoch - 2ms/step\n",
            "Epoch 864/1000\n",
            "250/250 - 1s - loss: 0.0720 - accuracy: 0.9822 - val_loss: 0.0540 - val_accuracy: 0.9855 - 611ms/epoch - 2ms/step\n",
            "Epoch 865/1000\n",
            "250/250 - 1s - loss: 0.0729 - accuracy: 0.9822 - val_loss: 0.0553 - val_accuracy: 0.9850 - 623ms/epoch - 2ms/step\n",
            "Epoch 866/1000\n",
            "250/250 - 1s - loss: 0.0729 - accuracy: 0.9834 - val_loss: 0.0661 - val_accuracy: 0.9840 - 622ms/epoch - 2ms/step\n",
            "Epoch 867/1000\n",
            "250/250 - 1s - loss: 0.0783 - accuracy: 0.9821 - val_loss: 0.0534 - val_accuracy: 0.9870 - 613ms/epoch - 2ms/step\n",
            "Epoch 868/1000\n",
            "250/250 - 1s - loss: 0.0889 - accuracy: 0.9779 - val_loss: 0.0610 - val_accuracy: 0.9865 - 583ms/epoch - 2ms/step\n",
            "Epoch 869/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9822 - val_loss: 0.0539 - val_accuracy: 0.9875 - 601ms/epoch - 2ms/step\n",
            "Epoch 870/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9814 - val_loss: 0.0601 - val_accuracy: 0.9860 - 537ms/epoch - 2ms/step\n",
            "Epoch 871/1000\n",
            "250/250 - 1s - loss: 0.0819 - accuracy: 0.9804 - val_loss: 0.0545 - val_accuracy: 0.9870 - 531ms/epoch - 2ms/step\n",
            "Epoch 872/1000\n",
            "250/250 - 1s - loss: 0.0798 - accuracy: 0.9806 - val_loss: 0.0566 - val_accuracy: 0.9855 - 620ms/epoch - 2ms/step\n",
            "Epoch 873/1000\n",
            "250/250 - 1s - loss: 0.0739 - accuracy: 0.9821 - val_loss: 0.0572 - val_accuracy: 0.9865 - 541ms/epoch - 2ms/step\n",
            "Epoch 874/1000\n",
            "250/250 - 1s - loss: 0.0773 - accuracy: 0.9816 - val_loss: 0.0538 - val_accuracy: 0.9875 - 615ms/epoch - 2ms/step\n",
            "Epoch 875/1000\n",
            "250/250 - 1s - loss: 0.0889 - accuracy: 0.9780 - val_loss: 0.0544 - val_accuracy: 0.9855 - 627ms/epoch - 3ms/step\n",
            "Epoch 876/1000\n",
            "250/250 - 1s - loss: 0.0848 - accuracy: 0.9795 - val_loss: 0.0614 - val_accuracy: 0.9850 - 529ms/epoch - 2ms/step\n",
            "Epoch 877/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9824 - val_loss: 0.0578 - val_accuracy: 0.9855 - 635ms/epoch - 3ms/step\n",
            "Epoch 878/1000\n",
            "250/250 - 1s - loss: 0.0805 - accuracy: 0.9830 - val_loss: 0.0620 - val_accuracy: 0.9850 - 536ms/epoch - 2ms/step\n",
            "Epoch 879/1000\n",
            "250/250 - 1s - loss: 0.0888 - accuracy: 0.9785 - val_loss: 0.0547 - val_accuracy: 0.9870 - 544ms/epoch - 2ms/step\n",
            "Epoch 880/1000\n",
            "250/250 - 1s - loss: 0.0772 - accuracy: 0.9811 - val_loss: 0.0627 - val_accuracy: 0.9865 - 602ms/epoch - 2ms/step\n",
            "Epoch 881/1000\n",
            "250/250 - 1s - loss: 0.0810 - accuracy: 0.9815 - val_loss: 0.0553 - val_accuracy: 0.9880 - 606ms/epoch - 2ms/step\n",
            "Epoch 882/1000\n",
            "250/250 - 1s - loss: 0.0742 - accuracy: 0.9824 - val_loss: 0.0621 - val_accuracy: 0.9825 - 547ms/epoch - 2ms/step\n",
            "Epoch 883/1000\n",
            "250/250 - 1s - loss: 0.0764 - accuracy: 0.9824 - val_loss: 0.0541 - val_accuracy: 0.9855 - 608ms/epoch - 2ms/step\n",
            "Epoch 884/1000\n",
            "250/250 - 1s - loss: 0.0769 - accuracy: 0.9806 - val_loss: 0.0574 - val_accuracy: 0.9855 - 541ms/epoch - 2ms/step\n",
            "Epoch 885/1000\n",
            "250/250 - 1s - loss: 0.0813 - accuracy: 0.9806 - val_loss: 0.0547 - val_accuracy: 0.9880 - 609ms/epoch - 2ms/step\n",
            "Epoch 886/1000\n",
            "250/250 - 1s - loss: 0.0785 - accuracy: 0.9797 - val_loss: 0.0528 - val_accuracy: 0.9865 - 639ms/epoch - 3ms/step\n",
            "Epoch 887/1000\n",
            "250/250 - 1s - loss: 0.0712 - accuracy: 0.9835 - val_loss: 0.0567 - val_accuracy: 0.9860 - 601ms/epoch - 2ms/step\n",
            "Epoch 888/1000\n",
            "250/250 - 1s - loss: 0.0709 - accuracy: 0.9841 - val_loss: 0.0574 - val_accuracy: 0.9870 - 602ms/epoch - 2ms/step\n",
            "Epoch 889/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9804 - val_loss: 0.0563 - val_accuracy: 0.9880 - 624ms/epoch - 2ms/step\n",
            "Epoch 890/1000\n",
            "250/250 - 1s - loss: 0.0894 - accuracy: 0.9781 - val_loss: 0.0527 - val_accuracy: 0.9865 - 619ms/epoch - 2ms/step\n",
            "Epoch 891/1000\n",
            "250/250 - 1s - loss: 0.0742 - accuracy: 0.9819 - val_loss: 0.0601 - val_accuracy: 0.9855 - 535ms/epoch - 2ms/step\n",
            "Epoch 892/1000\n",
            "250/250 - 1s - loss: 0.0712 - accuracy: 0.9818 - val_loss: 0.0562 - val_accuracy: 0.9860 - 532ms/epoch - 2ms/step\n",
            "Epoch 893/1000\n",
            "250/250 - 1s - loss: 0.0780 - accuracy: 0.9829 - val_loss: 0.0562 - val_accuracy: 0.9855 - 535ms/epoch - 2ms/step\n",
            "Epoch 894/1000\n",
            "250/250 - 1s - loss: 0.0748 - accuracy: 0.9816 - val_loss: 0.0560 - val_accuracy: 0.9860 - 553ms/epoch - 2ms/step\n",
            "Epoch 895/1000\n",
            "250/250 - 1s - loss: 0.0752 - accuracy: 0.9821 - val_loss: 0.0536 - val_accuracy: 0.9875 - 603ms/epoch - 2ms/step\n",
            "Epoch 896/1000\n",
            "250/250 - 1s - loss: 0.0829 - accuracy: 0.9805 - val_loss: 0.0541 - val_accuracy: 0.9870 - 614ms/epoch - 2ms/step\n",
            "Epoch 897/1000\n",
            "250/250 - 1s - loss: 0.0782 - accuracy: 0.9827 - val_loss: 0.0583 - val_accuracy: 0.9855 - 540ms/epoch - 2ms/step\n",
            "Epoch 898/1000\n",
            "250/250 - 1s - loss: 0.0803 - accuracy: 0.9811 - val_loss: 0.0581 - val_accuracy: 0.9870 - 620ms/epoch - 2ms/step\n",
            "Epoch 899/1000\n",
            "250/250 - 1s - loss: 0.0785 - accuracy: 0.9816 - val_loss: 0.0556 - val_accuracy: 0.9865 - 535ms/epoch - 2ms/step\n",
            "Epoch 900/1000\n",
            "250/250 - 1s - loss: 0.0718 - accuracy: 0.9835 - val_loss: 0.0559 - val_accuracy: 0.9865 - 552ms/epoch - 2ms/step\n",
            "Epoch 901/1000\n",
            "250/250 - 1s - loss: 0.0783 - accuracy: 0.9808 - val_loss: 0.0553 - val_accuracy: 0.9870 - 542ms/epoch - 2ms/step\n",
            "Epoch 902/1000\n",
            "250/250 - 1s - loss: 0.0724 - accuracy: 0.9824 - val_loss: 0.0539 - val_accuracy: 0.9870 - 616ms/epoch - 2ms/step\n",
            "Epoch 903/1000\n",
            "250/250 - 1s - loss: 0.0727 - accuracy: 0.9818 - val_loss: 0.0753 - val_accuracy: 0.9815 - 619ms/epoch - 2ms/step\n",
            "Epoch 904/1000\n",
            "250/250 - 1s - loss: 0.0812 - accuracy: 0.9810 - val_loss: 0.0622 - val_accuracy: 0.9860 - 629ms/epoch - 3ms/step\n",
            "Epoch 905/1000\n",
            "250/250 - 1s - loss: 0.0783 - accuracy: 0.9820 - val_loss: 0.0567 - val_accuracy: 0.9865 - 558ms/epoch - 2ms/step\n",
            "Epoch 906/1000\n",
            "250/250 - 1s - loss: 0.0769 - accuracy: 0.9811 - val_loss: 0.0569 - val_accuracy: 0.9860 - 534ms/epoch - 2ms/step\n",
            "Epoch 907/1000\n",
            "250/250 - 1s - loss: 0.0745 - accuracy: 0.9826 - val_loss: 0.0534 - val_accuracy: 0.9880 - 614ms/epoch - 2ms/step\n",
            "Epoch 908/1000\n",
            "250/250 - 1s - loss: 0.0821 - accuracy: 0.9819 - val_loss: 0.0590 - val_accuracy: 0.9870 - 544ms/epoch - 2ms/step\n",
            "Epoch 909/1000\n",
            "250/250 - 1s - loss: 0.0743 - accuracy: 0.9824 - val_loss: 0.0559 - val_accuracy: 0.9865 - 542ms/epoch - 2ms/step\n",
            "Epoch 910/1000\n",
            "250/250 - 1s - loss: 0.0763 - accuracy: 0.9810 - val_loss: 0.0593 - val_accuracy: 0.9880 - 621ms/epoch - 2ms/step\n",
            "Epoch 911/1000\n",
            "250/250 - 1s - loss: 0.0859 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9870 - 609ms/epoch - 2ms/step\n",
            "Epoch 912/1000\n",
            "250/250 - 1s - loss: 0.0708 - accuracy: 0.9831 - val_loss: 0.0589 - val_accuracy: 0.9880 - 554ms/epoch - 2ms/step\n",
            "Epoch 913/1000\n",
            "250/250 - 1s - loss: 0.0824 - accuracy: 0.9805 - val_loss: 0.0540 - val_accuracy: 0.9875 - 528ms/epoch - 2ms/step\n",
            "Epoch 914/1000\n",
            "250/250 - 1s - loss: 0.0830 - accuracy: 0.9812 - val_loss: 0.0560 - val_accuracy: 0.9855 - 615ms/epoch - 2ms/step\n",
            "Epoch 915/1000\n",
            "250/250 - 1s - loss: 0.0732 - accuracy: 0.9815 - val_loss: 0.0581 - val_accuracy: 0.9865 - 539ms/epoch - 2ms/step\n",
            "Epoch 916/1000\n",
            "250/250 - 1s - loss: 0.0746 - accuracy: 0.9818 - val_loss: 0.0568 - val_accuracy: 0.9860 - 538ms/epoch - 2ms/step\n",
            "Epoch 917/1000\n",
            "250/250 - 1s - loss: 0.0751 - accuracy: 0.9818 - val_loss: 0.0615 - val_accuracy: 0.9855 - 548ms/epoch - 2ms/step\n",
            "Epoch 918/1000\n",
            "250/250 - 1s - loss: 0.0801 - accuracy: 0.9819 - val_loss: 0.0561 - val_accuracy: 0.9865 - 607ms/epoch - 2ms/step\n",
            "Epoch 919/1000\n",
            "250/250 - 1s - loss: 0.0718 - accuracy: 0.9829 - val_loss: 0.0559 - val_accuracy: 0.9875 - 613ms/epoch - 2ms/step\n",
            "Epoch 920/1000\n",
            "250/250 - 1s - loss: 0.0770 - accuracy: 0.9805 - val_loss: 0.0582 - val_accuracy: 0.9865 - 544ms/epoch - 2ms/step\n",
            "Epoch 921/1000\n",
            "250/250 - 1s - loss: 0.0809 - accuracy: 0.9795 - val_loss: 0.0555 - val_accuracy: 0.9875 - 555ms/epoch - 2ms/step\n",
            "Epoch 922/1000\n",
            "250/250 - 1s - loss: 0.0836 - accuracy: 0.9796 - val_loss: 0.0566 - val_accuracy: 0.9870 - 537ms/epoch - 2ms/step\n",
            "Epoch 923/1000\n",
            "250/250 - 1s - loss: 0.0852 - accuracy: 0.9800 - val_loss: 0.0553 - val_accuracy: 0.9855 - 640ms/epoch - 3ms/step\n",
            "Epoch 924/1000\n",
            "250/250 - 1s - loss: 0.0803 - accuracy: 0.9814 - val_loss: 0.0558 - val_accuracy: 0.9870 - 619ms/epoch - 2ms/step\n",
            "Epoch 925/1000\n",
            "250/250 - 1s - loss: 0.0779 - accuracy: 0.9804 - val_loss: 0.0535 - val_accuracy: 0.9875 - 545ms/epoch - 2ms/step\n",
            "Epoch 926/1000\n",
            "250/250 - 1s - loss: 0.0815 - accuracy: 0.9818 - val_loss: 0.0582 - val_accuracy: 0.9865 - 628ms/epoch - 3ms/step\n",
            "Epoch 927/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9819 - val_loss: 0.0573 - val_accuracy: 0.9865 - 541ms/epoch - 2ms/step\n",
            "Epoch 928/1000\n",
            "250/250 - 1s - loss: 0.0769 - accuracy: 0.9815 - val_loss: 0.0647 - val_accuracy: 0.9850 - 654ms/epoch - 3ms/step\n",
            "Epoch 929/1000\n",
            "250/250 - 1s - loss: 0.0728 - accuracy: 0.9820 - val_loss: 0.0539 - val_accuracy: 0.9870 - 547ms/epoch - 2ms/step\n",
            "Epoch 930/1000\n",
            "250/250 - 1s - loss: 0.0680 - accuracy: 0.9827 - val_loss: 0.0617 - val_accuracy: 0.9855 - 544ms/epoch - 2ms/step\n",
            "Epoch 931/1000\n",
            "250/250 - 1s - loss: 0.0801 - accuracy: 0.9809 - val_loss: 0.0561 - val_accuracy: 0.9855 - 608ms/epoch - 2ms/step\n",
            "Epoch 932/1000\n",
            "250/250 - 1s - loss: 0.0746 - accuracy: 0.9816 - val_loss: 0.0557 - val_accuracy: 0.9865 - 605ms/epoch - 2ms/step\n",
            "Epoch 933/1000\n",
            "250/250 - 1s - loss: 0.0820 - accuracy: 0.9794 - val_loss: 0.0530 - val_accuracy: 0.9875 - 555ms/epoch - 2ms/step\n",
            "Epoch 934/1000\n",
            "250/250 - 1s - loss: 0.0759 - accuracy: 0.9821 - val_loss: 0.0557 - val_accuracy: 0.9865 - 537ms/epoch - 2ms/step\n",
            "Epoch 935/1000\n",
            "250/250 - 1s - loss: 0.0800 - accuracy: 0.9816 - val_loss: 0.0536 - val_accuracy: 0.9875 - 623ms/epoch - 2ms/step\n",
            "Epoch 936/1000\n",
            "250/250 - 1s - loss: 0.0817 - accuracy: 0.9805 - val_loss: 0.0565 - val_accuracy: 0.9865 - 614ms/epoch - 2ms/step\n",
            "Epoch 937/1000\n",
            "250/250 - 1s - loss: 0.0723 - accuracy: 0.9841 - val_loss: 0.0574 - val_accuracy: 0.9875 - 612ms/epoch - 2ms/step\n",
            "Epoch 938/1000\n",
            "250/250 - 1s - loss: 0.0687 - accuracy: 0.9841 - val_loss: 0.0549 - val_accuracy: 0.9875 - 617ms/epoch - 2ms/step\n",
            "Epoch 939/1000\n",
            "250/250 - 1s - loss: 0.0775 - accuracy: 0.9799 - val_loss: 0.0590 - val_accuracy: 0.9860 - 524ms/epoch - 2ms/step\n",
            "Epoch 940/1000\n",
            "250/250 - 1s - loss: 0.0740 - accuracy: 0.9821 - val_loss: 0.0564 - val_accuracy: 0.9870 - 545ms/epoch - 2ms/step\n",
            "Epoch 941/1000\n",
            "250/250 - 1s - loss: 0.0774 - accuracy: 0.9812 - val_loss: 0.0566 - val_accuracy: 0.9865 - 619ms/epoch - 2ms/step\n",
            "Epoch 942/1000\n",
            "250/250 - 1s - loss: 0.0743 - accuracy: 0.9820 - val_loss: 0.0560 - val_accuracy: 0.9860 - 541ms/epoch - 2ms/step\n",
            "Epoch 943/1000\n",
            "250/250 - 1s - loss: 0.0747 - accuracy: 0.9830 - val_loss: 0.0558 - val_accuracy: 0.9875 - 629ms/epoch - 3ms/step\n",
            "Epoch 944/1000\n",
            "250/250 - 1s - loss: 0.0748 - accuracy: 0.9836 - val_loss: 0.0544 - val_accuracy: 0.9880 - 627ms/epoch - 3ms/step\n",
            "Epoch 945/1000\n",
            "250/250 - 1s - loss: 0.0749 - accuracy: 0.9810 - val_loss: 0.0631 - val_accuracy: 0.9865 - 592ms/epoch - 2ms/step\n",
            "Epoch 946/1000\n",
            "250/250 - 1s - loss: 0.0762 - accuracy: 0.9812 - val_loss: 0.0616 - val_accuracy: 0.9855 - 530ms/epoch - 2ms/step\n",
            "Epoch 947/1000\n",
            "250/250 - 1s - loss: 0.0733 - accuracy: 0.9830 - val_loss: 0.0556 - val_accuracy: 0.9875 - 535ms/epoch - 2ms/step\n",
            "Epoch 948/1000\n",
            "250/250 - 1s - loss: 0.0816 - accuracy: 0.9821 - val_loss: 0.0555 - val_accuracy: 0.9860 - 598ms/epoch - 2ms/step\n",
            "Epoch 949/1000\n",
            "250/250 - 1s - loss: 0.0763 - accuracy: 0.9821 - val_loss: 0.0594 - val_accuracy: 0.9860 - 545ms/epoch - 2ms/step\n",
            "Epoch 950/1000\n",
            "250/250 - 1s - loss: 0.0884 - accuracy: 0.9771 - val_loss: 0.0590 - val_accuracy: 0.9865 - 543ms/epoch - 2ms/step\n",
            "Epoch 951/1000\n",
            "250/250 - 1s - loss: 0.0781 - accuracy: 0.9820 - val_loss: 0.0573 - val_accuracy: 0.9855 - 609ms/epoch - 2ms/step\n",
            "Epoch 952/1000\n",
            "250/250 - 1s - loss: 0.0755 - accuracy: 0.9833 - val_loss: 0.0582 - val_accuracy: 0.9870 - 555ms/epoch - 2ms/step\n",
            "Epoch 953/1000\n",
            "250/250 - 1s - loss: 0.0856 - accuracy: 0.9800 - val_loss: 0.0576 - val_accuracy: 0.9860 - 528ms/epoch - 2ms/step\n",
            "Epoch 954/1000\n",
            "250/250 - 1s - loss: 0.0726 - accuracy: 0.9827 - val_loss: 0.0561 - val_accuracy: 0.9855 - 629ms/epoch - 3ms/step\n",
            "Epoch 955/1000\n",
            "250/250 - 1s - loss: 0.0721 - accuracy: 0.9829 - val_loss: 0.0564 - val_accuracy: 0.9860 - 534ms/epoch - 2ms/step\n",
            "Epoch 956/1000\n",
            "250/250 - 1s - loss: 0.0755 - accuracy: 0.9825 - val_loss: 0.0548 - val_accuracy: 0.9870 - 545ms/epoch - 2ms/step\n",
            "Epoch 957/1000\n",
            "250/250 - 1s - loss: 0.0748 - accuracy: 0.9825 - val_loss: 0.0616 - val_accuracy: 0.9870 - 530ms/epoch - 2ms/step\n",
            "Epoch 958/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9803 - val_loss: 0.0579 - val_accuracy: 0.9860 - 627ms/epoch - 3ms/step\n",
            "Epoch 959/1000\n",
            "250/250 - 1s - loss: 0.0686 - accuracy: 0.9826 - val_loss: 0.0585 - val_accuracy: 0.9855 - 639ms/epoch - 3ms/step\n",
            "Epoch 960/1000\n",
            "250/250 - 1s - loss: 0.0804 - accuracy: 0.9812 - val_loss: 0.0575 - val_accuracy: 0.9860 - 608ms/epoch - 2ms/step\n",
            "Epoch 961/1000\n",
            "250/250 - 1s - loss: 0.0757 - accuracy: 0.9819 - val_loss: 0.0582 - val_accuracy: 0.9850 - 617ms/epoch - 2ms/step\n",
            "Epoch 962/1000\n",
            "250/250 - 1s - loss: 0.0753 - accuracy: 0.9829 - val_loss: 0.0547 - val_accuracy: 0.9865 - 603ms/epoch - 2ms/step\n",
            "Epoch 963/1000\n",
            "250/250 - 1s - loss: 0.0891 - accuracy: 0.9787 - val_loss: 0.0551 - val_accuracy: 0.9850 - 535ms/epoch - 2ms/step\n",
            "Epoch 964/1000\n",
            "250/250 - 1s - loss: 0.0769 - accuracy: 0.9822 - val_loss: 0.0568 - val_accuracy: 0.9865 - 615ms/epoch - 2ms/step\n",
            "Epoch 965/1000\n",
            "250/250 - 1s - loss: 0.0781 - accuracy: 0.9829 - val_loss: 0.0597 - val_accuracy: 0.9870 - 605ms/epoch - 2ms/step\n",
            "Epoch 966/1000\n",
            "250/250 - 1s - loss: 0.0748 - accuracy: 0.9819 - val_loss: 0.0639 - val_accuracy: 0.9875 - 618ms/epoch - 2ms/step\n",
            "Epoch 967/1000\n",
            "250/250 - 1s - loss: 0.0744 - accuracy: 0.9836 - val_loss: 0.0557 - val_accuracy: 0.9875 - 539ms/epoch - 2ms/step\n",
            "Epoch 968/1000\n",
            "250/250 - 1s - loss: 0.0828 - accuracy: 0.9812 - val_loss: 0.0577 - val_accuracy: 0.9875 - 551ms/epoch - 2ms/step\n",
            "Epoch 969/1000\n",
            "250/250 - 1s - loss: 0.0794 - accuracy: 0.9820 - val_loss: 0.0555 - val_accuracy: 0.9855 - 564ms/epoch - 2ms/step\n",
            "Epoch 970/1000\n",
            "250/250 - 1s - loss: 0.0845 - accuracy: 0.9800 - val_loss: 0.0556 - val_accuracy: 0.9885 - 621ms/epoch - 2ms/step\n",
            "Epoch 971/1000\n",
            "250/250 - 1s - loss: 0.0727 - accuracy: 0.9821 - val_loss: 0.0543 - val_accuracy: 0.9865 - 627ms/epoch - 3ms/step\n",
            "Epoch 972/1000\n",
            "250/250 - 1s - loss: 0.0753 - accuracy: 0.9816 - val_loss: 0.0547 - val_accuracy: 0.9870 - 605ms/epoch - 2ms/step\n",
            "Epoch 973/1000\n",
            "250/250 - 1s - loss: 0.0737 - accuracy: 0.9826 - val_loss: 0.0650 - val_accuracy: 0.9870 - 534ms/epoch - 2ms/step\n",
            "Epoch 974/1000\n",
            "250/250 - 1s - loss: 0.0745 - accuracy: 0.9803 - val_loss: 0.0529 - val_accuracy: 0.9870 - 622ms/epoch - 2ms/step\n",
            "Epoch 975/1000\n",
            "250/250 - 1s - loss: 0.0851 - accuracy: 0.9796 - val_loss: 0.0582 - val_accuracy: 0.9870 - 554ms/epoch - 2ms/step\n",
            "Epoch 976/1000\n",
            "250/250 - 1s - loss: 0.0814 - accuracy: 0.9796 - val_loss: 0.0626 - val_accuracy: 0.9865 - 603ms/epoch - 2ms/step\n",
            "Epoch 977/1000\n",
            "250/250 - 1s - loss: 0.0786 - accuracy: 0.9809 - val_loss: 0.0588 - val_accuracy: 0.9860 - 614ms/epoch - 2ms/step\n",
            "Epoch 978/1000\n",
            "250/250 - 1s - loss: 0.0832 - accuracy: 0.9804 - val_loss: 0.0547 - val_accuracy: 0.9870 - 553ms/epoch - 2ms/step\n",
            "Epoch 979/1000\n",
            "250/250 - 1s - loss: 0.0732 - accuracy: 0.9815 - val_loss: 0.0582 - val_accuracy: 0.9870 - 537ms/epoch - 2ms/step\n",
            "Epoch 980/1000\n",
            "250/250 - 1s - loss: 0.0868 - accuracy: 0.9799 - val_loss: 0.0560 - val_accuracy: 0.9875 - 538ms/epoch - 2ms/step\n",
            "Epoch 981/1000\n",
            "250/250 - 1s - loss: 0.0803 - accuracy: 0.9801 - val_loss: 0.0610 - val_accuracy: 0.9865 - 549ms/epoch - 2ms/step\n",
            "Epoch 982/1000\n",
            "250/250 - 1s - loss: 0.0723 - accuracy: 0.9840 - val_loss: 0.0590 - val_accuracy: 0.9860 - 546ms/epoch - 2ms/step\n",
            "Epoch 983/1000\n",
            "250/250 - 1s - loss: 0.0775 - accuracy: 0.9810 - val_loss: 0.0589 - val_accuracy: 0.9855 - 613ms/epoch - 2ms/step\n",
            "Epoch 984/1000\n",
            "250/250 - 1s - loss: 0.0829 - accuracy: 0.9803 - val_loss: 0.0594 - val_accuracy: 0.9835 - 540ms/epoch - 2ms/step\n",
            "Epoch 985/1000\n",
            "250/250 - 1s - loss: 0.0726 - accuracy: 0.9816 - val_loss: 0.0581 - val_accuracy: 0.9870 - 534ms/epoch - 2ms/step\n",
            "Epoch 986/1000\n",
            "250/250 - 1s - loss: 0.0725 - accuracy: 0.9818 - val_loss: 0.0562 - val_accuracy: 0.9850 - 602ms/epoch - 2ms/step\n",
            "Epoch 987/1000\n",
            "250/250 - 1s - loss: 0.0750 - accuracy: 0.9822 - val_loss: 0.0557 - val_accuracy: 0.9855 - 621ms/epoch - 2ms/step\n",
            "Epoch 988/1000\n",
            "250/250 - 1s - loss: 0.0698 - accuracy: 0.9837 - val_loss: 0.0605 - val_accuracy: 0.9860 - 617ms/epoch - 2ms/step\n",
            "Epoch 989/1000\n",
            "250/250 - 1s - loss: 0.0819 - accuracy: 0.9800 - val_loss: 0.0587 - val_accuracy: 0.9860 - 534ms/epoch - 2ms/step\n",
            "Epoch 990/1000\n",
            "250/250 - 1s - loss: 0.0716 - accuracy: 0.9827 - val_loss: 0.0563 - val_accuracy: 0.9855 - 631ms/epoch - 3ms/step\n",
            "Epoch 991/1000\n",
            "250/250 - 1s - loss: 0.0808 - accuracy: 0.9819 - val_loss: 0.0542 - val_accuracy: 0.9865 - 547ms/epoch - 2ms/step\n",
            "Epoch 992/1000\n",
            "250/250 - 1s - loss: 0.0745 - accuracy: 0.9831 - val_loss: 0.0540 - val_accuracy: 0.9865 - 604ms/epoch - 2ms/step\n",
            "Epoch 993/1000\n",
            "250/250 - 1s - loss: 0.0741 - accuracy: 0.9822 - val_loss: 0.0579 - val_accuracy: 0.9865 - 539ms/epoch - 2ms/step\n",
            "Epoch 994/1000\n",
            "250/250 - 1s - loss: 0.0656 - accuracy: 0.9837 - val_loss: 0.0598 - val_accuracy: 0.9855 - 618ms/epoch - 2ms/step\n",
            "Epoch 995/1000\n",
            "250/250 - 1s - loss: 0.0770 - accuracy: 0.9822 - val_loss: 0.0545 - val_accuracy: 0.9880 - 637ms/epoch - 3ms/step\n",
            "Epoch 996/1000\n",
            "250/250 - 1s - loss: 0.0700 - accuracy: 0.9826 - val_loss: 0.0551 - val_accuracy: 0.9865 - 619ms/epoch - 2ms/step\n",
            "Epoch 997/1000\n",
            "250/250 - 1s - loss: 0.0769 - accuracy: 0.9814 - val_loss: 0.0567 - val_accuracy: 0.9875 - 604ms/epoch - 2ms/step\n",
            "Epoch 998/1000\n",
            "250/250 - 1s - loss: 0.0752 - accuracy: 0.9820 - val_loss: 0.0516 - val_accuracy: 0.9885 - 614ms/epoch - 2ms/step\n",
            "Epoch 999/1000\n",
            "250/250 - 1s - loss: 0.0732 - accuracy: 0.9829 - val_loss: 0.0557 - val_accuracy: 0.9865 - 546ms/epoch - 2ms/step\n",
            "Epoch 1000/1000\n",
            "250/250 - 1s - loss: 0.0709 - accuracy: 0.9846 - val_loss: 0.0539 - val_accuracy: 0.9880 - 540ms/epoch - 2ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train,Y_train,validation_data = (X_test,Y_test),batch_size = 32, epochs = 1000, verbose = 2 ) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "MzmjYCxoIedc",
        "outputId": "91256b0b-0bb0-4d78-b10c-ced411f7e265"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'],label='train')\n",
        "plt.plot(history.history['val_loss'],label='test')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4xHXCSRIedd",
        "outputId": "784da29e-c263-4b00-fb62-677ab2779155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss and Accuracy\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9914\n",
            "Testing Loss and Accuracy\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.053919535130262375, 0.9879999756813049]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "print(\"Training Loss and Accuracy\")\n",
        "model.evaluate(X_train,Y_train)\n",
        "print(\"Testing Loss and Accuracy\")\n",
        "model.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S1952pvIedd"
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.round(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_d6djpmIedd",
        "outputId": "15e7f715-2d74-4588-b001-b3c8dba80b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       200\n",
            "           1       1.00      0.99      1.00       200\n",
            "           2       0.98      0.99      0.99       200\n",
            "           3       0.99      0.99      0.99       200\n",
            "           4       0.99      0.99      0.99       200\n",
            "           5       0.99      0.98      0.98       200\n",
            "           6       0.98      0.99      0.99       200\n",
            "           7       0.97      0.97      0.97       200\n",
            "           8       0.99      0.94      0.97       200\n",
            "           9       0.99      0.99      0.99       200\n",
            "\n",
            "   micro avg       0.99      0.99      0.99      2000\n",
            "   macro avg       0.99      0.99      0.99      2000\n",
            "weighted avg       0.99      0.99      0.99      2000\n",
            " samples avg       0.99      0.99      0.99      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(Y_test,Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCZk45PFIedd",
        "outputId": "73c3608f-a372-4c05-922f-50d8dd3903fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/IITMComp/cur_model/alpha_16.pb/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/IITMComp/cur_model/alpha_16.pb/assets\n"
          ]
        }
      ],
      "source": [
        "model.save(\"/content/drive/MyDrive/IITMComp/cur_model/alpha_16.pb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlbBpgezP87R"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY7lmDk2Iedd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAoibqdoIede"
      },
      "outputs": [],
      "source": [
        "# tflite model conversion\n",
        "def representative_data_gen():\n",
        "  for input_value in X_train.take(100):\n",
        "    yield [input_value]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFLite model conversion"
      ],
      "metadata": {
        "id": "6JT8rLvOKwfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UyLyiK1Iede",
        "outputId": "c02df153-32ab-4ffd-a4f3-9d7b2e0e0ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf3n0k6ni/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf3n0k6ni/assets\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49408"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "tflite_model_quant = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open('/content/drive/MyDrive/IITMComp/cur_model/alpha_16_small_11_classes_1.tflite', \"wb\").write(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46MWMea2Iede"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LYn_nycnMFig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING model with new data"
      ],
      "metadata": {
        "id": "EsPh7gtoMF-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsiZrWn3Iede"
      },
      "outputs": [],
      "source": [
        "path1 = '/content/drive/MyDrive/IITMComp/Alpha-10-test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNgVpiSnIede"
      },
      "outputs": [],
      "source": [
        "def data_load_test(path):\n",
        "    global input_per_sequence,length_of_sequence\n",
        "    # dir= glob.glob(path + '/tctodd12')\n",
        "    \n",
        "    # print(dir)\n",
        "    time_steps = []\n",
        "    X_data = [] # data\n",
        "    Y_data = [] # labels\n",
        "\n",
        "    np.random.seed(101)\n",
        "    count = 0\n",
        "    for file in os.listdir(path):\n",
        "        class_name = file.split('_')[0]\n",
        "        if classes_count[class_name] > 0: \n",
        "          count+=1\n",
        "          print(file)\n",
        "          arr = np.genfromtxt(path + '/' + file, delimiter='/')\n",
        "          arr = np.delete(arr, -1, axis=1) # chopping last index in all rows\n",
        "          arr = np.delete(arr, 0, axis=1) # chopping 1st index in all rows\n",
        "          # arr = np.delete(arr, 0, axis=1) # chopping 1st index in all rows\n",
        "          for i in range(0,len(arr)-1,2):\n",
        "            arr_n =  np.append(arr[i],arr[i+1])\n",
        "            X_data.append(arr_n)\n",
        "            Y_data.append(class_name)\n",
        "      \n",
        "            \n",
        "\n",
        "\n",
        "    # for a in range(len(X_data)):\n",
        "    #     for k in range(0,3):\n",
        "    #       X_data[a][k] /= 360\n",
        "    #     for k in range(3,8):\n",
        "    #       X_data[a][k] /= 1023\n",
        "    #     for k in range(8,11):\n",
        "    #       X_data[a][k] /= 360\n",
        "    #     for k in range(11,16):\n",
        "    #       X_data[a][k] /= 1023\n",
        "    for a in range(len(X_data)):\n",
        "        for k in range(0,2):\n",
        "          X_data[a][k] = (X_data[a][k])/360\n",
        "        for k in range(2,7):\n",
        "          X_data[a][k] /= 1023\n",
        "        for k in range(7,9):\n",
        "          X_data[a][k] = (X_data[a][k])/360\n",
        "        for k in range(9,14):\n",
        "          X_data[a][k] /= 1023\n",
        "    X_data = np.array(X_data)\n",
        "    print(count)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return X_data,Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1yWzzCWIedf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b625b56-4e80-4b64-9885-f9b7e796decc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_test.txt\n",
            "L_test.txt\n",
            "H_test.txt\n",
            "World_test.txt\n",
            "We_test.txt\n",
            "Welcome_test.txt\n",
            "Hello_test.txt\n",
            "Namaste_test.txt\n",
            "Thankyou_test.txt\n",
            "Sorry_test.txt\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "#loading dataset\n",
        "X1_test, Y1_test = data_load_test(path1)\n",
        "Y1_test = to_categorical(le.transform(Y1_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoSfuJMHIedf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKIqJY2SIedf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d9ca52-4198-4491-8da9-879a10d2307d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0860 - accuracy: 0.9820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08595943450927734, 0.9819999933242798]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "model.evaluate(X1_test,Y1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFLITE testing\n"
      ],
      "metadata": {
        "id": "eJOzvedAL_ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes_count = {'A': 0, 'H': 1, 'Hello': 2, 'I': 3, 'M': 4, 'Mynameis': 5, 'N': 6, 'Namaste': 7, 'O': 8, 'R': 9, 'S': 10, 'Sorry': 11, 'T': 12, 'Team': 13, 'Thankyou': 14, 'W': 15, 'We': 16, 'Welcome': 17, 'World': 18,'L':19}\n",
        "for i in classes_count:\n",
        "  classes_count[i] = [0]"
      ],
      "metadata": {
        "id": "3Ysd453wyVBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiwW5qeZIedf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c1edcc-2cb4-4004-c2bb-f3dff18014f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of tflite: 0.976\n"
          ]
        }
      ],
      "source": [
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path='/content/drive/MyDrive/IITMComp/cur_model/alpha_16_small_11_classes_1.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "correct = 0\n",
        "total = 0\n",
        "for i in range(len(X1_test)):\n",
        "  input_data = [X1_test[i]]\n",
        "  input_data = np.array(input_data)\n",
        "  input_data = input_data.astype(\"float32\")\n",
        "\n",
        "  # print(input_data)\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # The function `get_tensor()` returns a copy of the tensor data.\n",
        "  # Use `tensor()` in order to get a pointer to the tensor.\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  f = list(le_dict.keys())\n",
        "  classes_count[f[np.argmax(Y1_test[i])]].append(f[ np.argmax(np.round(output_data)[0])])\n",
        "  if np.argmax(np.round(output_data)[0]) == np.argmax(Y1_test[i]):\n",
        "      # print(1)\n",
        "      \n",
        "      # print(f[0])\n",
        "      classes_count[f[np.argmax(Y1_test[i])]][0]+=1\n",
        "      \n",
        "      correct += 1\n",
        "  # else:\n",
        "      # print(np.argmax(np.round(output_data)[0]),np.argmax(Y1_test[i]))\n",
        "  total+=1\n",
        "  # print(np.round(output_data),Y1_test[i])\n",
        "  # print(np.argmax(np.round(output_data)[0]),np.argmax(Y1_test[i]))\n",
        "print(\"accuracy of tflite:\",correct/total)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for i in classes_count:\n",
        "#   print(i)\n",
        "#   print(classes_count[i])"
      ],
      "metadata": {
        "id": "juUyYhsFxwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs90bLoWIedf"
      },
      "outputs": [],
      "source": [
        "# for i in range(70,73):\n",
        "#   print(\"data :\",X1_test[i])\n",
        "#   print(\"output :\",Y1_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "k = {'A': 0, 'H': 1, 'Hello': 2, 'I': 3, 'M': 4, 'Mynameis': 5, 'N': 6, 'Namaste': 7, 'O': 8, 'R': 9, 'S': 10, 'Sorry': 11, 'T': 12, 'Team': 13, 'Thankyou': 14, 'W': 15, 'We': 16, 'Welcome': 17, 'World': 18}"
      ],
      "metadata": {
        "id": "q56qmMIKNZmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k.keys()\n",
        "# print(\"{\")\n",
        "# for i in k.keys():\n",
        "  \n",
        "#   print(\"\\\"\"+i+ \"\\\"\"+\",\")\n",
        "# print(\"}\")"
      ],
      "metadata": {
        "id": "ww1n6SXK1Jlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8ENC41V01PXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ALPHA_SIMPLE_NN_DATA-PREPROCESSING.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:Anaconda3-tf_mod]",
      "language": "python",
      "name": "conda-env-Anaconda3-tf_mod-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}